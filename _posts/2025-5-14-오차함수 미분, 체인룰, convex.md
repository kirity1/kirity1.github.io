---
key:
title: '오차함수 미분, 체인룰, convex함에 대해'
excerpt: 'deeplearning'
tags: [인공지능]
---

# 선형 회귀의 손실 함수와 최적화: 미분과 볼록성의 이해

선형 회귀 모델을 학습하는 과정은 손실 함수를 최소화하는 파라미터를 찾는 과정이다. 이 글에서는 단순 선형 회귀의 제곱 오차 손실 함수를 살펴보고, 이 손실 함수를 각 파라미터로 미분하는 과정(그래디언트 계산)과 볼록 함수의 개념이 최적화에 어떤 의미를 갖는지 알아본다.

## 1. 단순 선형 회귀의 손실 함수와 그래디언트

단순 선형 회귀 모델은 입력 변수 $x_i$에 대해 예측값 $\hat{y}_i = \phi_0 + \phi_1 x_i$를 출력한다. 여기서 $\phi_0$는 절편(intercept), $\phi_1$은 기울기(slope)이다. $i$번째 데이터 샘플에 대한 제곱 오차 손실 함수 $l_i$는 다음과 같이 정의된다.

$$ l_i(\phi_0, \phi_1) = (\hat{y}_i - y_i)^2 = (\phi_0 + \phi_1 x_i - y_i)^2 $$

경사 하강법(Gradient Descent)을 사용하여 이 손실 함수를 최소화하는 $\phi_0$와 $\phi_1$을 찾기 위해서는 각 파라미터에 대한 편미분을 계산해야 한다.

### 1.1. 연쇄 법칙(Chain Rule)을 이용한 미분

손실 함수 $l_i$를 $u(\phi_0, \phi_1) = \phi_0 + \phi_1 x_i - y_i$라고 두고, $l_i = u^2$로 생각하면 연쇄 법칙을 적용하여 각 파라미터에 대한 편미분을 구할 수 있다.

![image-20250513212836179](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250513212836179.png)

#### 1.1.1. $\phi_0$에 대한 편미분

연쇄 법칙에 따라,
$$ \frac{\partial l_i}{\partial \phi_0} = \frac{\partial l_i}{\partial u} \cdot \frac{\partial u}{\partial \phi_0} $$

각 항을 계산하면:
*   $\frac{\partial l_i}{\partial u} = 2u = 2(\phi_0 + \phi_1 x_i - y_i)$
*   $\frac{\partial u}{\partial \phi_0} = \frac{\partial}{\partial \phi_0} (\phi_0 + \phi_1 x_i - y_i) = 1$

따라서,
$$ \frac{\partial l_i}{\partial \phi_0} = 2(\phi_0 + \phi_1 x_i - y_i) \cdot 1 = 2(\phi_0 + \phi_1 x_i - y_i) $$
이것은 교재의 식 (6.6)에 해당한다.

#### 1.1.2. $\phi_1$에 대한 편미분

마찬가지로 연쇄 법칙에 따라,
$$ \frac{\partial l_i}{\partial \phi_1} = \frac{\partial l_i}{\partial u} \cdot \frac{\partial u}{\partial \phi_1} $$

각 항을 계산하면:
*   $\frac{\partial l_i}{\partial u} = 2u = 2(\phi_0 + \phi_1 x_i - y_i)$
*   $\frac{\partial u}{\partial \phi_1} = \frac{\partial}{\partial \phi_1} (\phi_0 + \phi_1 x_i - y_i) = x_i$

따라서,
$$ \frac{\partial l_i}{\partial \phi_1} = 2(\phi_0 + \phi_1 x_i - y_i) \cdot x_i = 2(\phi_0 + \phi_1 x_i - y_i)x_i $$
이것은 교재의 식 (6.7)에 해당한다.

### 1.2. 직접 전개 후 미분

손실 함수를 직접 전개한 후 미분해도 동일한 결과를 얻을 수 있다.
$l_i = (\phi_0 + \phi_1 x_i - y_i)^2 = \phi_0^2 + \phi_1^2 x_i^2 + y_i^2 + 2\phi_0\phi_1 x_i - 2\phi_0 y_i - 2\phi_1 x_i y_i$

#### 1.2.1. $\phi_0$에 대한 편미분
$$ \frac{\partial l_i}{\partial \phi_0} = 2\phi_0 + 2\phi_1 x_i - 2y_i = 2(\phi_0 + \phi_1 x_i - y_i) $$

#### 1.2.2. $\phi_1$에 대한 편미분
$$ \frac{\partial l_i}{\partial \phi_1} = 2\phi_1 x_i^2 + 2\phi_0 x_i - 2x_i y_i = 2x_i (\phi_1 x_i + \phi_0 - y_i) = 2(\phi_0 + \phi_1 x_i - y_i)x_i $$

두 방법 모두 동일한 결과를 제공한다.

### 1.3. 연쇄 법칙을 사용하는 이유

단순 선형 회귀의 경우 식이 간단하여 직접 전개 후 미분하는 것도 가능하지만, 일반적인 딥러닝 모델에서는 연쇄 법칙이 필수적이다.
1.  **일반성**: 딥러닝 모델의 손실 함수는 다수의 함수가 복잡하게 중첩된 형태를 가진다. 이 경우 함수를 일일이 전개하는 것은 매우 어렵거나 불가능하다. 연쇄 법칙은 이러한 복잡한 합성 함수의 미분을 체계적으로 계산할 수 있게 한다.
2.  **계산 효율성 및 자동화**: 현대 딥러닝 프레임워크(PyTorch, TensorFlow 등)의 자동 미분(Automatic Differentiation) 기능은 연쇄 법칙에 기반하여 그래디언트를 효율적으로 계산한다.
3.  **모듈성**: 각 함수(레이어, 활성화 함수 등)의 미분만 알면, 이를 조합하여 전체 모델의 그래디언트를 계산할 수 있다. 이는 모델 설계의 유연성을 높인다.

## 2. 볼록 함수(Convex Function)와 최적화

선형 회귀 문제의 손실 함수는 일반적으로 **볼록(convex)** 함수의 형태를 가진다.

### 2.1. 볼록 함수의 정의와 특징

볼록 함수는 기하학적으로 그래프가 아래로 오목한 그릇과 같은 모양을 갖는다. 더 공식적인 정의는 다음과 같다:

함수 $f$가 볼록 함수라면, 정의역 내의 임의의 두 점 $x_1, x_2$와 $t \in [0,1]$에 대해 다음 부등식이 성립한다.
$$ f(tx_1 + (1-t)x_2) \le tf(x_1) + (1-t)f(x_2) $$
이는 함수 곡면 위의 두 점을 잇는 **현(chord, 선분)**이 항상 함수 곡면 자체보다 위쪽에 놓여 있거나 곡면에 접하며, 함수 곡면과 교차하지 않음을 의미한다. 

![image-20250513212939494](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250513212939494.png)(예: 그림 6.1c의 손실 함수 등고선도)

볼록 함수의 중요한 특징은 다음과 같다.
*   **유일한 전역 최솟값(Global Minimum)**: 볼록 함수는 단 하나의 전역 최솟값을 가진다. 즉, 여러 개의 지역 최솟값(local minimum)에 빠질 위험이 없다.
*   **모든 지역 최솟값이 전역 최솟값**: 만약 어떤 지점이 지역 최솟값이라면, 그 지점은 반드시 전역 최솟값이다.

### 2.2. 볼록성과 경사 하강법

손실 함수가 볼록하다는 사실은 경사 하강법을 사용한 최적화에 다음과 같은 중요한 의미를 가진다.
1.  **안정적인 수렴**: 경사 하강법은 현재 파라미터 위치에서 계산된 그래디언트의 반대 방향으로 파라미터를 업데이트한다. 볼록 함수에서는 현재 지점이 전역 최솟값이 아니라면 항상 손실을 더 줄일 수 있는 "내리막길"이 존재하며, 경사 하강법은 이 길을 따라 전역 최솟값으로 안정적으로 수렴할 수 있다 (적절한 학습률이 주어졌을 때).
2.  **전역 최적해 보장**: 지역 최솟값에 갇힐 걱정 없이, 경사 하강법을 통해 도달한 최솟값은 전역 최솟값임을 보장받을 수 있다.

요약하자면, 선형 회귀의 손실 함수가 볼록 함수라는 특성 덕분에, 그래디언트 기반 최적화 방법을 통해 손실을 줄여나가는 방향으로 파라미터를 업데이트하면 유일한 최적해에 도달할 수 있다. 이는 딥러닝 모델의 복잡한 비볼록(non-convex) 손실 함수를 다룰 때와 비교하여 선형 회귀 문제의 최적화가 상대적으로 간단한 이유 중 하나이다.

그러면 비볼록 손실 함수를 다룰 떄는 어떤 식으로 돌아갈까? 즉 손실함수가 비선형함수일떄는?

# 비볼록 손실 함수와 최적화의 난관인 지역 최솟값

기계 학습 모델을 학습시키는 과정은 손실 함수를 최소화하는 최적의 파라미터를 찾는 여정과 같다. 선형 회귀 문제의 손실 함수는 특별한 성질을 가지고 있어 이 여정이 비교적 순탄하다. 그러나 대부분의 비선형 모델, 특히 신경망에서는 손실 함수의 형태가 복잡해지면서 여러 난관에 봉착하게 된다.

## 1. 선형 회귀와 볼록(Convex) 손실 함수: 보장된 최적해

선형 회귀 문제의 손실 함수(예: 평균 제곱 오차)는 일반적으로 **볼록(convex)** 함수의 형태를 띤다 (위의 그림 6.1c 참고). 볼록 함수는 기하학적으로 아래로 오목한 그릇 모양을 가지며, 다음과 같은 중요한 특징이 있다:

*   **단일 전역 최솟값(Global Minimum)**: 볼록 함수는 유일한 전역 최솟값을 가진다. 즉, 여러 개의 '웅덩이'(지역 최솟값)가 존재하지 않는다.
*   **현(Chord)의 성질**: 함수 곡면 위의 임의의 두 점을 잇는 선분(현)은 항상 함수 곡면보다 위쪽에 놓여 있거나 곡면에 접하며, 곡면을 가로지르지 않는다.

이러한 볼록성 덕분에, 경사 하강법(gradient descent)과 같은 최적화 알고리즘을 사용하면 파라미터의 초기 위치에 상관없이 항상 '내리막길'을 따라 전역 최솟값에 도달할 수 있다. 학습 과정이 실패할 가능성이 매우 낮다는 의미이다.

## 2. 비선형 모델과 비볼록(Non-convex) 손실 함수

안타깝게도, 대부분의 비선형 모델, 예를 들어 얕은 신경망(shallow networks)이나 깊은 신경망(deep networks)의 손실 함수는 **비볼록(non-convex)**이다. 비볼록 손실 함수는 전역 최솟값 외에도 다수의 **지역 최솟값(local minima)**을 가질 수 있어 최적화 과정을 훨씬 어렵게 만든다.

신경망 손실 함수의 시각화는 파라미터 수가 매우 많아 어렵기 때문에, 여기서는 두 개의 파라미터만 가진 간단한 비선형 모델인 **가버 모델(Gabor model)**을 통해 비볼록 손실 함수의 특징을 살펴보자.

### 2.1. 가버 모델 소개

가버 모델은 입력 $x$를 출력 $y$로 매핑하는 함수로, 다음과 같이 정의된다 :

$$ f(x, \phi) = \sin(\phi_0 + 0.06 \cdot \phi_1 x) \cdot \exp\left(-\frac{(\phi_0 + 0.06 \cdot \phi_1 x)^2}{32.0}\right) $$

이 모델은 두 가지 주요 구성 요소로 이루어져 있다:
1.  **사인(Sinusoidal) 구성 요소**: $\sin(\phi_0 + 0.06 \cdot \phi_1 x)$는 진동하는 함수 형태를 만든다.
2.  **음의 지수(Negative Exponential) 구성 요소**: $\exp\left(-\frac{(\phi_0 + 0.06 \cdot \phi_1 x)^2}{32.0}\right)$는 중심에서 멀어질수록 진폭이 감소하도록 만든다. (가우시안 함수의 형태와 유사)

가버 모델은 두 개의 파라미터 $\phi = [\phi_0, \phi_1]^T$를 가진다:
*   $\phi_0 \in \mathbb{R}$: 함수의 평균 위치(mean position)를 결정한다.
*   $\phi_1 \in \mathbb{R}^+$: 함수를 x축을 따라 늘리거나(stretch) 압축(squeeze)한다 (그림 6.2 참고).

![image-20250513225510436](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250513225510436.png)

### 2.2. 가버 모델의 최소 제곱 손실 함수

![image-20250513225533082](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250513225533082.png)

$I$개의 학습 예제 $\{ (x_i, y_i) \}$ (그림 6.3 참고)가 주어졌을 때, 가버 모델에 대한 최소 제곱 손실 함수는 다음과 같이 정의된다 :

$$ L(\phi) = \sum_{i=1}^{I} (f(x_i, \phi) - y_i)^2 $$

여기서도 목표는 이 손실 함수 $L(\phi)$를 최소화하는 파라미터 $\hat{\phi}$를 찾는 것이다.(여기서 샘플 데이터는 그냥 인풋 예시샘플 xi에서 uniformly하게 뽑아서 저기에 파라미터에 따르는 가버모델에 임의로 넣고 yi값을 뽑아내서 거기다 추가적으로 정규분포 노이즈를 추가해준거다, 이건 현실 데이터가 보통 완벽한 수학적인 형태로 측정되지않는다는 점을 고려한 노이즈다.)

## 3. 비볼록 손실 함수의 문제점: 지역 최솟값의 덫 (Local Minima)

![image-20250513225726747](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250513225726747.png)

그림 6.4a는 위에서 정의한 가버 모델과 학습 데이터셋에 대한 손실 함수 $L(\phi)$의 등고선을 보여준다. 선형 회귀의 볼록한 손실 함수와 달리, 이 손실 함수는 매우 복잡한 지형을 가지고 있음을 알 수 있다.

*   **다수의 지역 최솟값**: 그림에서 청록색 원(cyan circles)으로 표시된 지점들은 **지역 최솟값**이다. 이 지점들에서는 그래디언트가 0이며, 어떤 방향으로 움직여도 손실 값이 증가한다. 하지만 이들은 전체 함수에서 가장 낮은 손실 값을 가지는 지점(전역 최솟값)은 아니다.
*   **전역 최솟값**: 회색 원(gray circle)으로 표시된 지점이 이 손실 함수의 **전역 최솟값**이다. 이곳이 우리가 찾고자 하는 최적의 파라미터 조합에 해당한다.

만약 임의의 지점에서 경사 하강법을 시작하여 '내리막길'을 따라간다면, 전역 최솟값에 도달한다는 보장이 없다 (그림 6.5a 참고). 오히려 지역 최솟값 중 하나에서 알고리즘이 멈출 가능성이 높거나 더 높다. 더 큰 문제는, 일단 지역 최솟값에 도달하면 그곳이 최선인지, 아니면 다른 곳에 더 나은 해(더 낮은 손실 값)가 있는지 알 방법이 없다는 것이다.

이는 마치 안개가 자욱한 산에서 가장 낮은 골짜기를 찾으려고 할 때, 현재 발밑이 움푹 파여 더 이상 내려갈 곳이 없어 보인다고 해서 그곳이 산 전체에서 가장 낮은 지점이라고 확신할 수 없는 것과 유사하다. 경사 하강법은 현재 위치 주변의 정보만을 활용하기 때문에, 더 멀리 있는 더 좋은 해를 발견하지 못하고 가까운 지역 최솟값에 만족하게 될 수 있다.

