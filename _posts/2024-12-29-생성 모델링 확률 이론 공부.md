---
key:
title: '생성 모델링 확률 이론 공부'
excerpt: '확률 이론 기본적인 공부'
tags: [확률이론]
---

## 확률 밀도 함수

확률 밀도 함수란 어떠한 X집합(X = x1,x2,x3...)이 있을 떄, 특정 구간에서의 적분값이 확률이 되도록 하는 함수 $P(x)$, 표본 공간에 있는 모든 포인트에 대해 밀도 함수를 적분 했을 떄 1이 되어야 한다.

이 떄 관측 데이터 셋을 생성하는 실제 밀도 함수 $P_{data}(x)$ 는 하나지만, 이 밀도 함수를 추정하는데 사용할 수 있는 밀도 함수 $P_{model}(x)$는 무수히 많다, 즉 실제 밀도 함수를 구하는게 모델링 입장에선 베스트겠지만, 불가능 하기 떄문에 저 함수를 추정하는 밀도 함수 $P_{model}(x)$ 중에서 가장 원본과 가까운 함수를 구하고자, 관측 데이터 셋에 가장 가깝게 찍어 내는 모델 함수를 구하고자 하는거다.

```python
import numpy as np
import matplotlib.pyplot as plt

# Data generation
np.random.seed(42)

# Real data distribution (pdata(x))
real_data = np.random.normal(loc=0, scale=1, size=1000)

# Multiple model distributions (pmodel(x))
model1 = np.random.normal(loc=0.1, scale=1.1, size=1000)
model2 = np.random.normal(loc=-0.1, scale=0.9, size=1000)
model3 = np.random.normal(loc=0, scale=1.2, size=1000)

# Visualization
plt.figure(figsize=(15, 10))

# 1. Real data distribution
plt.subplot(2, 2, 1)
plt.hist(real_data, bins=50, density=True, alpha=0.7, color='blue')
plt.title('Real Data Distribution (pdata(x))\nmean=0, std=1')
plt.grid(True)

# 2. Model 1
plt.subplot(2, 2, 2)
plt.hist(model1, bins=50, density=True, alpha=0.7, color='red')
plt.title('Model 1 Distribution (pmodel1(x))\nmean=0.1, std=1.1')
plt.grid(True)

# 3. Model 2
plt.subplot(2, 2, 3)
plt.hist(model2, bins=50, density=True, alpha=0.7, color='green')
plt.title('Model 2 Distribution (pmodel2(x))\nmean=-0.1, std=0.9')
plt.grid(True)

# 4. Model 3
plt.subplot(2, 2, 4)
plt.hist(model3, bins=50, density=True, alpha=0.7, color='purple')
plt.title('Model 3 Distribution (pmodel3(x))\nmean=0, std=1.2')
plt.grid(True)

plt.tight_layout()
plt.show()

# Compare all distributions in one graph
plt.figure(figsize=(12, 6))
plt.hist(real_data, bins=50, density=True, alpha=0.5, color='blue', label='Real Data')
plt.hist(model1, bins=50, density=True, alpha=0.5, color='red', label='Model 1')
plt.hist(model2, bins=50, density=True, alpha=0.5, color='green', label='Model 2')
plt.hist(model3, bins=50, density=True, alpha=0.5, color='purple', label='Model 3')
plt.title('Comparison of All Distributions')
plt.legend()
plt.grid(True)
plt.show()
```

![image-20250101192426437](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250101192426437.png)

## 모수 모델링

그러한 안정적인, 가까운 $P_{model}(x)$를 찾는 데 사용하는 기법인 **모수 모델**이란 유한한 개수의 파라미터(무엇이 되었던 간에)를 사용해 기술 할 수 있는 밀도 함수, 즉 파라미터들을 가지고 어떤 확률 분포를 구하고자 하는 방법이다.

```py
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats

# Set random seed for reproducibility
np.random.seed(42)

class ParametricModeling:
    def __init__(self, data):
        self.data = data
        # Estimate parameters using MLE (Maximum Likelihood Estimation)
        self.mean = np.mean(data)
        self.std = np.std(data)
        
    def fit_normal(self):
        """Fit normal distribution to data"""
        return stats.norm(self.mean, self.std)
    
    def generate_samples(self, n_samples):
        """Generate new samples from fitted distribution"""
        return np.random.normal(self.mean, self.std, n_samples)
    
    def plot_comparison(self):
        """Plot original data vs fitted parametric model"""
        plt.figure(figsize=(12, 6))
        
        # Plot original data histogram
        plt.hist(self.data, bins=50, density=True, alpha=0.7, 
                color='blue', label='Original Data')
        
        # Plot fitted parametric model
        x = np.linspace(min(self.data), max(self.data), 100)
        fitted_dist = self.fit_normal()
        plt.plot(x, fitted_dist.pdf(x), 'r-', lw=2, 
                label=f'Fitted Normal (μ={self.mean:.2f}, σ={self.std:.2f})')
        
        plt.title('Parametric Modeling: Data vs Fitted Distribution')
        plt.xlabel('Value')
        plt.ylabel('Density')
        plt.legend()
        plt.grid(True)
        plt.show()
        
    def goodness_of_fit(self):
        """Perform Kolmogorov-Smirnov test for normality"""
        ks_statistic, p_value = stats.kstest(self.data, 'norm', 
                                           args=(self.mean, self.std))
        return {
            'KS Statistic': ks_statistic,
            'p-value': p_value,
            'Is Normal': p_value > 0.05
        }

# Generate example data (mixture of two normal distributions)
n_samples = 1000
data1 = np.random.normal(0, 1, n_samples // 2)
data2 = np.random.normal(3, 1.5, n_samples // 2)
mixed_data = np.concatenate([data1, data2])

# Create and fit parametric model
model = ParametricModeling(mixed_data)

# Plot the comparison
model.plot_comparison()

# Generate new samples
new_samples = model.generate_samples(1000)

# Compare original and generated distributions
plt.figure(figsize=(12, 6))
plt.hist(mixed_data, bins=50, density=True, alpha=0.7, 
         color='blue', label='Original Data')
plt.hist(new_samples, bins=50, density=True, alpha=0.7, 
         color='red', label='Generated Samples')
plt.title('Comparison: Original vs Generated Data')
plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.grid(True)
plt.show()

# Print goodness of fit results
fit_results = model.goodness_of_fit()
print("\nGoodness of Fit Test Results:")
for key, value in fit_results.items():
    print(f"{key}: {value}")
```

![image-20250101192439685](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250101192439685.png)

물론 그런다고 정확하게 예측되는게 아니기 떄문에, 저렇게 데이터가 예측이랑 빗나가는 경우도 있다. 그래서 비모수적으로 하는 방법도 있는 듯 하다.

## 가능도

파라미터 집합 $\theta$ 의 가능도는 $L($\theta$ | x)$ 는 관측된 포인트 x가 주어졌을 떄 $\theta$의 타당성을 측정하는 함수이다.
$$
L(\theta | x) = P_\theta(x)
$$
이렇게 정의 되는데, 대충 "모수가 얼마나 확률 밀도 함수 설명을 그럴듯 하게 하는가"를 측정하는 정도라고 생각하면 될 거 같다, 앞에서 모수 모델링을 구할 떄 쓰이는거라고 생각하면 될 거 같다.  **주어진 관측값이 특정 확률분포로부터 나왔을 확률 (= 연속확률밀도함수의 y 값)** 으로 어떤 값이 관측되었을 때, 해당 관측값이 어떤 확률분포로부터 나왔는지에 대한 확률이다. 확률의 반대 개념으로 고정되는 요소가 관측값 x이고 그걸 통해 확률분포가 여기서 나왔을 확률을 구하는, 이름 그대로 이런 값이 나오는게 어느정도로 가능해? 이런 느낌인거 같다. 그러므로 저 위의 개념이랑 합치면 확률밀도 함수를 구성하는 파라미터 $\theta$도 관측 포인트 x에 따라 구해지는 값이 되는거다. **가능도라는건 포인트집합들을 가지고 어떤 밀도함수가 어느정도 확률이 있을지에 대해 논하는거** 

그러므로 모수 모델링은 데이터셋 X가 관측 될 가능도를 최대화하는 파라미터 $\hat{\theta}$ 의 최적값을 구하는 것이다.

## 최대 가능도 추정

그렇다면 그 $\hat{\theta}$ 를 구하는 기법이 최대 가능도 추정인데, 여기서 $\hat{\theta}$는 관측된 데이터 X를 가장 잘 설명하는 밀도 함수의 파라미터 집합이다, 
$$
\hat{\theta} = \arg\max_{\theta} L(\theta|x)
$$
신경망은 손실 함수를 최소화하는 방향으로 파라미터들을 경사하강법으로 보통 업데이트 하기에, 음의 로그 가능도를 최소화하는 방향으로 피라미터 집합을 찾는 거라고 생각 할 수 있다,
$$
\hat{\theta} = \arg\min_{\theta} -\log L(\theta|x)
$$
여기서 파라미터가 모델에 담긴 신경망의 가중치이다. 즉 주어진 데이터를 관측할 가능도를. 최대화하는 (음의 로그 가능도를 최소화하는) 파라미터 값을 찾는 것이다.

```py
import numpy as np
from scipy.optimize import minimize
import matplotlib.pyplot as plt

class MaximumLikelihoodEstimation:
    def __init__(self, data):
        self.data = data
        
    def negative_log_likelihood(self, params):
        """
        음의 로그 가능도 계산
        params[0]: 평균(mu)
        params[1]: 표준편차(sigma)
        """
        mu, sigma = params
        
        # 정규분포의 로그 가능도
        log_likelihood = np.sum(
            -0.5 * np.log(2 * np.pi) 
            - np.log(sigma) 
            - 0.5 * ((self.data - mu) / sigma) ** 2
        )
        
        return -log_likelihood  # 최소화를 위해 음수 취함
    
    def fit(self):
        """
        최대 가능도 추정 수행
        """
        # 초기값 설정
        initial_guess = [np.mean(self.data), np.std(self.data)]
        
        # 최적화 수행
        result = minimize(
            self.negative_log_likelihood,
            initial_guess,
            method='Nelder-Mead'
        )
        
        return result.x
    
    def plot_likelihood_surface(self):
        """
        가능도 표면 시각화
        """
        mu_range = np.linspace(np.mean(self.data) - 2, np.mean(self.data) + 2, 100)
        sigma_range = np.linspace(np.std(self.data) * 0.5, np.std(self.data) * 1.5, 100)
        
        mu_grid, sigma_grid = np.meshgrid(mu_range, sigma_range)
        nll_grid = np.zeros_like(mu_grid)
        
        for i in range(len(mu_range)):
            for j in range(len(sigma_range)):
                nll_grid[j,i] = self.negative_log_likelihood([mu_grid[j,i], sigma_grid[j,i]])
        
        plt.figure(figsize=(12, 8))
        plt.contour(mu_grid, sigma_grid, nll_grid, levels=50)
        plt.colorbar(label='Negative Log-Likelihood')
        plt.xlabel('μ (mean)')
        plt.ylabel('σ (standard deviation)')
        plt.title('Negative Log-Likelihood Surface')
        
        # 최적값 표시
        mle_params = self.fit()
        plt.plot(mle_params[0], mle_params[1], 'r*', markersize=15, label='MLE')
        plt.legend()
        plt.show()

# 예제 데이터 생성
np.random.seed(42)
true_mean = 2.0
true_std = 1.5
sample_size = 1000
data = np.random.normal(true_mean, true_std, sample_size)

# MLE 수행
mle = MaximumLikelihoodEstimation(data)
estimated_params = mle.fit()

print(f"실제 파라미터: 평균={true_mean}, 표준편차={true_std}")
print(f"추정된 파라미터: 평균={estimated_params[0]:.3f}, 표준편차={estimated_params[1]:.3f}")

# 결과 시각화
plt.figure(figsize=(12, 6))
plt.hist(data, bins=50, density=True, alpha=0.7, label='Data')
x = np.linspace(min(data), max(data), 100)
plt.plot(x, 1/(estimated_params[1]*np.sqrt(2*np.pi)) * 
         np.exp(-0.5*((x-estimated_params[0])/estimated_params[1])**2),
         'r-', label='Estimated Distribution')
plt.xlabel('Value')
plt.ylabel('Density')
plt.title('Data and Estimated Distribution')
plt.legend()
plt.grid(True)
plt.show()

# 가능도 표면 시각화
mle.plot_likelihood_surface()
```

![image-20250101192814075](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250101192814075.png)

![image-20250101192832827](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250101192832827.png)

물론 고차원 문제에서 $P_{\theta}(x)$ 를 직접 계산하는건 매우 어렵기 때문에 다른 우회적인 방법들로 구한다.

