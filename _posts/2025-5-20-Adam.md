---
key:
title: 'Adam, 모멘텀'
excerpt: 'deeplearning'
tags: [인공지능]
---

# Adam: 적응적 모멘트 추정 최적화 알고리즘

Adam (Adaptive Moment Estimation)은 경사 하강법 기반의 최적화 알고리즘으로, 딥러닝 모델 학습에 널리 사용된다. 이 알고리즘은 두 가지 중요한 아이디어를 결합한다: 모멘텀(Momentum) 방식과 적응적 학습률(Adaptive Learning Rate) 방식 (특히 RMSProp과 유사한 접근). 이를 통해 각 파라미터마다 개별적인 학습률을 효과적으로 조정하며, 빠르고 안정적인 수렴을 목표로 한다.

## 핵심 구성 요소

Adam은 그래디언트의 1차 모멘트(평균)와 2차 모멘트(중심을 고려하지 않은 분산, 즉 제곱값의 평균)를 지수적으로 감쇠하는 이동 평균(exponentially decaying moving average)으로 추정한다.

### 1. 1차 모멘트 추정 (Momentum-like term): $m_t$

1차 모멘트 $m_t$는 과거 그래디언트들의 지수 이동 평균으로, 현재 그래디언트 방향뿐 아니라 과거의 진행 방향도 함께 고려하여 업데이트 방향을 결정한다. 이는 모멘텀 최적화 방식과 유사하게 작동하여, 그래디언트의 부호를 유지하며 진동을 줄이고 수렴 속도를 높이는 데 기여한다.

![image-20250519205747076](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250519205747076.png)

시점 $t$에서의 파라미터 $\phi_t$와 현재 미니배치 $\mathcal{B}_t$에 대한 손실 함수 $l_i$의 그래디언트 $\nabla_{\phi} l_i(\phi_t)$를 사용하여 다음과 같이 업데이트된다 (여기서는 미니배치 전체 그래디언트를 $g_t = \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} \nabla_{\phi} l_i(\phi_t)$로 표현):

$$
m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t
$$

여기서 $\beta_1$은 1차 모멘트의 지수 감쇠율로, 보통 0.9 정도의 값을 사용한다. $m_0$는 0으로 초기화된다.

### 2. 2차 모멘트 추정 (RMSProp-like term): $v_t$

2차 모멘트 $v_t$는 그래디언트 제곱값의 지수 이동 평균으로, 그래디언트의 변동성 또는 최근 그래디언트 크기에 대한 정보를 담는다. 이는 각 파라미터별로 학습률을 조절하는 데 사용된다. 그래디언트가 컸던 파라미터는 학습률을 줄이고, 작았던 파라미터는 학습률을 늘리는 방식으로 적응적인 학습이 가능해진다.
$$
v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2
$$

여기서 $g_t^2$는 현재 미니배치 그래디언트 $g_t$의 각 성분을 제곱한 것이다. $\beta_2$는 2차 모멘트의 지수 감쇠율로, 보통 0.999 정도의 값을 사용한다. $v_0$는 0으로 초기화된다.

## 초기 편향 보정 (Bias Correction)

학습 초기에는 $m_0=0$과 $v_0=0$으로 초기화되기 때문에, $m_t$와 $v_t$가 0에 가깝게 편향되는 경향이 있다. 특히 감쇠율 $\beta_1, \beta_2$가 1에 가까울 때 이 현상이 두드러진다. Adam은 이러한 초기 편향을 보정하기 위해 다음과 같은 수정을 가한다:

$$
\hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$

$$
\hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$

여기서 $t$는 현재 반복 횟수(iteration step)이다. 학습이 진행됨에 따라 $\beta_1^t$와 $\beta_2^t$는 0에 가까워지므로, 보정의 효과는 점차 사라진다.

## 파라미터 업데이트 규칙

편향이 보정된 1차 및 2차 모멘트 추정치를 사용하여 최종적으로 파라미터를 업데이트한다:

$$
\phi_t = \phi_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$

여기서:
- $\alpha$는 학습률(learning rate)이다.
- $\epsilon$은 분모가 0이 되는 것을 방지하기 위한 매우 작은 상수이다 (예: $10^{-8}$).

분모의 $\sqrt{\hat{v}_t}$ 항은 각 파라미터별로 학습률을 효과적으로 스케일링하는 역할을 한다. 즉, 과거 그래디언트 제곱이 컸던 (즉, $\hat{v}_t$가 큰) 파라미터는 실질적인 학습률이 작아지고, 작았던 파라미터는 실질적인 학습률이 커진다.

## Adam의 장점

-   **계산 효율성**
-   **메모리 요구량 적음**
-   **파라미터별 적응적 학습률**: 각 파라미터에 대해 서로 다른 학습률을 적용하여 학습 과정을 개선한다.
-   **다양한 문제에 적합**: 고정적이지 않은 목적 함수(non-stationary objectives)나 매우 희소한 그래디언트(sparse gradients)를 가진 문제에도 잘 작동하는 경향이 있다.
-   **초기 학습률 설정에 덜 민감**: 다른 최적화 알고리즘에 비해 초기 학습률 값에 덜 민감한 편이다.

이러한 특징들로 인해 Adam은 현재 딥러닝 분야에서 가장 널리 사용되는 최적화 알고리즘 중 하나로 자리매김했다.

# $m_t$와 $v_t$는 어떻게 학습을 조율하는가?

Adam 알고리즘의 핵심은 1차 모멘트 추정치 $m_t$와 2차 모멘트 추정치 $v_t$를 사용하여 파라미터 업데이트를 정교하게 조율하는 데 있다. 이 두 추정치가 각각 어떤 정보를 담고 있으며, 어떻게 상호작용하여 Adam의 강력한 성능을 이끌어내냐면

## $m_t$: 업데이트의 방향과 기본적인 추진력 - 어디로, 얼마나 세게 갈 것인가?

$m_t$는 그래디언트 자체의 지수 이동 평균이다:
$$m_t = \beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$$

여기서 $g_t$는 현재 미니배치의 그래디언트이다.

-   **정보의 본질**: $m_t$는 벡터 연산으로, 그래디언트의 **방향**과 **크기(세기)** 정보를 모두 포함한다. 과거 그래디언트들의 평균적인 방향과 세기를 기억하고, 현재 그래디언트와 혼합하여 다음 업데이트의 기본적인 방향과 추진력을 결정한다.
-   **모멘텀의 역할**: 만약 현재 미니배치의 그래디언트 $g_t$가 이전의 $m_{t-1}$과 유사한 방향을 가리킨다면, $m_t$는 그 방향으로 더 큰 추진력을 갖게 된다. 반대로, $g_t$가 이전과 다른 방향을 가리키거나 그래디언트들이 서로 상쇄되면, $m_t$의 크기는 작아져 업데이트 강도가 약해진다. 이는 마치 언덕을 내려오는 공처럼, 일관된 경사에서는 가속도가 붙고, 평지나 불규칙한 지형에서는 속도가 줄어드는 것과 유사하다.
-   **"평균적으로 얼마나 세게"**: $m_t$의 크기(norm)는 그래디언트의 평균적인 세기를 나타낸다. 하지만 중요한 것은 이 세기가 방향성을 동반한다는 점이다. 따라서 $m_t$는 "어느 방향으로, 평균적으로 얼마나 세게 나아갈지"에 대한 종합적인 정보를 제공한다.

## $v_t$: 그래디언트 크기의 변동성 이력 - 이 파라미터는 과거에 얼마나 요란하게 움직였나?

$v_t$는 그래디언트 **제곱값**의 지수 이동 평균이다:
$$v_t = \beta_2 \cdot v_{t-1} + (1 - \beta_2) \cdot g_t^2$$

-   **정보의 본질**: $g_t^2$는 그래디언트 벡터 $g_t$의 각 성분을 제곱한 것이다. 이 제곱 연산으로 인해 $v_t$는 그래디언트의 **방향 정보를 완전히 잃어버리고, 오직 크기의 제곱에 대한 정보**만을 담게 된다. 즉, 그래디언트가 $+5$였든 $-5$였든, 제곱하면 $25$가 되어 $v_t$에 동일한 크기의 영향을 미친다.
-   **변동성의 척도**: $v_t$는 각 파라미터에 대한 그래디언트가 과거에 평균적으로 얼마나 "컸었는지" 또는 "작았었는지"를 나타낸다. 이 값이 크다는 것은 해당 파라미터의 그래디언트가 (양의 방향이든 음의 방향이든 상관없이) 지속적으로 큰 값을 가져왔음을 의미하며, 이는 손실 함수 표면에서 해당 파라미터 축 방향으로 변화가 "가파르거나 변동이 심했음"을 말한다. 반대로 $v_t$가 작다면 변화가 "완만했음"을 의미한다.
-   **제곱의 효과와 RMS와의 유사성**: 제곱을 통해 큰 그래디언트 값은 $v_t$에 더욱 큰 영향을 미친다. $\sqrt{v_t}$는 그래디언트 크기의 RMS(Root Mean Square, 제곱-평균-제곱근)와 유사한 개념으로, 신호의 평균적인 진폭이나 에너지 크기를 나타내는 지표로 해석될 수 있다. 즉, "이 파라미터의 그래디언트는 평균적으로 이 정도 크기의 스케일을 갖는다"라는 정보를 제공한다.

## $m_t$와 $v_t$의 협력

Adam의 파라미터 업데이트 규칙은 다음과 같다 (편향 보정된 값을 사용):
$$\phi_t = \phi_{t-1} - \alpha \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$

여기서 $\hat{m}_t$와 $\hat{v}_t$는 각각 $m_t$와 $v_t$의 편향 보정된 버전이다.

-   **독립적인 정보, 협력적인 업데이트**: $m_t$와 $v_t$는 현재 그래디언트($g_t$ 또는 $g_t^2$)와 각자의 과거 이력($m_{t-1}$ 또는 $v_{t-1}$)을 바탕으로 **서로 독립적으로 계산된다.** $m_t$가 $v_t$의 계산에 영향을 주거나, 그 반대의 경우는 발생하지 않는다.
-   **업데이트 메커니즘**:
    1.  **기본 방향 및 추진력 제공 ($\hat{m}_t$)**: 분자의 $\hat{m}_t$가 업데이트의 기본적인 방향과 세기를 결정한다.
    2.  **학습률 스케일링 ($\sqrt{\hat{v}_t} + \epsilon$)**: 분모의 $\sqrt{\hat{v}_t} + \epsilon$ 항이 각 파라미터별로 이 기본적인 추진력을 조절한다.
        -   만약 특정 파라미터에 대해 $\hat{v}_t$가 크다면 (즉, 과거 그래디언트가 컸다면), 분모가 커져서 해당 파라미터의 실질적인 학습률 ($\alpha / (\sqrt{\hat{v}_t} + \epsilon)$)은 작아진다. 이는 "과거에 크게 움직였으니, 이번에는 좀 더 조심스럽게 가자"는 의미이다.
        -   반대로 $\hat{v}_t$가 작다면 (즉, 과거 그래디언트가 작았다면), 분모가 작아져 실질적인 학습률은 커진다. 이는 "과거에 거의 안 움직였으니, 이번에는 좀 더 과감하게 가보자"는 의미이다.

-   **결론적 역할 분담**:
    -   $\hat{m}_t$: "어디로 갈 것인가? 기본적인 힘은 어느 정도인가?"
    -   $\sqrt{\hat{v}_t}$: "그곳으로 갈 때, 과거의 경험(가파름/변동성)에 비추어 보폭은 얼마나 조절해야 하는가?"

결국 Adam은 "평균적인 진행 방향과 세기"(\(\hat{m}_t\))와 "각 파라미터별 과거 그래디언트 크기의 변동성 이력"(\(\sqrt{\hat{v}_t}\))이라는 두 가지 서로 다른 종류의 통계 정보를 지능적으로 결합하여, 각 파라미터의 상황에 맞게 학습 속도와 방향을 효과적으로 제어하는 것이다. 이로 인해 손실 함수의 복잡한 지형에서도 안정적이고 효율적인 탐색이 가능해진다.