---
key:
title: 'PixelCNN 이해'
excerpt: 'PixelCNN, Auto-Regressive Model, Masked Convolution'
tags: [컴퓨터비전, 생성모델]
---

내용은 [Github: Understanding PixelCNN](https://github.com/demul/basic_idea/blob/master/Understanding%20PixelCNN/understanding_pixelcnn.md)를 참고하였습니다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/pixelrnn4.png)

> **자기 회귀(Auto-Regressive) 모델**은 자신의 결과물을 다시 자신의 input으로 재귀시키는 모델로서 생성 모델 도메인에서는 보편적으로 PixelRNN, PixelCNN과 그 후속 파생 모델들을 일컫는다.
> 영상을 만들 때 한번에 처리하지 않고 한 픽셀, 한 채널 따로따로 순서대로 만든다. **각 픽셀을 만들 땐 지금까지 생성한 픽셀들이 입력으로 들어가 context(문맥)가 된다.**

여기서 대문자 $X$는 영상, 그리고 $n^2$은 이미지에서 2D를 생성한다고 했을 때 $width \times height$일 것이고, 소문자 $x$는 각 픽셀이다.
$p(x_i | x_1, x_2, ..., x_{i-1})$는 이전 픽셀들이 주어졌을 때 $i$번째 픽셀의 조건부 확률이라는 말이다.

즉, $p(x_i)$는 0번째 픽셀부터 $i-1$까지의 이미 만들어진 픽셀들이 **prior**로 주어졌을 때, $i$번째 픽셀의 확률을 나타낸다. 그 $i$가 1부터 $n^2$까지 반복하는 각 항을 모두 곱한 게 영상 $X$의 확률이라는 의미로, **"모델이 이 픽셀에서 만들 값은 지금까지의 픽셀들의 값에 영향을 받는다"**라는 부분이 표현된 수식이다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/survey2.png)

그렇기에 각 픽셀이 지금까지의 픽셀들을 참고해 만드니까 결과물의 데이터 분포를 **Log Likelihood**로 명시적으로 파악할 수 있으며, 매우 안정적인 학습이 가능하다. 또한 한 픽셀 한 픽셀을 확률분포에서 샘플링해가며 만드는 특성상, 높은 **Consistency(일관성)**와 **Multimodality(다양성)**를 보여준다.

여기서 로그 가능도(Log Likelihood)라는 말은 확률이 계산하기 쉬운 형태로 바뀐다는 말로, 아래와 같이 표현된다.

$$ \log p(x) = \sum_{i=1}^{n^2} \log p(x_i | x_1, x_2, ..., x_{i-1}) $$

이런 식으로 양변에 로그만 취하고 모델의 성능을 평가할 수 있다. 그렇다는 말은 모델 간의 성능을 비교할 수도 있고, 모델에게 로그 가능도를 최대화하는 방향으로 학습시키면 된다는 뜻이다.

$$ \max_\theta \sum_{x \in D} \log p_\theta(x) $$
*(여기서 $\theta$는 모델 파라미터, $D$는 학습 데이터셋)*

이 목적 함수는 가능도를 직접 최대화하므로, GAN에서 Minimax 게임을 해서 학습하는 것과 달리 **단일 목적 함수**를 가진다. 그렇기 때문에:

1.  명확한 단일 목적 함수가 있어 **수렴이 보장됨**
2.  GAN에서 발생할 수 있는 **Mode Collapse(모드 붕괴)** 문제가 없음
3.  학습 과정이 안정적이어서 **하이퍼파라미터 튜닝에 덜 민감함**

이러한 **안정적인 학습 장점**이 있고, $x_i \sim p(x_i | x_1, x_2, ..., x_{i-1})$ 각 픽셀은 조건부 확률 분포에서 샘플링하기 때문에 모델 파라미터가 고정되면 완전히 결정적으로 확률 분포가 나온다. 즉, 동일한 이전 픽셀 값들이 주어지면 항상 동일한 조건부 확률 분포가 나오기에 결과가 예측되는 **일관성**을 가지게 된다.

1.  생성된 이미지의 구조적 특성이 일관되게 유지
2.  이미지 전체에 걸쳐 일관된 구조와 스타일을 가진 이미지가 생성
3.  명확한 확률 모델을 따르므로 결과가 예측 가능

그리고 그 확률분포에서 뽑는 건 확률이기 때문에 다양성이 존재한다.

$$ p_T(x_i | x_{<i}) \propto p(x_i | x_{<i})^{1/T} $$

*(여기서 $T$는 temperature 파라미터. 1보다 커지면 확률 분포가 평탄해져 다양성이 증가하고, 1보다 작아지면 뾰족해져 더 결정적인 샘플링을 함)*

즉 동일한 모델로 다양한 이미지를 생성이 가능한데, 여기서 위에 말한 일관성은 **확률분포가 일관적**이라는 거고, 거기서 뽑는 샘플링 과정에서 **다양성**이 생긴다는 말이다.

하지만 순차적으로 생성해야 하기에 처리 시간이 오래 걸려 **실시간 처리에 단점**이 있다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/pixelrnn1.png)

> 위 사진은 PixelCNN, Row LSTM, Diagonal BiLSTM이 픽셀 간 Dependency를 구현하는 형태를 나타낸 것이다.

**Row LSTM**은 이미지를 행 단위로 처리하는 자기회귀 모델이다. 이 모델은 각 픽셀을 예측할 때 같은 행의 왼쪽 픽셀들과 위쪽 행들의 모든 픽셀 정보를 사용한다.

### 1. Row LSTM: 행 기반 이미지 생성
이름 그대로 이미지를 **행(Row) 단위**로 처리하는 방식이다. 위에서 아래로, 각 행 내에서는 왼쪽에서 오른쪽으로 순차적으로 연산한다.

* **작동 원리:**
    * **i-s (Input-to-State):** 3x1 마스크를 사용하여 **현재 픽셀의 위쪽 정보**를 입력으로 받는다. (Vertical dependency)
    * **s-s (State-to-State):** 별도의 마스크 없이 **같은 행의 바로 왼쪽 픽셀**의 상태(State) 정보를 활용한다. (Horizontal dependency)
* **특징:** 수평 방향의 문맥을 효과적으로 학습할 수 있고, 행 단위 연산 덕분에 일부 병렬화가 가능해서 계산 효율성도 어느 정도 확보된다.

### 2. Diagonal BiLSTM: 대각선 기반 이미지 생성
Row LSTM이 일부 문맥을 놓치는 한계를 보완하기 위해 고안된 모델이다. 이미지를 **대각선 방향**으로 처리하여 더 넓은 범위의 종속성을 포착한다.

* **작동 원리:**
    * 이미지를 왼쪽 위에서 오른쪽 아래 대각선 방향으로 처리한다.
    * **양방향(Bidirectional):** 각 대각선에서 정방향(↙↗)과 역방향(↗↙) 두 번을 훑어서 정보를 수집한다.
    * **i-s:** 1x1 마스크를 사용하여 **직전 대각선**의 픽셀 정보를 입력으로 받는다.
    * **s-s:** 마스크 없이 **현재 대각선 내의 이전 픽셀** 상태 정보를 활용한다.
* **특징:** 대각선 처리와 양방향 LSTM을 결합해 복잡한 공간적 의존성을 잘 잡아내므로 생성 품질이 우수하다. 하지만 구조가 복잡해 계산 비용이 높다는 단점이 있다.

LSTM에서 이미지를 먼저 처리하기 위해 PixelCNN이 나오기 전에 나왔던 모델들로, 이 논문의 저자는 저 2가지 방법과 PixelCNN까지 3가지 방법을 서로 trade-off하면서 적절한 모델을 골라 쓸 수 있게 하였다.

> 이 논문에서 설명하는 모델의 Task는 맨 왼쪽, 맨 위, R픽셀에 랜덤한 값을 준 뒤, 이것으로부터 시퀸셜하게 한 픽셀 한 픽셀씩 Likelihood가 높은 값들만 선택해가면서 그럴듯한(ImageNet이나 CIFAR10 데이터들과 같은 분포를 가지는) 영상을 만들어 내는 것이다.

> **Evaluation (NLL 측정)**
> 완전한 이미지 하나를 준 뒤, 각 픽셀을 예측할 때 이전의 픽셀들이 모두 올바르게 예측되었을 것이라고 가정하고 Context로 이 완전한 Image(즉 Label)의 일부를 사용한다. (**Teacher Forcing**)
> 그리고 이렇게 예측된 픽셀 값을 Label의 픽셀 값과 비교한 **Cross-Entropy-Error**를 구한 뒤 저장하고, 다음 픽셀을 예측할 땐 다시 Label의 픽셀값을 사용한다.

**1. Teacher Forcing 기법 (답지 보고 풀기)**
순차적 모델을 훈련할 때 쓰는 기법으로, 모델이 이전 시점에 자기가 예측한 값(뇌피셜)을 다음 입력으로 쓰는 게 아니라, **실제 정답(Ground Truth)**을 다음 입력으로 넣어주는 방식이다.

**2. 픽셀별 확률 계산 및 NLL 도출**
모델이 뱉은 예측값이랑 실제 Label이랑 비교해서 **Cross Entropy(교차 엔트로피)**로 에러값을 구한다. 이렇게 구한 모든 픽셀의 에러값을 싹 다 더해서 전체 픽셀 수(차원 수)로 나눈 게 바로 **NLL(Negative Log-Likelihood)**이다.

```python
def evaluate(model, test_loader, device):
    model.eval()
    test_loss = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # 모델 출력 (Teacher Forcing 방식)
            output = model(data, target if model.n_classes else None)
            
            # 채널별 손실 계산
            loss = 0
            for c in range(data.shape[1]):  # 각 채널에 대해
                # 타겟 이미지 (0-255 정수)
                target_img = (data[:, c] * 255).long()
                output_c = output[:, c]  # [batch, height, width, 256]
                
                # 크로스 엔트로피 손실 계산
                loss += F.cross_entropy(
                    output_c.reshape(-1, 256),  # 각 픽셀의 256가지 값에 대한 확률
                    target_img.reshape(-1)      # 실제 픽셀 값 (0-255)
                )
            
            loss /= data.shape[1]  # 채널 수로 나누어 정규화
            test_loss += loss.item()
    
    return test_loss / len(test_loader)
```

여기서 PyTorch의 `cross_entropy()`는 logits(원시값)에다가 **Softmax + Log + Negative**를 내부적으로 모두 처리한다.

```python
# 네 번째 셀: 모델 구현 (Masked Convolution)
class MaskedConv2d(nn.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size, mask_type='A', stride=1, padding='same', **kwargs):
        if padding == 'same':
            padding = kernel_size // 2
        super(MaskedConv2d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, **kwargs)
        self.register_buffer('mask', torch.ones_like(self.weight))
        
        # 마스크 생성
        _, _, h, w = self.weight.size()
        self.mask[:, :, h//2, w//2 + 1:] = 0  # 오른쪽 마스킹
        self.mask[:, :, h//2 + 1:, :] = 0     # 아래쪽 마스킹
        
        # A 타입 마스크는 현재 픽셀도 마스킹 (B 타입은 현재 픽셀 포함)
        if mask_type == 'A':
            self.mask[:, :, h//2, w//2] = 0
            
    def forward(self, x):
        self.weight.data *= self.mask  # 가중치에 마스크 적용
        return super(MaskedConv2d, self).forward(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, mask_type='B'):
        super(ResidualBlock, self).__init__()
        self.conv1 = MaskedConv2d(in_channels, out_channels, kernel_size=3, mask_type=mask_type)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = MaskedConv2d(out_channels, out_channels, kernel_size=3, mask_type=mask_type)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # 채널 차원이 다를 경우 1x1 컨볼루션으로 맞춤
        if in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.skip = nn.Identity()
        
    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.skip(residual)
        out = self.relu(out)
        return out

class PixelCNN(nn.Module):
    def __init__(self, input_channels=3, hidden_channels=128, n_residual=15, n_classes=None, kernel_size=7):
        super(PixelCNN, self).__init__()
        self.n_classes = n_classes
        self.input_channels = input_channels
        
        # 조건부 임베딩 (있을 경우)
        if n_classes:
            self.embedding = nn.Embedding(n_classes, hidden_channels)
        
        # 첫 번째 레이어는 'A' 타입 마스크 사용 (자기 자신을 보지 못하게 함)
        self.conv_in = MaskedConv2d(input_channels, hidden_channels, kernel_size=kernel_size, mask_type='A')
        self.bn_in = nn.BatchNorm2d(hidden_channels)
        
        # 레지듀얼 블록들
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_channels, hidden_channels, mask_type='B')
            for _ in range(n_residual)
        ])
        
        # 출력 레이어
        self.relu = nn.ReLU(inplace=True)
        self.conv_out1 = MaskedConv2d(hidden_channels, hidden_channels, kernel_size=3, mask_type='B')
        self.bn_out1 = nn.BatchNorm2d(hidden_channels)
        self.conv_out2 = MaskedConv2d(hidden_channels, hidden_channels, kernel_size=3, mask_type='B')
        self.bn_out2 = nn.BatchNorm2d(hidden_channels)
        self.conv_out3 = MaskedConv2d(hidden_channels, input_channels * 256, kernel_size=1, mask_type='B')
        
    def forward(self, x, label=None):
        # [0, 255] 범위를 [0, 1]로 정규화
        if x.max() > 1:
            x = x / 255.0
        
        batch_size, _, height, width = x.shape
        
        # 초기 합성곱
        out = self.relu(self.bn_in(self.conv_in(x)))
        
        # 조건부 임베딩 추가 (있을 경우)
        if self.n_classes and label is not None:
            label_emb = self.embedding(label).view(batch_size, -1, 1, 1)
            out = out + label_emb
        
        # 레지듀얼 블록
        for block in self.residual_blocks:
            out = block(out)
        
        # 출력 레이어
        out = self.relu(self.bn_out1(self.conv_out1(out)))
        out = self.relu(self.bn_out2(self.conv_out2(out)))
        out = self.conv_out3(out)
        
        # 출력 형태 변환: [batch, channels*256, height, width] -> [batch, channels, height, width, 256]
        out = out.view(batch_size, self.input_channels, 256, height, width)
        out = out.permute(0, 1, 3, 4, 2)  # [batch, channels, height, width, 256]
        
        return out
```