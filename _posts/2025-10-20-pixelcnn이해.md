---
key:
title: 'pixelcnn 이해'
excerpt: 'pixelcnn'
tags: [컴퓨터비전]
---

내용은 https://github.com/demul/basic_idea/blob/master/Understanding%20PixelCNN/understanding_pixelcnn.md를 참고하였습니다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/pixelrnn4.png)

> 자기 회귀 모델은 자신의 결과물을 다시 자신의 input으로 재귀시키는 모델로서 생성모델, 도메인에서는 Auto-Regressive Model은 보편적으로 PixelRNN, PixelCNN과 그 후속, 파생모델들 따위를 일컫는다. 영상을 만들 떄 한번에 처리하지 않고 한 픽셀, 한 채널, 따로 따로 순서대로 만든다, **각 픽셀을 만들 떈 지금까지 생성한 픽셀들의 입력으로 들어가 context(문맥)이 된다**
>


여기서 대문자 x는 영상, 그리고 n은 제곱이니까 이미지에서 2d를 생성한다고 했을 떄 width와 height 일꺼고 소문자 x는 각 픽셀픽셀이다, $p(x_i | x_1, x_2, ..., x_{i-1})$는 이전 픽셀들이 주어졌을 때 $i$번째 픽셀의 조건부 확률이라는 말로, 즉 p(x_i)는 0번째 픽셀부터 i-1까지의 이미 만들어진 픽셀들이 prior로 주어졌을때, i번째 픽셀의 확률을 나타낸다,  그 i가 1부터 n제곱까지 반복하는걸 각 항을 모두 곱한게 영상 x의 확률이라는 의미로, 수식으로 **모델이 이 픽셀에서 만들 값은 지금까지의 픽셀들의 값에 영향을 끼친다**라는 부분이 표현된 수식이다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/survey2.png)

그렇기에 각 픽셀이 지금까지의 픽셀들을 참고해 만드니까 결과물의 데이터 분포를 Log Likelihood로 명시적으로 파악할 수 있으며, 매우 안정적인 학습이 가능하고, 한 픽셀 한 픽셀을 확률분포에서 샘플링해가며 만드는 특성 상, 높은 Consistency(일관성)와 Multimodality(다중성)를 보여준다. 

여기서 로그 가능도라는 말은 확률이 계산하기 쉬운 형태로 바뀐다는 말로, $$\log p(x) = \sum_{i=1}^{n} \log p(x_i | x_1, x_2, ..., x_{i-1})$$ 이런식으로 양변에 로그만 취하고 모델의 성능을 평가할 수 있다, 그렇다는 말은 모델간의 성능을 비교할 수도 있고, 모델에게 로그 가능도를 최대화 하는 방향으로 $$\max_\theta \sum_{x \in D} \log p_\theta(x)$$ (세타는 모델 파라미터, D는 학습 데이터셋) 하면 되기 떄문에, 이 목적 함수는 가능도를 직접 최대화 하므로, gan에서 minimax게임을 해서 학습한다거나 하는거랑 달리 단일 목적 함수를 가진다, 그렇기 떄문에 

1. 명확한 단일 목적 함수가 있어 수렴이 보장

1. GAN에서 발생할 수 있는 모드 붕괴(mode collapse) 문제가 없음

1. 학습 과정이 안정적이어서 하이퍼파라미터 튜닝에 덜 민감

이러한 **안정적인 학습 장점**이 있고 $$x_i \sim p(x_i | x_1, x_2, ..., x_{i-1})$$ 각 픽셀은 조건부 확률 분포에서 샘플링 하기 때문에 모델 파라미터가 고정되면 완전히 결정적으로 확률 분포가 나온다, 즉, 동일한 이전 픽셀 값들이 주어지면 항상 동일한 조건부 확률 분포가 나오기에 결과가 예측 되는 **일관성** 을 가지게 된다. 

1. 생성된 이미지의 구조적 특성이 일관되게 유지

1. 전 이미지 전체에 걸쳐 일관된 구조와 스타일을 가진 이미지가 생성

1. 명확한 확률 모델을 따르므로 결과가 예측 가능

그리고 그 확률분포에서 뽑는건 확률이기 떄문에 $$p_T(x_i | x_{<i}) \propto p(x_i | x_{<i})^{1/T}$$ (여기서 T는 temperature 파라미터, 1보다 커지면 확률 분포가 평탄해져 다양성이 증가, 1보다 작아지면 뾰족해져 더 결정적인 샘플링을 함)에 따라 다양하게 뽑을 수 있는 **다양성**이 존재한다. 즉 동일한 모델로 다양한 이미지를 생성이 가능한데, 여기서 위에 말한 일관성은 **확률분포**가 일관적이라는 거고 거기서 뽑는 게 다양하다는걸 다양하다라고 말하는 거다.

하지만 그렇기에 처리시간이 오래 걸리기떄문에 실시간 처리에 단점이 있다.

![img](https://github.com/demul/basic_idea/raw/master/Understanding%20PixelCNN/images/pixelrnn1.png)

> 위 사진은 PixelCNN, Row LSTM, Diagonal BiLSTM이 픽셀간 Dependency를 구현하는 형태를 나타낸 것이다.

Row LSTM은 이미지를 행 단위로 처리하는 자기회귀 모델이다. 이 모델은 각 픽셀을 예측할 때 같은 행의 왼쪽 픽셀들과 위쪽 행들의 모든 픽셀 정보를 사용한다.

- ### 1. Row LSTM: 행 기반 이미지 생성

  이름 그대로 이미지를 **행(Row) 단위**로 처리하는 방식이다. 위에서 아래로, 각 행 내에서는 왼쪽에서 오른쪽으로 순차적으로 연산한다.

  - **작동 원리:**
    - **i-s (Input-to-State):** 3x1 마스크를 사용하여 **현재 픽셀의 위쪽 정보**를 입력으로 받는다. (Vertical dependency)
    - **s-s (State-to-State):** 별도의 마스크 없이 **같은 행의 바로 왼쪽 픽셀**의 상태(State) 정보를 활용한다. (Horizontal dependency)
  - **특징:** 수평 방향의 문맥을 효과적으로 학습할 수 있고, 행 단위 연산 덕분에 일부 병렬화가 가능해서 계산 효율성도 어느 정도 확보된다.

  ### 2. Diagonal BiLSTM: 대각선 기반 이미지 생성

  Row LSTM이 일부 문맥을 놓치는 한계를 보완하기 위해 고안된 모델이다. 이미지를 **대각선 방향**으로 처리하여 더 넓은 범위의 종속성을 포착한다.

  - **작동 원리:**
    - 이미지를 왼쪽 위에서 오른쪽 아래 대각선 방향으로 처리한다.
    - **양방향(Bidirectional):** 각 대각선에서 정방향(↙↗)과 역방향(↗↙) 두 번을 훑어서 정보를 수집한다.
    - **i-s:** 1x1 마스크를 사용하여 **직전 대각선**의 픽셀 정보를 입력으로 받는다.
    - **s-s:** 마스크 없이 **현재 대각선 내의 이전 픽셀** 상태 정보를 활용한다.
  - **특징:** 대각선 처리와 양방향 LSTM을 결합해 복잡한 공간적 의존성을 잘 잡아내므로 생성 품질이 우수하다. 하지만 구조가 복잡해 계산 비용이 높다는 단점이 있다.

lstm에서 이미지를 먼저 처리하기위해 pixelcnn이 나오기 전에 나왔던 모델들로 이 논문의 저자는 저 2가지 방법과 pixelcnn까지 3가지 방법을 서로 trade-off하면서 적절한 모델을 골라 쓸 수 있게 하였다.

여기서 i-s는 input to state로 입력값에서 현재값으로 미치는 영향값을 표현한거고 s-s는 state to state로 현재값(이미지)에서 서로 영향을 끼치는거를 말하는대 row는 바로 현재 다루는 픽셀 옆에있는 픽셀값을 의미하는거고 diagonal은 현재 대각선의 픽셀값이 영향을 끼친다, 여기서 no mask라는 걸로, 즉 row에서는 이 영역 내의 모든 픽셀(현재 처리 중인 픽셀 왼쪽에 있는)이 마스킹 없이 그대로 상태 정보에 기여한다는 의미이고 diagonal도 마스킹없이 그대로 이전상태값을 활용한다는 말이다.

> 이 논문에서 설명하는 모델의 Task는 맨 왼쪽, 맨 위, R픽셀에 랜덤한 값을 준 뒤, 이것으로 부터 시퀸셜하게 한 픽셀 한 픽셀씩 Likelihood가 높은 값들만 선택해가면서 그럴듯한(ImageNet이나 CIFAR10 데이터들과 같은 분포를 가지는) 영상을 만들어 내는 것이다.

> NLL을 측정할 때(Evaluation)는, 완전한 이미지 하나를 준 뒤, 각 픽셀을 예측할 때, 이전의 픽셀들이 모두 올바르게 예측되었을 것이라고 가정하고 Context로 이 완전한 Image(즉 Label)의 일부를 사용한다.(일종의 Teacher Forcing 느낌) 그리고 이렇게 예측된 픽셀 값을 Label의 픽셀 값과 비교한 Cross-Entropy-Error를 구한 뒤 저장하고, 예측한 픽셀값은 버리고, 다음 픽셀을 예측할 땐 다시 Label의 픽셀값을 사용한다. 이렇게 저장한 Cross-Entropy-Error를 모두 더한 값을 차원수(CIFAR10의 경우 32x32x3)로 나눈 값을 NLL로 한다.

NLL은 Negative Log Likehood로 로그 가능도가 음수이므로, 이를 양수로 변환하여 손실 함수(Loss)로 쓰기 위해 마이너스를 붙인 것이고

**1. Teacher Forcing 기법 (답지 보고 풀기)**

먼저 **Teacher Forcing**이란 순차적 모델을 훈련할 때 쓰는 기법인데, 모델이 이전 시점에 자기가 예측한 값(뇌피셜)을 다음 입력으로 쓰는 게 아니라, **실제 정답(Ground Truth)**을 다음 입력으로 넣어주는 방식이다.

일반적인 자기회귀(Auto-Regressive) 모델은 원래 자기 예측값을 계속 입력으로 쓰면서 가는데, 이렇게 평가하거나 훈련할 때는 오류가 눈덩이처럼 불어나는 걸 막기 위해 정답을 강제로 주입한다.

- 이미지 왼쪽 상단부터 순차적으로 처리한다고 치면,
- 픽셀 $(i, j)$를 예측할 때, 실제 이미지의 $(0,0)$부터 $(i, j-1)$까지의 **진짜 픽셀값**을 컨텍스트로 쓴다.
- 이전에 내가 예측한 픽셀값은 버린다.
- 이 짓을 모든 픽셀에 대해 반복한다.

그니까 지금 여기선, **"이전 픽셀들이 다 맞았다고 쳤을 때, 과연 이번 픽셀은 맞출 수 있나?"**를 보는 거다. 모델이 각 위치에서 정확한 조건부 확률을 얼마나 잘 학습했는지, 그 이론적인 성능만 딱 발라내서 보겠다는 소리다.

**2. 픽셀별 확률 계산 및 NLL 도출**

그래서 모델이 뱉은 예측값이랑 실제 Label이랑 비교해서 **Cross Entropy(교차 엔트로피)**로 에러값을 구한다. 그리고 다음 픽셀로 넘어갈 땐 방금 구한 예측값은 갖다 버리고, 다시 실제 Label 픽셀값을 입력으로 써서 또 에러를 구한다.

이렇게 구한 모든 픽셀의 에러값을 싹 다 더해서 전체 픽셀 수(차원 수)로 나눈 게 바로 **NLL(Negative Log-Likelihood)**이다. 간단히 말해, NLL은 모든 픽셀에 대한 크로스 엔트로피 손실의 평균이고, **"모델이 실제 이미지 분포를 얼마나 정확하게 흉내 냈는지"**를 수치로 찍어주는 거다. 값이 낮을수록 실제 분포랑 비슷하다는 뜻이니까 성능이 좋은 거다.

**3. 왜? **

정보 이론적으로 보면 NLL은 **"모델이 각 픽셀을 인코딩하는 데 필요한 평균 비트 수"**로 해석할 수 있다. 여기서 NLL에 $\log_2(e)$를 곱하면 **bpd (bits per dimension)**라는 단위로 변환이 되는데, 이게 샤논의 정보 이론에서 말하는 "최적의 데이터 압축에 필요한 비트 수" 개념이랑 연결된다.

그냥 왜 이런 짓을 하냐면, **데이터 압축 관점**에서 모델 성능을 보여주면 훨씬 직관적이기 때문이다. "이 모델 쓰면 이미지 압축할 때 픽셀당 몇 비트면 돼!"라고 말할 수 있으니까.

```py
def evaluate(model, test_loader, device):
    model.eval()
    test_loss = 0
    
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            
            # 모델 출력 (Teacher Forcing 방식)
            output = model(data, target if model.n_classes else None)
            
            # 채널별 손실 계산
            loss = 0
            for c in range(data.shape[1]):  # 각 채널에 대해
                # 타겟 이미지 (0-255 정수)
                target_img = (data[:, c] * 255).long()
                output_c = output[:, c]  # [batch, height, width, 256]
                
                # 크로스 엔트로피 손실 계산
                loss += F.cross_entropy(
                    output_c.reshape(-1, 256),  # 각 픽셀의 256가지 값에 대한 확률
                    target_img.reshape(-1)      # 실제 픽셀 값 (0-255)
                )
            
            loss /= data.shape[1]  # 채널 수로 나누어 정규화
            test_loss += loss.item()
    
    return test_loss / len(test_loader)
 
```

 

즉 output은 256개의 한 픽셀에 있는 rgb 각 채널의 이산 값의 대한 확률 분포 [0.02, 0.04, ... , 0.01] 이런식으로 256개의 가능한 값의 리스트로 나타내고, target은 실제 이미지 값으로 여기서 pytorch는 toTensor()를 해서 0에서 1 사이의 값으로 정규화했기 떄문에 다시 255를 곱하고, 픽셀의 rgb각 채널의 값은 이산 값이므로 long()을 해서 소수점 떄는거다, 그렇게 하고 나서 실제값이랑 예측값의 크로스 엔트로피를 구한다.

크로스 엔트로피는   CE(y, p) = -Σ y_i * log(p_i) 이렇게 계산하니까 타켓이 지금 원-핫 인코딩이므로 CE(p, q) = -Σ p(x) * log(q(x))
         = -(0*log(q₀) + 0*log(q₁) + ... + 1*log(q_y) + ... + 0*log(q_n))
         = -log(q_y) 이고 이건 "원-핫 인코딩된 타겟의 경우 이는 -log(p_y)로 단순화됨"은 오직 정답 클래스 y의 예측 확률에 -log를 취한 값만 남는다는 의미이다, 즉 그러니까 실제값 y가 어차피 [0,0,0,...1,0] 이런식일테고 거기서 log(p_i)라는 확률값을 곱하면 어차피 해당 값이 아닌 나머지 값들은 0과 곱해져서 사라지고 해당값만 남고 그 해당값의 확률만 로그취한게 남는다.

여기서 pytorch의 cross_entropy()는 logics(원시값)에다가 스프트맥스(합하면 1이 되게 값들을 확률로 만드는거) 하고 거기다 -log를 적용하고 배치 내 모든 샘플의 평균을 계산하는 과정을 거치기 때문에 loss의 입력값에 저렇게 하면 된다.

```py
# 타겟 클래스 인덱스 (예: 클래스 1)
target = 1

# 타겟 클래스의 예측 확률
prob_y = softmax[target]  # 0.24(값 중 가장 큰 값)

# -log 적용
loss = -torch.log(prob_y)  # -log(0.24) ≈ 1.43

# 배치 크기가 3인 경우의 개별 손실값
losses = [1.43, 2.1, 0.7]

# 배치 평균
batch_loss = sum(losses) / len(losses)  # (1.43 + 2.1 + 0.7) / 3 ≈ 1.41

loss += F.cross_entropy(
    output_c.reshape(-1, 256),  # [배치×높이×너비, 256] 형태의 로짓
    target_img.reshape(-1)      # [배치×높이×너비] 형태의 타겟 인덱스 (0-255)
)
```

1. 각 픽셀 위치의 256개 로짓에 softmax 적용 → 각 픽셀값(0-255)의 확률 계산

1. 각 픽셀 위치에서 실제 픽셀값(타겟)에 해당하는 확률에 -log 적용

1. 모든 픽셀 위치에 대한 평균 손실 계산

예를 들어, 한 픽셀의 실제 값이 42이고 모델이 예측한 42의 확률이 0.7이라면, 이 픽셀의 손실은 -log(0.7) ≈ 0.36

원-핫 인코딩된 타겟의 경우 -log(p_y)로 단순화됨"이란:

- 범주형 타겟(클래스 인덱스)에 대한 크로스 엔트로피는

- 실제 클래스에 해당하는 예측 확률에 -log를 취한 값으로 단순화됨을 의미

```py
# 네 번째 셀: 모델 구현 (설정 객체 활용하도록 수정)
class MaskedConv2d(nn.Conv2d):
    def __init__(self, in_channels, out_channels, kernel_size, mask_type='A', stride=1, padding='same', **kwargs):
        if padding == 'same':
            padding = kernel_size // 2
        super(MaskedConv2d, self).__init__(in_channels, out_channels, kernel_size, stride=stride, padding=padding, **kwargs)
        self.register_buffer('mask', torch.ones_like(self.weight))
        
        # 마스크 생성
        _, _, h, w = self.weight.size()
        self.mask[:, :, h//2, w//2 + 1:] = 0  # 오른쪽 마스킹
        self.mask[:, :, h//2 + 1:, :] = 0     # 아래쪽 마스킹
        
        # A 타입 마스크는 현재 픽셀도 마스킹 (B 타입은 현재 픽셀 포함)
        if mask_type == 'A':
            self.mask[:, :, h//2, w//2] = 0
            
    def forward(self, x):
        self.weight.data *= self.mask  # 가중치에 마스크 적용
        return super(MaskedConv2d, self).forward(x)

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, mask_type='B'):
        super(ResidualBlock, self).__init__()
        self.conv1 = MaskedConv2d(in_channels, out_channels, kernel_size=3, mask_type=mask_type)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = MaskedConv2d(out_channels, out_channels, kernel_size=3, mask_type=mask_type)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
        # 채널 차원이 다를 경우 1x1 컨볼루션으로 맞춤
        if in_channels != out_channels:
            self.skip = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1),
                nn.BatchNorm2d(out_channels)
            )
        else:
            self.skip = nn.Identity()
        
    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.skip(residual)
        out = self.relu(out)
        return out

class PixelCNN(nn.Module):
    def __init__(self, input_channels=3, hidden_channels=128, n_residual=15, n_classes=None, kernel_size=7):
        super(PixelCNN, self).__init__()
        self.n_classes = n_classes
        self.input_channels = input_channels
        
        # 조건부 임베딩 (있을 경우)
        if n_classes:
            self.embedding = nn.Embedding(n_classes, hidden_channels)
        
        # 첫 번째 레이어는 'A' 타입 마스크 사용
        self.conv_in = MaskedConv2d(input_channels, hidden_channels, kernel_size=kernel_size, mask_type='A')
        self.bn_in = nn.BatchNorm2d(hidden_channels)
        
        # 레지듀얼 블록들
        self.residual_blocks = nn.ModuleList([
            ResidualBlock(hidden_channels, hidden_channels, mask_type='B')
            for _ in range(n_residual)
        ])
        
        # 출력 레이어
        self.relu = nn.ReLU(inplace=True)
        self.conv_out1 = MaskedConv2d(hidden_channels, hidden_channels, kernel_size=3, mask_type='B')
        self.bn_out1 = nn.BatchNorm2d(hidden_channels)
        self.conv_out2 = MaskedConv2d(hidden_channels, hidden_channels, kernel_size=3, mask_type='B')
        self.bn_out2 = nn.BatchNorm2d(hidden_channels)
        self.conv_out3 = MaskedConv2d(hidden_channels, input_channels * 256, kernel_size=1, mask_type='B')
        
    def forward(self, x, label=None):
        # [0, 255] 범위를 [0, 1]로 정규화
        if x.max() > 1:
            x = x / 255.0
        
        batch_size, _, height, width = x.shape
        
        # 초기 합성곱
        out = self.relu(self.bn_in(self.conv_in(x)))
        
        # 조건부 임베딩 추가 (있을 경우)
        if self.n_classes and label is not None:
            label_emb = self.embedding(label).view(batch_size, -1, 1, 1)
            out = out + label_emb
        
        # 레지듀얼 블록
        for block in self.residual_blocks:
            out = block(out)
        
        # 출력 레이어
        out = self.relu(self.bn_out1(self.conv_out1(out)))
        out = self.relu(self.bn_out2(self.conv_out2(out)))
        out = self.conv_out3(out)
        
        # 출력 형태 변환: [batch, channels*256, height, width] -> [batch, channels, height, width, 256]
        out = out.view(batch_size, self.input_channels, 256, height, width)
        out = out.permute(0, 1, 3, 4, 2)  # [batch, channels, height, width, 256]
        
        return out
```





