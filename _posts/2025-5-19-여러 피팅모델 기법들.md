---
key:
title: ' 네스테로프 가속 모멘텀 (NAG)과 최적화 과정 '
excerpt: 'deeplearning'
tags: [인공지능]
---

# 모멘텀의 진화: 네스테로프 가속 모멘텀 (NAG)과 최적화 과정의 시각적 이해

이전 글에서는 확률적 경사 하강법(SGD)의 변동성을 줄이고 수렴 속도를 개선하기 위해 도입된 모멘텀(Momentum) 방식에 대해 알아보았다. 모멘텀은 과거의 업데이트 방향(관성)을 현재 업데이트에 반영하여 학습 경로를 부드럽게 하고, 특정 지형에서 학습을 가속하는 효과가 있었다. 이번 글에서는 모멘텀을 한 단계 더 발전시킨 **네스테로프 가속 모멘텀 (Nesterov Accelerated Momentum, NAG)**을 소개하고, 이러한 최적화 과정이 파라미터 공간과 손실 함수 위에서 어떻게 이루어지는지 시각 자료(그림 6.1)를 통해 직관적으로 이해해 본다.

## 1. 네스테로프 가속 모멘텀 (NAG)

일반적인 모멘텀 방식은 현재 위치에서 계산된 그래디언트와 이전의 모멘텀을 결합하여 다음 스텝을 결정한다. 반면, 네스테로프 가속 모멘텀(NAG)은 여기서 한 걸음 더 나아가, 모멘텀이 다음 스텝을 "예측"하는 능력을 더 적극적으로 활용한다.

**핵심 아이디어**: "일단 현재 모멘텀 방향으로 한 발짝 먼저 가보고 (예측된 위치), 그곳에서 주변을 살펴본 후 (그래디언트 계산), 최종적으로 움직일 방향을 결정하자."

### 1.1. NAG의 업데이트 규칙 (식 6.12)

$$ \mathbf{m}_{t+1} \leftarrow \beta \cdot \mathbf{m}_t + (1-\beta) \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\phi_t - \alpha\beta \cdot \mathbf{m}_t]}{\partial \phi} $$
$$ \phi_{t+1} \leftarrow \phi_t - \alpha \cdot \mathbf{m}_{t+1} $$

일반 모멘텀(식 6.11)과 비교했을 때, 파라미터 업데이트 식($\phi_{t+1}$ 계산)은 동일하다. 가장 큰 차이는 **모멘텀 벡터 $\mathbf{m}_{t+1}$을 계산할 때 사용되는 그래디언트의 계산 지점**이다.

*   일반 모멘텀: 그래디언트를 **현재 파라미터 위치 $\phi_t$**에서 계산한다.
    ($\sum \frac{\partial \ell_i[\mathbf{\phi_t}]}{\partial \phi}$)
*   네스테로프 모멘텀: 그래디언트를 **"예측된" 다음 위치 $\phi_t - \alpha\beta \cdot \mathbf{m}_t$**에서 계산한다.
    ($\sum \frac{\partial \ell_i[\mathbf{\phi_t - \alpha\beta \cdot \mathbf{m}_t}]}{\partial \phi}$)

**질문**: "원래 SGD에서 그래디언트 항은 모멘텀 항에 의해 어디로 이동하는지와는 관련 없이, 따로 지금 현재 있는 위치에 따라 미분값을 계산했는데, 이제는 (NAG에서는) 모멘텀 항도 영향을 끼쳐서 어디로 이동하는지도 미분값 (계산 위치)에 영향을 끼친다고?"
**답변**: 정확하다! 이것이 NAG의 핵심적인 개선점이다.

### 1.2. 예측된 위치 $\phi_t - \alpha\beta \cdot \mathbf{m}_t$의 의미

이 항이 "예측된 위치"로 해석되는 이유는 다음과 같다:
1.  $\mathbf{m}_t$: 이전 스텝까지 누적된, 현재 나아가고 있는 주된 방향과 속도를 나타내는 모멘텀 벡터.
2.  $\alpha\beta \mathbf{m}_t$: 만약 현재 스텝에서 모멘텀의 영향($\alpha\mathbf{m}_t$) 중 일부($\beta$ 비율)만큼을 미리 적용하여 파라미터 공간에서 이동한다면 도달할 가상의 위치를 나타낸다. 즉, 현재 위치 $\phi_t$에서 모멘텀 $\mathbf{m}_t$의 영향으로 잠정적으로 이동한 파라미터 값이다.
3.  NAG는 이 **잠정적으로 이동한 지점에서의 손실 함수의 기울기**를 살펴본다. 이는 "만약 모멘텀을 따라 이만큼 움직인다면 손실 함수의 기울기가 어떻게 될까?"를 미리 확인하는 것과 같다.

손실 함수 자체가 움직이는 것이 아니라, 우리가 손실 함수 위의 다른 지점(새로운 파라미터 값)에서 그래디언트를 평가하는 것이다. 이 "한 발 앞서 내다보는" 접근 방식은 모멘텀이 너무 과도하게 진행하여 좋지 않은 방향으로 가는 것을 미리 감지하고 경로를 "수정(correct)"할 기회를 제공한다. 이로 인해 일반 모멘텀보다 더 안정적이고 빠른 수렴을 보이는 경우가 많다.

## 2. 최적화 과정의 시각적 이해

![image-20250519152532886](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250519152532886.png)

이러한 최적화 알고리즘들이 파라미터 공간과 손실 함수 위에서 어떻게 작동하는지 그림 6.1을 통해 직관적으로 이해해 보자. (그림 6.1은 단순 선형 회귀 $\hat{y} = \phi_0 + \phi_1 x$의 파라미터 $\phi_0, \phi_1$을 찾는 과정을 보여준다.)

*   **그림 6.1a (실제 훈련 데이터)**: 입력 $x$와 출력 $y$의 관계를 보여주는 산점도. 우리의 목표는 이 데이터를 가장 잘 설명하는 선형 모델을 찾는 것이다.
*   **그림 6.1b (3D 손실 함수 표면)**: 두 파라미터 $\phi_0$(절편)와 $\phi_1$(기울기)의 값에 따른 손실 함수 $L(\phi)$의 값을 3차원으로 그린 것이다. 마치 그릇과 같은 볼록한 형태를 띤다. 최적화의 목표는 이 그릇의 가장 낮은 바닥 지점을 찾는 것이다.
*   **그림 6.1c (2D 손실 함수 등고선)**: 그림 6.1b를 위에서 내려다본 등고선 형태이다. 각 선은 동일한 손실 값을 가지는 $(\phi_0, \phi_1)$ 조합을 나타내며, 중심부로 갈수록 손실이 낮아진다.
*   **그림 6.1b와 6.1c의 점들의 움직임 (0 $\to$ 1 $\to$ ... $\to$ 4)**:
    *   이 점들은 파라미터 공간 $(\phi_0, \phi_1)$에서의 **서로 다른 파라미터 조합**을 의미한다.
    *   점이 움직인다는 것은 경사 하강법 (또는 SGD, 모멘텀 등) 알고리즘에 의해 **파라미터 $\phi_0, \phi_1$의 값이 업데이트되어 변한다**는 것을 나타낸다.
    *   알고리즘은 손실 함수 $L(\phi)$의 값이 더 낮은 지점(그릇의 바닥 또는 등고선의 중심)을 향해 파라미터 값을 점진적으로 수정한다. 즉, "오차 함수에서 이동한다는 건 저 파라미터 값이 변한다는 말"이다.
*   **그림 6.1d (데이터와 모델 예측선)**: 실제 훈련 데이터(주황색 점) 위에, 그림 6.1b,c의 각 파라미터 지점(0, 1, ..., 4)에 해당하는 모델 예측선 $\hat{y} = \phi_0 + \phi_1 x$을 겹쳐 그린 것이다. 파라미터가 업데이트됨에 따라 모델이 데이터를 점점 더 잘 설명해 나가는 과정을 보여준다. 최종적으로 지점 4에 도달한 모델이 데이터에 가장 잘 맞는 선을 나타낸다.

결국, 최적화 과정은 주어진 데이터(a)를 가장 잘 설명하는 모델 파라미터 $(\phi_0, \phi_1)$ (c에서 점의 최종 위치)를 찾기 위해, 손실 함수(b, c의 지형) 위에서 현재 파라미터에서의 손실을 줄이는 방향으로 파라미터 값을 반복적으로 조정해나가는 여정이라고 할 수 있다. 모멘텀, 네스테로프 가속 모멘텀 등은 이 여정을 더 빠르고 안정적으로 만들어주는 다양한 전략들이다.