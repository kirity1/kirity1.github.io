---
key:
title: 'SGD, 확률적 경사 하강법'
excerpt: 'deeplearning'
tags: [deeplearning]
---

# 경사 하강법의 진화: SGD와 모멘텀으로 최적화 개선하기

이전 글들에서는 선형 회귀의 볼록 손실 함수와 달리, 대부분의 비선형 모델, 특히 딥러닝 모델의 손실 함수가 비볼록(non-convex)하여 지역 최솟값이나 안장점 같은 여러 난관이 존재함을 살펴보았다. 이러한 복잡한 손실 함수 위에서 최적의 파라미터를 찾기 위해 기본 경사 하강법(Gradient Descent)에서 발전된 여러 기법들이 사용된다. 이번 글에서는 그중 핵심적인 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**과 이를 개선한 **모멘텀(Momentum)** 방식에 대해 알아본다.

## 1. 확률적 경사 하강법 (SGD): 빠르고 무작위적인 탐색

일반적인 경사 하강법(배치 경사 하강법, BGD)은 한 번 파라미터를 업데이트할 때 전체 학습 데이터셋을 사용하여 그래디언트를 계산한다. 이는 정확한 방향을 제시하지만, 데이터셋이 클 경우 계산 비용이 엄청나다는 단점이 있다.

**확률적 경사 하강법(SGD)**은 이러한 문제를 해결하기 위해 등장했다.

### 1.1. SGD의 핵심 아이디어: 미니배치 (Minibatch)

SGD는 전체 데이터셋 대신, 데이터셋의 작은 부분집합인 **미니배치(minibatch)** 또는 간단히 **배치(batch)**를 무작위로 선택하여 그래디언트를 계산하고 파라미터를 업데이트한다.

*   **이터레이션(Iteration)**: 한 번의 미니배치 처리 및 파라미터 업데이트.
*   **에포크(Epoch)**: 전체 학습 데이터셋을 한 번 모두 사용하는 것. 예를 들어, 30,000개의 학습 데이터가 있고 배치 크기가 128이라면, 약 $30000 / 128 \approx 235$번의 이터레이션을 거쳐야 1 에포크가 완료된다. 즉, 1 에포크 동안 파라미터 업데이트가 여러 번 발생한다.

SGD의 파라미터 업데이트 규칙은 다음과 같다:
$$ \phi_{t+1} \leftarrow \phi_t - \alpha \cdot \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\phi_t]}{\partial \phi} $$
여기서 $\phi_t$는 현재 파라미터, $\alpha$는 학습률, $\mathcal{B}_t$는 현재 $t$번째 이터레이션의 미니배치에 속한 데이터 샘플들의 인덱스 집합, $\ell_i$는 $i$번째 샘플에 대한 손실이다. 즉, 현재 미니배치에 대한 그래디언트의 (가중) 합을 사용하여 파라미터를 업데이트한다.

**질문**: "SGD는 128개 단위로 끊어서 그때그때 전체 그래디언트에 대한 근사치를 구하는 건가?"
**답변**: 정확하다. BGD는 3만 개 전체로 "진짜" 그래디언트를 한 번 계산하여 업데이트하는 반면, SGD는 128개(미니배치 크기)로 "근사치" 그래디언트를 계산하여 더 자주 업데이트한다.

![image-20250519145734285](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250519145734285.png)

### 1.2. SGD의 특징과 장점

*   **계산 효율성**: 각 이터레이션이 매우 빠르다.
*   **노이즈와 탐색**: 미니배치의 무작위성 때문에 그래디언트에 노이즈가 발생한다. 이 노이즈 덕분에 알고리즘은 때때로 일시적으로 손실이 증가하는 방향(오르막길)으로도 움직일 수 있으며, 이를 통해 얕은 지역 최솟값에서 벗어나 더 좋은 해(다른 "계곡")를 탐색할 가능성을 가진다 (그림 6.5b 참고).
*   **비복원 추출**: 일반적으로 한 에포크 내에서는 미니배치를 구성할 때 비복원 추출(sampling without replacement) 방식을 사용한다. 즉, 한 번 사용된 데이터는 해당 에포크가 끝날 때까지 다시 사용되지 않는다. 모든 데이터를 다 사용하면 (1 에포크 종료), 다시 전체 데이터셋으로부터 샘플링을 시작한다.

### 1.3. SGD에 대한 또 다른 해석

![image-20250519145923788](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250519145923788.png)

(미니배치,배치의 사이즈를 3이라고 정하고 Gabor 모델의 SGD뷰를 보여주는 그래프)

SGD를 "각 반복마다 서로 다른 (미니배치에 대한) 손실 함수의 그래디언트를 계산하는 것"으로 해석할 수도 있다. 손실 함수는 모델과 사용된 학습 데이터에 따라 달라지므로, 무작위로 선택된 각 배치마다 실질적으로 다른 손실 함수를 다루는 셈이다. 이 관점에서 SGD는 "끊임없이 변화하는 손실 함수에 대해 결정론적인 경사 하강법을 수행하는 것"과 같다 (그림 6.6). 중요한 점은, 이러한 변동성에도 불구하고 어떤 지점에서든 **기대되는 손실(expected loss)과 기대되는 그래디언트(expected gradients)**는 풀배치 경사 하강법의 경우와 동일하게 유지된다는 것이다. 당연히 배치는 데이터셋에서 뽑기 떄문이다, 이 때문에 SGD는 장기적으로 올바른 방향으로 수렴해 갈 수 있다.

### 1.4. SGD의 수렴과 학습률 스케줄

SGD는 전통적인 의미에서 한 점으로 완전히 "수렴"하지 않을 수 있다. 미니배치의 노이즈 때문에 최적해 주변을 계속 미세하게 맴돌 수 있다. 그러나 전역 최솟값 근처에서는 모델이 데이터를 잘 설명하게 되어 어떤 배치를 선택하든 그래디언트가 작아지고 파라미터 변화도 줄어든다.

실제로는 **학습률 스케줄(learning rate schedule)**을 함께 사용하는 경우가 많다. 여기서 학습률 스케쥴은 식에서 알파를 의미한다.

*   **초기**: 높은 학습률로 설정하여 파라미터 공간을 넓게 탐색하고, SGD의 "계곡 뛰어넘기" 능력을 활용해 좋은 지역을 찾도록 한다.
*   **후기**: 학습이 진행됨에 따라 학습률을 점차 줄여, 이미 찾은 좋은 해 근처에서 파라미터를 세밀하게 조정하고 안정적으로 수렴하도록 한다.

## 2. 모멘텀 (Momentum): SGD에 관성을 더하다

SGD는 효율적이지만, 그래디언트의 노이즈로 인해 학습 경로가 불안정하거나 특정 지형(예: 좁고 긴 계곡)에서 진동하며 느리게 수렴할 수 있다. **모멘텀(Momentum)**은 이러한 SGD의 단점을 보완하기 위해 도입된 기법이다.

### 2.1. 모멘텀의 기본 아이디어

모멘텀은 물리에서의 관성(inertia)과 유사한 개념을 최적화에 적용한 것이다. 현재의 업데이트 방향을 결정할 때, 현재 미니배치의 그래디언트뿐만 아니라 **이전 단계에서 움직였던 방향과 속도(모멘텀)**를 함께 고려한다.

업데이트 규칙은 다음과 같다 (식 6.11):
$$ \mathbf{m}_{t+1} \leftarrow \beta \cdot \mathbf{m}_t + (1-\beta) \sum_{i \in \mathcal{B}_t} \frac{\partial \ell_i[\phi_t]}{\partial \phi} $$
$$ \phi_{t+1} \leftarrow \phi_t - \alpha \cdot \mathbf{m}_{t+1} $$

*   $\mathbf{m}_t$: $t$ 시점의 모멘텀 벡터 (과거 움직임의 누적).
*   $\beta$: 모멘텀 계수 (보통 0.9와 같이 1에 가까운 값). 과거 모멘텀을 얼마나 유지할지 결정.
*   첫 번째 식: 새로운 모멘텀 $\mathbf{m}_{t+1}$은 이전 모멘텀과 현재 미니배치 그래디언트의 가중 합으로 계산된다. (지수 이동 평균의 형태)
*   두 번째 식: 파라미터는 이 새로운 모멘텀 $\mathbf{m}_{t+1}$ 방향으로 학습률 $\alpha$만큼 업데이트된다.

"SGD에서 배치의 랜덤성 때문에(뽑는게 랜덤이니까) 학습이 매번 불안정할 수 있는 것 때문에 모멘텀을 도입한 건가?"
모멘텀은 SGD 그래디언트의 노이즈를 완화하고 학습 경로를 안정화하며, 특정 조건에서는 학습을 가속하는 효과도 있다.

### 2.2. 모멘텀의 효과

*   **재귀적 누적**: 모멘텀 계산은 재귀적이어서, 현재의 모멘텀은 이론적으로 학습 시작부터 모든 이전 그래디언트들의 정보를 (과거로 갈수록 가중치가 작아지는 형태로) 담고 있다. 마지막 이터레이션의 모멘텀은 해당 에포크의 모든 배치 정보를 반영하는 셈이다.
*   **학습 경로 평활화 (Smoothing)**: 그래디언트의 급격한 변화를 완화하여 파라미터 경로를 부드럽게 만든다.
*   **진동 감소 및 수렴 가속**:
    *   좁은 계곡에서 SGD가 좌우로 진동하는 것을 줄여준다. 관성 덕분에 계곡 바닥을 따라 더 빠르게 진행한다.
    *   여러 스텝에 걸쳐 그래디언트 방향이 일관되면, 모멘텀이 그 방향으로 축적되어 업데이트 속도를 높인다 (유효 학습률 증가 효과).
    *   반대로 그래디언트 방향이 자주 바뀌면, 모멘텀이 상쇄되어 업데이트 폭이 줄어든다 (유효 학습률 감소 효과, 안정성 증가).
*   **안장점 탈출 도움**: SGD가 안장점 근처의 평탄한 지역에서 멈칫할 때, 과거로부터 이어져 온 모멘텀이 있다면 그 힘으로 안장점에서 벗어나는 데 도움을 줄 수 있다. (비록 본 글에서는 이전에 논의한 "SGD 자체가 노이즈로 안장점 탈출을 돕는다"는 점과 별개로, 모멘텀도 이러한 탈출을 보조할 수 있다는 의미로 해석 가능)

![image-20250519150420649](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250519150420649.png)

전반적으로 모멘텀은 SGD의 학습 과정을 더 안정적이고 효율적으로 만들어, 더 부드러운 경로로 최적해를 찾아가도록 돕는다 (그림 6.7).