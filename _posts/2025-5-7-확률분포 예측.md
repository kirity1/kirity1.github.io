---
key:
title: '확률분포예측,최대가능도 추정'
excerpt: 'deeplearning'
tags: [deeplearning]
---

## 예측, 점을 넘어 분포로: 모델이 불확실성을 이야기하는 방법
**(회귀 문제에서 Pr(y|x)와 최대 가능도 손실 함수의 이해)**

기존의 많은 머신러닝 회귀 모델은 입력 \(x\)가 주어졌을 때, 특정 출력값 \(y\) 하나만을 예측하는 방식으로 작동한다. 예를 들어, "내일의 예상 기온은 25도"와 같이 명확한 하나의 숫자를 제시한다. 하지만 현실 세계의 많은 문제는 본질적으로 불확실성을 내포한다. "내일 기온이 정확히 25도일까, 아니면 24도나 26도일 가능성은 없을까?" 이런 질문에 답하기 위해, 모델이 단순히 점(point) 예측을 넘어 출력값에 대한 전체 **확률 분포(probability distribution)**를 예측하는 접근 방식이 중요해지고 있다.

### 1. 점 예측의 한계와 분포 예측의 등장

전통적인 회귀 모델은 입력 \(x\)에 대해 최적의 단일 예측값 \(\hat{y}\)를 찾는 것을 목표로 한다. 예를 들어, 최소 제곱 오차(Mean Squared Error, MSE)를 손실 함수로 사용하는 선형 회귀 모델은 평균적인 예측을 제공하려 하지만, 그 예측이 얼마나 확실한지에 대한 정보는 주지 않는다.

이에 반해, **분포 예측**은 입력 \(x\)가 주어졌을 때 가능한 모든 출력 \(y\) 값들에 대한 확률을 나타내는 함수, 즉 **조건부 확률 분포 \(Pr(y|x)\)**를 모델링한다.
*   **\(Pr(y|x)\)란?**: "입력 \(x\)의 조건 하에서, 출력이 \(y\)일 확률"을 의미한다.
*   **시각화**: 만약 \(y\)가 연속적인 실수 값이라면, \(Pr(y|x)\)는 특정 \(x\)에 대해 종 모양의 곡선(예: 정규분포)으로 나타날 수 있다. 이 곡선의 넓이는 1이며, 특정 구간의 넓이는 \(y\)가 해당 구간에 속할 확률을 나타낸다. 곡선의 봉우리가 높은 곳은 \(y\)가 발생할 확률이 높은 지점이며, 넓게 퍼진 곡선은 예측의 불확실성이 크다는 것을 의미한다.

이러한 분포 예측은 "내일 기온은 평균 25도이고, 표준편차는 1.5도인 정규분포를 따를 것으로 예상된다"와 같이 훨씬 풍부한 정보를 제공할 수 있다. 이를 통해 우리는 예측값 주변의 신뢰 구간을 설정하거나, 특정 값 이상/이하일 확률 등을 계산할 수 있게 된다.

### 2. 모델은 어떻게 분포를 학습할까? - 손실 함수의 역할

모델이 \(Pr(y|x)\)라는 확률 분포를 예측하도록 학습시키기 위해서는 손실 함수의 역할이 중요하다. 여기서 핵심적인 아이디어는 **최대 가능도 추정 (Maximum Likelihood Estimation, MLE)**이다. MLE의 기본 원리는 "우리가 가지고 있는 학습 데이터 \(\{x_i, y_i\}\)가 현재 모델 하에서 나타날 가능성(가능도, likelihood)이 가장 높아지도록 모델의 파라미터(가중치, 편향 등)를 조정하자"는 것이다.

즉, 손실 함수는 각 학습 샘플 \((x_i, y_i)\)에 대해, 모델이 예측한 분포 \(Pr(y|x_i)\)에서 실제 정답 \(y_i\)가 관찰될 확률을 최대화하도록 설계된다.

수학적으로는 보통 **음의 로그 가능도 (Negative Log-Likelihood, NLL)**를 최소화하는 형태로 손실 함수를 정의한다. 로그를 취하는 이유는 곱셈 연산을 덧셈 연산으로 바꿔 계산을 용이하게 하고, 음수를 취하는 이유는 최적화 문제에서 일반적으로 최소화 문제를 다루기 때문이다.
만약 모델 파라미터를 \(\theta\)라고 할 때, NLL 손실 함수는 다음과 같이 표현될 수 있다:
\[ \text{Loss}(\theta) = - \sum_{i=1}^{N} \log Pr(y_i | x_i; \theta) \]
여기서 \(N\)은 전체 학습 데이터의 수이다.

*   **작동 방식**:
    *   만약 특정 입력 \(x_i\)에 대해 모델이 예측한 분포 \(Pr(y|x_i)\)가 실제 정답 \(y_i\) 근처에서 높은 확률 값을 가진다면 (즉, \(Pr(y_i|x_i)\)가 크다면), \(\log Pr(y_i|x_i)\)는 덜 음수가 되고, \(-\log Pr(y_i|x_i)\)는 작은 양수가 되어 손실이 작아진다.
    *   반대로, 모델이 실제 정답 \(y_i\)에 낮은 확률을 할당한다면, \(\log Pr(y_i|x_i)\)는 매우 큰 음수가 되고, \(-\log Pr(y_i|x_i)\)는 큰 양수가 되어 손실이 커진다.
    *   따라서 모델은 학습 과정에서 이 NLL 손실을 줄이기 위해 실제 정답에 높은 확률을 부여하는 방향으로 파라미터 \(\theta\)를 업데이트하게 된다.

### 3. 시각적으로 이해하기

(앞선 대화에서 언급된 그림 5.1(a)와 같은 상황을 상상해보자)
*   **X축**은 입력 변수 \(x\), **Y축**은 출력 변수 \(y\)를 나타낸다.
*   **주황색 점들**은 우리가 가진 실제 학습 데이터 \((x_i, y_i)\) 쌍들을 나타낸다.
*   모델이 학습된 후, 특정 입력 \(x_{test}\) (예: \(x=2.0\) 또는 \(x=7.0\))에 대해 모델이 예측하는 **출력 분포 \(Pr(y|x_{test})\)는 청록색 곡선(예: 정규분포의 확률 밀도 함수)으로 시각화**될 수 있다.
*   **잘 학습된 모델이라면**, 각 학습 데이터의 주황색 점 \(y_i\)는 해당 \(x_i\)에 대한 청록색 예측 분포 곡선에서 확률이 높은, 즉 곡선이 봉긋 솟아있는 부분에 위치하게 될 것이다. 손실 함수는 바로 이러한 상태를 목표로 모델을 이끌어간다.

### 4. 개념적 구현 단계

실제로 신경망 모델이 확률 분포를 예측하도록 하려면 다음과 같은 단계를 고려할 수 있다:

1.  **분포 선택**: 예측하려는 출력 \(y\)의 특성에 맞는 확률 분포를 선택한다.
    *   \(y\)가 일반적인 실수 값이라면 **정규분포(Gaussian distribution)**를 가정할 수 있다. 이 경우 모델은 각 입력 \(x\)에 대해 정규분포의 파라미터인 평균 \(\mu(x)\)와 분산 \(\sigma^2(x)\) (또는 표준편차 \(\sigma(x)\))를 예측해야 한다.
    *   \(y\)가 개수(count) 데이터라면 포아송 분포, 이산적인 클래스라면 다항분포(또는 소프트맥스 출력을 통한 카테고리컬 분포) 등을 사용할 수 있다.
2.  **모델 아키텍처**: 신경망의 마지막 레이어가 선택된 분포의 파라미터들을 출력하도록 설계한다.
    *   예를 들어 정규분포를 가정한다면, 네트워크는 두 개의 출력 노드를 가질 수 있다. 하나는 \(\mu(x)\)를 예측하고, 다른 하나는 \(\log(\sigma^2(x))\) 또는 \(\sigma(x)\)를 예측한다. (분산은 항상 양수여야 하므로, 직접 분산을 예측하기보다는 로그-분산을 예측한 후 지수 함수를 취하거나, ReLU와 같은 활성화 함수를 통해 양수성을 보장하는 방법을 사용한다.)
3.  **손실 함수 구현**: 선택된 분포의 확률 밀도 함수(PDF)나 확률 질량 함수(PMF)를 사용하여 NLL을 계산한다.
    *   정규분포의 경우, \(Pr(y|x; \mu(x), \sigma^2(x))\)는 정규분포의 PDF가 된다. 이 PDF에 실제 \(y_i\)와 모델이 예측한 \(\mu(x_i)\), \(\sigma^2(x_i)\)를 대입하여 가능도를 계산하고, 이를 통해 NLL 손실을 구성한다.

### 5. 분포 예측의 강력한 이점

출력에 대한 분포를 예측하는 방식은 다음과 같은 중요한 이점을 제공한다:

*   **불확실성 정량화**: 모델이 예측값과 함께 그 예측이 얼마나 확실한지에 대한 정보를 제공한다. 예를 들어, 예측된 분포의 분산이 크다면 모델이 해당 예측에 대해 불확실하다는 것을 의미한다. 이는 특히 안전이 중요한 분야(의료, 자율주행 등)에서 매우 중요하다.
*   **신뢰 구간 추정**: 예측된 분포로부터 신뢰 구간(confidence interval) 또는 예측 구간(prediction interval)을 쉽게 계산할 수 있다.
*   **더 나은 의사결정 지원**: 단순히 하나의 예측값만 보는 것보다, 가능한 결과의 범위를 이해하는 것이 더 합리적인 의사결정을 내리는 데 도움이 된다.
*   **이상치 탐지**: 학습 데이터에 없던 패턴의 입력이 들어왔을 때, 모델이 매우 넓은(불확실성이 큰) 분포를 출력하거나, 낮은 가능도를 할당함으로써 이상치를 탐지하는 데 활용될 수 있다.

결론적으로, 모델이 출력에 대한 확률 분포를 예측하도록 학습하는 것은 머신러닝 모델이 현실 세계의 불확실성을 더 잘 다루고, 더 풍부하며 신뢰할 수 있는 정보를 제공하도록 하는 강력한 패러다임이다. 이는 단순한 점 예측을 넘어, 모델의 예측에 대한 깊이 있는 이해를 가능하게 한다.