---
key:
title: '선형대수학 4강 정리1'
excerpt: '딥러닝을 위해 필요한 내용을 다룹니다.'
tags: [선형대수학]

---



![image-20231229151224694](https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229151224694.png)

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229151424162.png" alt="image-20231229151424162" style="zoom:67%;" />

리니어 함수에 대한 조건들이다

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229151844282.png" alt="image-20231229151844282" style="zoom: 67%;" />

이 함수에서 보듯이 꼭 직선함수라고 해서 선형이라고 말 할수는 없다. n이라는 항이 사라져야 슈퍼포지션과, 호모지니어티가 성립해서 선형이 될 수 있다. 즉 n=0일때만, 직선이 원점을 지날때 만 linear하다라고 말 할수 있다, x와 y가 비례한다라고 말 할수 있다. 이걸 2차원 말고 3차원으로 확장하면 평면도 원점을 지나는 평면만 linear하다라고 말 할수 있다.

미분, 적분은 함수는 아니고 함수에 대해 가하는 연산이다, 여기서도 linear인지 아닌지 말 할수 있다.

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229152540635.png" alt="image-20231229152540635" style="zoom:67%;" />

그렇듯이 여러개의 인풋을 넣고 아웃풋을 뽑아내는 시스템에 linear인지 아닌지를 따질 수가 있다. 대표적으로 am(우리가 잘 아는 라디오)가 linear 시스템이다.

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229153405598.png" alt="image-20231229153405598" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229153722641.png" alt="image-20231229153722641" style="zoom:67%;" />

우리가 앞에서 말했던 물리적인 의미의 벡터는 크기와 방향이였다, 하지만 이제 벡터를 2차원,3차원 그 이상인 n차원에서는 각도를 무슨 축을 기준으로 각도를 잴 수가 없다, 그러므로 우리가 앞에서 했던거처럼 수학적인 정의를 거친거였다.

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229154021465.png" alt="image-20231229154021465" style="zoom:67%;" />

이제는 벡터를 그냥 데이터들의 나열쯤으로 이해하면 된다, 우리가 딥러닝을 할 때 예를 들면 이미지 6만장을 넣는다고 해보자, 그러면 입력값으로 [60000,r,g,b,이미지x값,이미지y값] 이렇게 5차원 이상도 넘어간다, 그러므로 이런식의 이해가 중요하다. $x \in R^n$이라고 생각하자.

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229154356736.png" alt="image-20231229154356736" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229154634801.png" alt="image-20231229154634801" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229154828241.png" alt="image-20231229154828241" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229154914639.png" alt="image-20231229154914639" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229155243550.png" alt="image-20231229155243550" style="zoom:67%;" />

보면 빨강, 파랑, 주황을 좌표에서 difference를 비교한다고 했을 때, 길이가 제각각이면 빨강에서 주황까지와 파랑에서 주황까지의 distance가 normalization하지 않으면 색들이 얼마나 떨어졌는지에 대해 정보가 유의미하지 않다, 그러므로 똑같은 1이라는 크기로 만들어서(normalization) distance를 구한다. 딥러닝할 때 중요한 부분이다.

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229162328660.png" alt="image-20231229162328660" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/kirity1/blogimages/main/uPic/image-20231229155925212.png" alt="image-20231229155925212" style="zoom:67%;" />

내적은 한 벡터가 다른 쪽의 벡터로 그 성분이 얼마나 있는지를 나타내는거다. 이 때 x와 z가 다른 벡터들의 기저가 되는거고 이게 차원의 기본축이 되는거다.



