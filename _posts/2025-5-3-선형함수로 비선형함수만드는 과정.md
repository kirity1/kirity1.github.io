---
key:
title: '선형함수로 비선형함수를 만드는 과정'
excerpt: 'deeplearning'
tags: [인공지능]
---

![image-20250503094542630](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503094542630.png)

이렇게 10개의 파라미터에 의해 설계된 모델이 있다고하자, 그러면 지금 선형함수 3개와 그 선형함수 앞에 a(ReLU함수)활성화 함수와 가중치파라미터 3개가 있고 상수항 파라미터가 있다고 해보자, 그러면 

![image-20250503094721579](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503094721579.png)

이런식으로 3개의 내부선형함수를 만들 수 있고

![image-20250503094746382](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503094746382.png)

이렇게 표현이 가능하다, 그러면 이 함수는 일단 6개의 파라미터로 3개의 내부선형함수를 만들 수 있고, 그 다음에 가중치들을 이용해서 함수 형태를 변하게 할 수있다.

![image-20250503095155883](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503095155883.png)

여기서 3-2 a 하나만 예를 들어보자면 이 비선형 함수 안에는 각각의 h1, h2, h3의 내부함수가 들어가 있고 이것들은 선형함수이기떄문에 원래 이 3개를 합쳐봤자 선형함수여야하지만, ReLU에 의해서 하나의 내부함수가 값이 0이 되는 지점인 x좌표에서 그 떄 이 함수도 하나의 내부함수가 영향을 끼치다 0이 되기떄문에 꺾이는 관절현상이 나타난다, 그러면 지금 3개의 내부함수가 합쳐져서 나타난 함수이기때문에 하나의 함수 그래프에 3개의 꺾이는 지점이 나타나는게 당연하다는 말인거다. 예를 들자면 h1이 0.0과 1.0사이 저 좌표에서 값이 0이 되던가 아니면 값이 이제 나오던가(ReLU에 의해서)해서 꺾이는 지점이 생겼고, 저렇게 1.0에도 h2에서 변화가 생긴거고 1.5에도 h3가 변화생겨서 저런 꺾이는 지점이 생긴거다, 이렇게 6개의 파라미터에 의해서 3개의 내부함수를 각각 활성화함수를 이용해서 비선형 함수를 구현해낸거다, 모델이 하나의 선형함수로 이루어진거보다 저렇게 다양한 모양으로 해야 예측이 더 잘된다는건 바로 생각 할 수 있기 떄문에 합리적이다.

![함수](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503095459030.png)

그리고 이렇게 3개의 가중치 파라미터를 이용해서 다양한 모양의 비선형 함수를 설계 할 수 있는거다, 높이는 상수항 파라미터에 의해 결정된다.

![image-20250503100228989](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503100228989.png)

이게 그말이다

![image-20250503100617095](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503100617095.png)

![image-20250503100630960](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503100630960.png)

![image-20250503101015665](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503101015665.png)

## 보편 근사 정리

![image-20250503110105713](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503110105713.png)

결국 저런 히든 유닛을 일반화 시켜서 많이 가지면 가질수록 국부적으로 표현하는 선(regions linears)들이 많아지므로, 닫힌 구간에서 정의된 일차원 함수(입력값이 하나란 소리)에 대해 임의의 정확도로(이론상으로 무한대에 보내면 아예 완벽하게 정확도가 일치하게 가능하다)근사가 가능하다는 소리다.

![image-20250503110231541](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503110231541.png)

## 인풋값 2개의 보편 근사 정리

만약 input값이 2개이상이라면? 그러면

![image-20250503122019999](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503122019999.png)

이런식으로 등고선을 생각해 볼 수 있다, 밝은 부분이 y값이 높고 어두운 부분이 y값이 낮은 입체적인 표면이라 생각해보자, 그러면 같은 y값들끼리 선을 그은 등고선을 표현하고, 이 등고선들의 기울기야 말로 인풋값 각각 x1 x2앞에 붙은 파라미터에 의해서 달라지는 모습을 표현한거다, 그리고 여기서 또 다른 통찰을 얻을 수 있는데, 등고선들간의 거리가 **일정**하다는거다, 즉 표면이 일정하게 기울어 있다는거로 더 쉬운 말로 y값이 일정하게 오른다는 소리로 선형함수라는 소리다, 즉 아무리 x값이 늘어나도 x제곱같은 형태가 아니면 선형의 모습일거고 그렇기 떄문에 등고선간의 거리차이는 일정 할 수 밖에 없다는 소리다.

![image-20250503122339162](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503122339162.png)

즉 y값이 0이 되는 지점을 경계로 하늘색 굵은 등고선이 y값이 0인 밟기를 가진 점들의 모임인거고 이렇게 Relu를 구현 할 수 있는거다.

![image-20250503122447973](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503122447973.png)

그리고 나서야 가중치 파라미터를 곱하는걸 보면 등고선간의 **촘촘함**은 달라졌을 지 몰라도 적어도 등고선 어떤 하나랑 어떤 하나끼리의 거리는 달라지지 않았다, 등고선이 **등간격이고, 평행하다** 기울기는 변했어도 선형함수는 그대로라는 소리다, 등고선이 일정하게 촘촘해졌다는건 그만큼 기울기가 **선형적으로 가파라졌다는** 소리다. 보면 g,h,i 3개의 전체적인 밝기가 전의 모습의 비해 더 어두어지거나 더 밝아지거나 하는 걸 통해 값 자체가 음수든 양수든 절대값이 뻥튀기됐다는걸 알 수 있다.

![image-20250503122731622](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503122731622.png) 

그리고 나서 이것들을 선형결합 한 최종적인 모습이야 말로 결국 보편근사정리에 의해 선형적인 표면들을 합쳐 비선형적인 표면을 만들어 낸거다, 물론 각각의 국소적인 표면들은 선형적이여도 이 표면들을 자잘자잘하게 나누다보면(히든유닛이 늘어나다 보면) 등고선간의 촘촘함이 달라지는게 비선형적인 모습으로 근사시킬 수 있다는 소리다(선형적으로 달라진다는건 촘촘함 정도는 변해도 등고선간의 거리는 변하지 않는건데 지금 이 표면을 보면 등고선간의 거리가 표면들에 따라 변하는데, 이 나누는 선들이 무한대로 가면 비선형적으로, 등고선간의 거리가 하나하나에 따라 서로 달라지는 모습을 그려 볼 수 있다), 즉 위에 본 보편근사정리의 3차원형태를 생각 해 볼 수 있다는거다.

## 시각적으로 나타낸 2개의 인풋값을 가진 보편 근사 정리

![image-20250503123438818](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503123438818.png)

ReLU를 적용한 인풋값이 2개인 선형결합의 모습을 나타낸거고 

![image-20250503123524165](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503123524165.png)

![image-20250503123651242](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503123651242.png)

![image-20250503123708526](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503123708526.png)

이런식으로 가중치를 곱해 **촘촘함**은 달라진거를 볼 수 있고, 이런식으로 히든 유닛 수에 따른 갯수만큼의 평면이 만들어 지는거고

![image-20250503123720405](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503123720405.png)

결국 3개의 평면을 합쳐서 저런 모습을 구현하는거고, 저 히든유닛의 갯수에 의해 생기는 평면의 개수가 늘어나면 늘어날 수록 비선형적인 모습의 평면이 생 길수 있다는걸 시각적으로 나타내 보았다.

![image-20250503124003862](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503124003862.png)

즉 convex하지만 국소적부분선형 표면을 가진 continuous surface를 만들 수 있다.

## 결론

activation함수들은 결국 내부선형함수의 끊어진 부분들을 만들어내고 이 선형함수들의 선형결합으로 이루어진 외부함수의 **관절**을 만들어내고 이것들이 모여 비선형적인 모습을 만들어 내는게 가능해진다는거고, 즉 activation함수라는게 선형함수를 비선형적으로 만들어준다는게 이런 원리였다는 거다.

![image-20250503131458683](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503131458683.png)

![image-20250503132127308](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250503132127308.png)

여기를 보면 polytopes,다면체는 각각의 output에 같다고 하는데, 이건 앞에서 입력값 1차원에서 아웃풋 여려개일떄 모델 선들이이 각각 joint에 따라 변할 떄 joint의 위치(x좌표)만큼은 같다는걸 봤는데, 그건 같은 ativation함수를 쓰고 같은 히든유닛을 공유하기 떄문에 joint의 위치는 같다는걸 다각형, input값이 여러개일 떄로 확장한거다. 

### 조(다면체) = 동일

- 모든 출력이 같은 은닉층을 공유하기 때문에 입력 공간이 나뉘는 패턴(다면체 경계)은 동일

- 어떤 입력 점이 어떤 다면체에 속하는지는 모든 출력에 대해 동일

### 값(선형 함수) = 다름

- 각 출력은 같은 다면체 영역 내에서도 서로 다른 값을 가짐

- 이는 각 출력이 은닉층에서 나온 같은 활성화 패턴에 자신만의 가중치를 곱하기 때문

### 시각화

- 지형도를 여러 장 겹쳐놓은 것처럼 생각할 수 있음

- 산과 계곡의 위치(다면체 경계)는 모든 지도에서 같지만

- 각 지도마다 높이(값)는 다를 수 있음.

$$
y = f[x, \phi] = \phi_0 + \phi_1\text{a}[\theta_{10} + \theta_{11}x] + \phi_2\text{a}[\theta_{20} + \theta_{21}x] + \phi_3\text{a}[\theta_{30} + \theta_{31}x]
$$

