---
key:
title: 'ë³€ì´í˜• ì˜¤í† ì¸ì½”ë” ì½”ë“œ ì–¼êµ´ìƒì„± íŒŒì´í† ì¹˜ë¡œ'
excerpt: 'ë³€ì´í˜• ì˜¤í† ì¸ì½”ë”'
tags: [VAE]
---

```py
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets,transforms
import matplotlib.pyplot as plt
import numpy as np

#í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •
IMAGE_SIZE = 64
CHANNELS = 3
BATCH_SIZE = 128
NUM_FEATURES = 64
Z_DIM = 200
LEARNING_RATE = 0.0005
EPOCHS = 10
BETA = 2000
LOAD_MODEL = False
```

```py
import os
import zipfile
import gdown

def download_and_extract_celeba():
    dataset_folder = "./img_align_celeba"
    zip_file = 'img_align_celeba.zip'

    if not os.path.exists(dataset_folder):
        if not os.path.exists(zip_file):
            print("CelebA ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤.")
            gdown.download(id='15gJhiDBkltMQz3T97xG-fO4gXTKAWkSB', output=zip_file, quiet=False)
        print("ë‹¤ìš´ë¡œë“œ ì™„ë£Œ, ì••ì¶•ì„ í•´ì œ ì¤‘...")
        with zipfile.ZipFile(zip_file, 'r') as zip_ref:
            zip_ref.extractall("./data/")
        print("ì••ì¶• í•´ì œ ì™„ë£Œ!")
    else:
        print("ë°ì´í„°ì…‹ í´ë”ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.")

#ì‹¤í–‰
download_and_extract_celeba()

device = torch.device("mps" if torch.mps.is_available() else "cpu")
print(f"Using device: {device}")
```

```py
from torch.utils.data import random_split
# ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì •ì˜
data_transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),  # ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¼€ë¼ìŠ¤ì˜ image_sizeì™€ ë™ì¼í•˜ê²Œ ì¡°ì •
    transforms.ToTensor(),                          # ì´ë¯¸ì§€ë¥¼ [0, 1] ë²”ìœ„ì˜ Tensorë¡œ ë³€í™˜
    # í•„ìš”ì— ë”°ë¼ ì¶”ê°€ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

#ì „ì²´ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê¸°

# ë°ì´í„°ì…‹ ë¡œë“œ
# ImageFolderí•¨ìˆ˜ëŠ” ì§€ì •í•œ root í´ë” ë‚´ì˜ ì„œë¸Œí´ë” êµ¬ì¡°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¼ë²¨ì„ ìë™ ìƒì„±í•˜ì§€ë§Œ,
# ë§Œì•½ ë¼ë²¨ ì—†ì´ ì´ë¯¸ì§€ íŒŒì¼ë§Œ ìˆëŠ” ê²½ìš°, ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ì½ì–´ì˜¤ê²Œ ë©ë‹ˆë‹¤.
data_dir = './data/img_align_celeba/'
full_dataset = datasets.ImageFolder(root=data_dir, transform=data_transform)

#ë°ì´í„°ì…‹ í¬ê¸°ì™€ ë¶„í•  ë¹„ìœ¨ ê²°ì • (80% í›ˆë ¨, 20% í…ŒìŠ¤íŠ¸)
dataset_size = len(full_dataset)
train_size = int(0.8 * dataset_size)
test_size = dataset_size - train_size

# random_splitì„ ì´ìš©í•´ ì „ì²´ ë°ì´í„°ì…‹ì„ train/testë¡œ ë¶„í• 
train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])


# DataLoaderë¥¼ í†µí•´ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ì œê³µ
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)

print("ì´ ì´ë¯¸ì§€ ê°œìˆ˜:", len(full_dataset))
print("í›ˆë ¨ ì´ë¯¸ì§€ ê°œìˆ˜:", len(train_dataset))
print("í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ê°œìˆ˜:", len(test_dataset))

data_iter = iter(train_loader)
image , label = next(data_iter)

print(f"ì´ë¯¸ì§€ í…ì„œ í¬ê¸°: {image.shape}")
print(f"ë ˆì´ë¸” í…ì„œ í¬ê¸°: {label.shape}")
```

```py
def sample_batch(loader):
    #DataLoaderì—ì„œ ì²« ë²ˆì§¸ ë°°ì¹˜ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.
    #ImageFolderë¥¼ ì‚¬ìš©í•˜ë©´ ë°°ì¹˜ë§ˆë‹¤ (ì´ë¯¸ì§€, ë¼ë²¨) íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    #ì—¬ê¸°ì„œëŠ” ì£¼ë¡œ ì‹œê°í™”ìš©ìœ¼ë¡œ ì´ë¯¸ì§€ ë°°ì¹˜ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    batch = next(iter(loader))
    images, _ = batch #ImageFolderí•¨ìˆ˜ëŠ” (ì´ë¯¸ì§€, ë¼ë²¨) íŠœí”Œì„ ë°˜í™˜í•˜ë¯€ë¡œ ì—¬ê¸°ì„  ë¼ë²¨ í•„ìš” ì—†ìœ¼ë‹ˆ _ë¡œ ìƒëµ
    return images

train_sample = sample_batch(train_loader)
```

```py
from torchvision.utils import make_grid

def show_face_batch(batch, nrow=8):
    """
    ì£¼ì–´ì§„ ì´ë¯¸ì§€ í…ì„œ ë°°ì¹˜ë¥¼ ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜.
    ì¸ì:
      batch: ì´ë¯¸ì§€ í…ì„œ, shape: (N, C, H, W)
      nrow: ê·¸ë¦¬ë“œì—ì„œ í•œ í–‰ì— í‘œì‹œí•  ì´ë¯¸ì§€ ìˆ˜
    """
    # make_gridë¥¼ ì´ìš©í•´ ë°°ì¹˜ ì´ë¯¸ì§€ë¥¼ í•˜ë‚˜ì˜ í° ì´ë¯¸ì§€(grid)ë¡œ ë§Œë“¦.
    # normalize=True ì˜µì…˜ì„ ì£¼ë©´ í”½ì…€ ê°’ì„ [0, 1] ë²”ìœ„ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.
    grid_img = make_grid(batch, nrow=nrow, padding=2, normalize=True)
    
    plt.figure(figsize=(10, 10))
    # í…ì„œì˜ shapeì€ (C, H, W)ì´ë¯€ë¡œ, (H, W, C)ë¡œ ë³€ê²½í•´ matplotlibì—ì„œ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë˜ê²Œ í•©ë‹ˆë‹¤.
    plt.imshow(grid_img.permute(1, 2, 0).cpu().numpy())
    plt.title("í›ˆë ¨ ì„¸íŠ¸ì˜ ì¼ë¶€ ì–¼êµ´ ì´ë¯¸ì§€")
    plt.axis("off")
    plt.show()

# DataLoaderì—ì„œ ë°°ì¹˜ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì˜ˆì‹œ.
# ë§Œì•½ DataLoaderê°€ (ì´ë¯¸ì§€, ë ˆì´ë¸”) íŠœí”Œì„ ë°˜í™˜í•œë‹¤ë©´ imagesë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
show_face_batch(train_sample)
```

![image-20250214102329260](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250214102329260.png)

```py
class VAEFace(nn.Module):
    def __init__(self, image_size = IMAGE_SIZE, latent_dim = Z_DIM , base_fliter = 64):
        super(VAEFace, self).__init__()
        self.image_size = image_size
        self.latent_dim = latent_dim
        self.base_fliter = base_fliter

        #ì¸ì½”ë”
        self.encoder = nn.Sequential(
            nn.Conv2d(3, base_fliter, kernel_size=3, stride=2, padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.Conv2d(base_fliter, base_fliter, kernel_size=3, stride=2,padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.Conv2d(base_fliter, base_fliter, kernel_size=3, stride=2,padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.Conv2d(base_fliter, base_fliter, kernel_size=3, stride=2,padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.Conv2d(base_fliter, base_fliter, kernel_size=3, stride=2,padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
        )

        # 64x64 -> 32x32 -> 16x16 -> 8x8 -> 4x4 -> 2x2, ì±„ë„ì€ ëª¨ë‘ base_filters (64)
        #ì¸ì½”ë”ì˜ ì¶œë ¥ ì°¨ì› ê³„ì‚°
        reduced_size = image_size // 32
        self.flatten_dim = base_fliter * reduced_size * reduced_size

        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)
        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)

        #ë””ì½”ë”
        self.decoder_input = nn.Sequential(
            nn.Linear(latent_dim, base_fliter * reduced_size * reduced_size),
            nn.BatchNorm1d(base_fliter * reduced_size * reduced_size),
            nn.LeakyReLU(0.2)
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(base_fliter, base_fliter, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.ConvTranspose2d(base_fliter, base_fliter, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.ConvTranspose2d(base_fliter, base_fliter, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.ConvTranspose2d(base_fliter, base_fliter, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.ConvTranspose2d(base_fliter, base_fliter, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(base_fliter),
            nn.ReLU(0.2),
            nn.ConvTranspose2d(base_fliter, CHANNELS, stride=1, padding=1, output_padding=0, kernel_size=3),
            nn.Sigmoid()
        )

    def encode(self, x):
        h = self.encoder(x)
        h = h.view(-1, self.flatten_dim)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self,z):
        h = self.decoder_input(z)
        h = h.view(-1, 64, 2, 2) # í‰í‰í•œê±° ë‹¤ì‹œ ì°¨ì› ë§ì¶”ê¸°
        x_recon = self.decoder(h)
        return x_recon
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar
    
model = VAEFace().to(device)
print(model)
```

êµ¬ì¡°ëŠ” 64ì°¨ì›ì˜ í•„í„°ì±„ë„ë¡œ, 5ì¸µì˜ ë ˆì´ì–´ë¥¼ ê±°ì³ ì¸ì½”ë”©ì„ í•˜ê³  reparameterizeë¥¼ í•˜ê³ ë‚˜ì„œ ë§ˆì°¬ê°€ì§€ë¡œ 5ì¸µì˜ ë ˆì´ì–´ë¥¼ ê±°ì³ ë””ì½”ë”©ì„ í•œë‹¤, ì—¬ê¸°ì„œ viewí•¨ìˆ˜ëŠ” flattení•¨ìˆ˜ë¡œ ì¸í•´ í‰í‰í•´ì§„ ì°¨ì›ì„ ë‹¤ì‹œ ë””ì½”ë”ì— ì•Œë§ê²Œ ë„£ì–´ì£¼ëŠ”ê±´ë°, -1ì´ batch sizeì´ê³  ë‚˜ë¨¸ì§€ëŠ” ì¸ì½”ë”ì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì¶œë ¥ê°’ì´ë‹¤. ê·¸ë˜ì„œ model(x)ë¥¼ í•˜ê²Œ ë˜ë©´ forward(self, x)ì— xìë¦¬ì— ë„£ê³  ì‹¤í–‰ì´ ë˜ëŠ”ê±°ë‹¤.

```py
def vae_loss(recon_x, x, mu, logvar, beta=BETA):
    # ì¬êµ¬ì„± ì†ì‹¤: í‰ê· ì œê³±ì˜¤ì°¨(MSE)
    recon_loss = F.mse_loss(recon_x, x, reduction='mean')
    # KL ë°œì‚°: í‰ê· ì„ ë²¡í„° ì°¨ì›(axis=1)ë³„ë¡œ í•©ì‚°í•œ í›„ ì „ì²´ ë°°ì¹˜ì˜ í‰ê· ì„ ì·¨í•¨.
    kl_loss = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1))
    total_loss = beta * recon_loss + kl_loss
    return total_loss, recon_loss, kl_loss
  
import torch.optim as optim

# ì˜µí‹°ë§ˆì´ì € ì •ì˜ (í•™ìŠµë¥ ì€ í•„ìš”ì— ë”°ë¼ ì¡°ì ˆ)
optimizer = optim.Adam(model.parameters(), lr=0.0005)

# ì—í¬í¬ ìˆ˜ ì„¤ì •
num_epochs = 10

# --- Training / Testing Loop í•¨ìˆ˜ ì •ì˜ ---
def train_epoch(model, data_loader, optimizer):
    model.train() #ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¨ë‹¤ê³  ëª…ì‹œí•˜ê¸° ìœ„í•œ ì½”ë“œ
    total_loss, total_recon, total_kl = 0.0, 0.0, 0.0
    for batch_idx, (images, _) in enumerate(data_loader):
        images = images.to(device)
        optimizer.zero_grad()
        recon_images, mu, logvar = model(images)
        loss, recon_loss, kl_loss = vae_loss(recon_images, images, mu, logvar)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        total_recon += recon_loss.item()
        total_kl += kl_loss.item()
    avg_loss = total_loss / len(data_loader)
    avg_recon = total_recon / len(data_loader)
    avg_kl = total_kl / len(data_loader)
    return avg_loss, avg_recon, avg_kl

def test_epoch(model, data_loader):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for images, _ in data_loader:
            images = images.to(device)
            recon_images, mu, logvar = model(images)
            loss, _, _ = vae_loss(recon_images, images, mu, logvar)
            total_loss += loss.item()
    avg_loss = total_loss / len(data_loader)
    return avg_loss

def show_image_examples(model, data_loader, title="ì¬êµ¬ì„± ê²°ê³¼"):
    model.eval()
    with torch.no_grad():
        images, _ = next(iter(data_loader))
        images = images.to(device)
        recon_images, _, _ = model(images)
        # ì›ë³¸ ë° ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ë¥¼ ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ì‹œê°í™”
        orig_grid = make_grid(images.cpu(), nrow=8, padding=2, normalize=True)
        recon_grid = make_grid(recon_images.cpu(), nrow=8, padding=2, normalize=True)
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        axes[0].imshow(orig_grid.permute(1, 2, 0))
        axes[0].set_title("ì›ë³¸ ì´ë¯¸ì§€")
        axes[0].axis("off")
        axes[1].imshow(recon_grid.permute(1, 2, 0))
        axes[1].set_title(title)
        axes[1].axis("off")
        plt.show()

# --- ë©”ì¸ í•™ìŠµ ë£¨í”„ ---
for epoch in range(1, num_epochs + 1):
    train_loss, train_recon, train_kl = train_epoch(model, train_loader, optimizer)
    test_loss = test_epoch(model, test_loader)
    print(f"Epoch {epoch}/{num_epochs} | Train Loss: {train_loss:.4f} (Recon: {train_recon:.4f}, KL: {train_kl:.4f}) | Test Loss: {test_loss:.4f}")
    show_image_examples(model, test_loader, title=f"Epoch {epoch} ì¬êµ¬ì„± ê²°ê³¼")
```

![image-20250214103058951](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250214103058951.png)

Show_image_example í•¨ìˆ˜ëŠ” train loader, í›ˆë ¨ì…‹ì˜ ë°ì´í„°ë¡œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, test loader, ë¯¸ë¦¬ 80:20ìœ¼ë¡œ 20ì„ ë”°ë¡œ ë‚˜ëˆ ë‘” í…ŒìŠ¤íŠ¸ ì…‹ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ë½‘ì•„ë‘” ë°ì´í„°ë¥¼ ê°€ì ¸ë‹¤ train epochë¥¼ í†µí•´ í›ˆë ¨ëœ ëª¨ë¸ì—ë‹¤ê°€ ë„£ì–´ì„œ ì œëŒ€ë¡œ ëŒì•„ê°€ëŠ”ì§€ íŒë‹¨í•œë‹¤, ì¦‰ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ê°€ ì•Œë§ê²Œ ëëŠ”ì§€, ì˜ í›ˆë ¨ì´ ë¬ëŠ”ì§€ë¥¼ ë³´ê¸°ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë„£ê³  ëª¨ë¸ì˜ ì ì¬ê³µê°„ì—ì„œ ë“¤ì–´ì˜¨ ì´ë¯¸ì§€ë°ì´í„°ì— ë§ëŠ” ì˜ì—­ì—ì„œ ë½‘ì€ ì´ë¯¸ì§€ì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ ë¹„êµí•˜ëŠ” ì½”ë“œì´ë‹¤.

```py
# í‘œì¤€ ì •ê·œ ë¶„í¬ì—ì„œ ì ì¬ ê³µê°„ì˜ ì¼ë¶€ í¬ì¸íŠ¸ë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
grid_width, grid_height = (10, 3)
z_sample = np.random.normal(size=(grid_width * grid_height, Z_DIM))

# ìƒ˜í”Œë§ëœ í¬ì¸íŠ¸ ë””ì½”ë”©
z_sample = torch.from_numpy(z_sample).float().to(device)
reconstructions = model.decode(z_sample)

# ë””ì½”ë”©ëœ ì´ë¯¸ì§€ì˜ ê·¸ë¦¬ê¸°
fig = plt.figure(figsize=(18, 5))
fig.subplots_adjust(hspace=0.4, wspace=0.4)

# ì–¼êµ´ ê·¸ë¦¬ë“œ ì¶œë ¥
for i in range(grid_width * grid_height):
    ax = fig.add_subplot(grid_height, grid_width, i + 1)
    ax.axis("off")
    ax.imshow(reconstructions[i, :, :].detach().cpu().numpy().transpose(1,2,0))

plt.show()
```

![image-20250214103443093](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250214103443093.png)

ì ì¬ê³µê°„ì—ì„œ ëœë¤í•œ ê³³ì„ ë½‘ì•„ ë‚˜ì˜¨(ìƒ˜í”Œë§í•œ) í¬ì¸íŠ¸ë¥¼ ë””ì½”ë”©í•´ì„œ ì´ë¯¸ì§€ë¥¼ ì¶œë ¥í•œ ì½”ë“œ, ì—¬ê¸°ì„œ 

```py
ax.imshow(reconstructions[i, :, :].detach().cpu().numpy().transpose(1,2,0))
```

detach()ëŠ” gradë¥¼ ê³„ì‚° ì•ˆí•˜ê² ë‹¤ë¼ê³  ì„ ì–¸í•˜ëŠ” í•¨ìˆ˜ì´ê³  ì´ë˜ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ê·¸ë˜ì•¼ numpyë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê¸° ë–„ë¬¸ì´ë‹¤, ê·¸ë¦¬ê³  plotì€ ë‹¤ë£°ë ¤ë©´ numpyë°°ì—´ì´ê¸° ë•Œë¬¸ì— numpy()í•¨ìˆ˜ë¥¼ ë¶™ì¸ê±°ê³ , cpu()ëŠ” ê·¸ë˜í”½ì¹´ë“œê°™ì€ ê³³ì— í…ì„œê°€ ìˆëŠ” ìƒí™©ì—ì„œëŠ” numpyë¡œ ë³€í™˜í•˜ë ¤ê³  í•  ë–„ ì—ëŸ¬ê°€ ìƒê²¨ì„œ ì €ë ‡ê²Œ í–ˆë‹¤, ê·¸ëŸ¬ê³  ë‚˜ë©´ reconstructions[i, :, :]ëŠ” (3,64,64)ì˜ ë°°ì—´ì´ ë˜ëŠ”ë° imshowì—ì„œ ì´ë¯¸ì§€ë¥¼ ë„ìš¸ë–„ (ê°€ë¡œ,ì„¸ë¡œ,rgbì±„ë„ìˆ˜) ì´ë ‡ê²Œ ì…ë ¥í•´ì•¼í•˜ê¸° ë–„ë¬¸ì— transpose(1,2,0)ë¥¼ í•´ì¤€ë‹¤.

```py
# íŠ¹ì • ë ˆì´ë¸”ì´ ë¶€ì°©ëœ ì–¼êµ´ ë°ì´í„° ë¡œë“œ
LABEL = "Blond_Hair"  # <- ì´ ë ˆì´ë¸” ì„¤ì •
labelled_test = utils.image_dataset_from_directory(
    "./img_align_celeba",
    labels=attributes[LABEL].tolist(),
    color_mode="rgb",
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE,
    shuffle=True,
    seed=42,
    validation_split=0.2,
    subset="validation",
    interpolation="bilinear",
)

labelled = labelled_test.map(lambda x, y: (preprocess(x), y))
```

ì¼€ë¼ìŠ¤ì˜ ì–¼êµ´ ë°ì´í„°ë¡œë“œ ì½”ë“œ, ì—¬ê¸°ì„œ utilsê°€ kerasì—ì„œ ì œê³µí•˜ëŠ” ìœ í‹¸ë¦¬í‹°í•¨ìˆ˜ë¡œ í•´ë‹¹ë ˆì´ë¸”ì˜ ì´ë¦„ì„ ê°€ì§„ ì—´ì˜ ê°’ì„ tolist()í•¨ìˆ˜ë¥¼ í†µí•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì„œ í•´ë‹¹ ë ˆì´ë¸”ì— ê°’ì´ ìˆëŠ” ì´ë¯¸ì§€ë“¤ì„ ëª¨ì€ ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ” ì½”ë“œ

```py
# ë ˆì´ë¸” ë°ì´í„°ì…‹ ë¡œë“œ
attributes = pd.read_csv("./list_attr_celeba.csv")
print(attributes.columns)
attributes.head()
```

Index(['image_id', '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive',       'Bags_Under_Eyes', 'Bald', 'Bangs', 'Big_Lips', 'Big_Nose',       'Black_Hair', 'Blond_Hair', 'Blurry', 'Brown_Hair', 'Bushy_Eyebrows',       'Chubby', 'Double_Chin', 'Eyeglasses', 'Goatee', 'Gray_Hair',       'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open',       'Mustache', 'Narrow_Eyes', 'No_Beard', 'Oval_Face', 'Pale_Skin',       'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks', 'Sideburns',       'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',       'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace',       'Wearing_Necktie', 'Young'],      dtype='object')

|      |   image_id | 5_o_Clock_Shadow | Arched_Eyebrows | Attractive | Bags_Under_Eyes | Bald | Bangs | Big_Lips | Big_Nose | Black_Hair |  ... | Sideburns | Smiling | Straight_Hair | Wavy_Hair | Wearing_Earrings | Wearing_Hat | Wearing_Lipstick | Wearing_Necklace | Wearing_Necktie | Young |
| ---: | ---------: | ---------------: | --------------: | ---------: | --------------: | ---: | ----: | -------: | -------: | ---------: | ---: | --------: | ------: | ------------: | --------: | ---------------: | ----------: | ---------------: | ---------------: | --------------: | ----: |
|    0 | 000001.jpg |               -1 |               1 |          1 |              -1 |   -1 |    -1 |       -1 |       -1 |         -1 |  ... |        -1 |       1 |             1 |        -1 |                1 |          -1 |                1 |               -1 |              -1 |     1 |
|    1 | 000002.jpg |               -1 |              -1 |         -1 |               1 |   -1 |    -1 |       -1 |        1 |         -1 |  ... |        -1 |       1 |            -1 |        -1 |               -1 |          -1 |               -1 |               -1 |              -1 |     1 |
|    2 | 000003.jpg |               -1 |              -1 |         -1 |              -1 |   -1 |    -1 |        1 |       -1 |         -1 |  ... |        -1 |      -1 |            -1 |         1 |               -1 |          -1 |               -1 |               -1 |              -1 |     1 |
|    3 | 000004.jpg |               -1 |              -1 |          1 |              -1 |   -1 |    -1 |       -1 |       -1 |         -1 |  ... |        -1 |      -1 |             1 |        -1 |                1 |          -1 |                1 |                1 |              -1 |     1 |
|    4 | 000005.jpg |               -1 |               1 |          1 |              -1 |   -1 |    -1 |        1 |       -1 |         -1 |  ... |        -1 |      -1 |            -1 |        -1 |               -1 |          -1 |                1 |               -1 |              -1 |     1 |

5 rows Ã— 41 columns

ì´ëŸ¬í•œ êµ¬ì¡°ì´ê¸° ë–„ë¬¸ì— 1ì´ ì•„ë‹Œ(-1)ì¸ ê°’ì€ ë ˆì´ë¸”ì— í•´ë‹¹ë˜ëŠ” í•´ë‹¹ íŠ¹ì§•ì´ ì—†ë‹¤ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ìë™ì ìœ¼ë¡œ ì—†ëŠ” ê°’ìœ¼ë¡œ ì·¨ê¸‰í•´ì„œ ì € labeled_test ë¦¬ìŠ¤íŠ¸ì— ì•ˆë“¤ì–´ê°€ê²Œ ëœë‹¤. ì¦‰ í•´ë‹¹ íŠ¹ì§•ì„ ê°€ì§„ ì´ë¯¸ì§€ë§Œì´ ê±¸ëŸ¬ì§€ê²Œ ëœë‹¤.

```py
def get_vector_from_label(data, vae, embedding_dim, label):
    current_sum_POS = np.zeros(shape=embedding_dim, dtype="float32")
    current_n_POS = 0
    current_mean_POS = np.zeros(shape=embedding_dim, dtype="float32")

    current_sum_NEG = np.zeros(shape=embedding_dim, dtype="float32")
    current_n_NEG = 0
    current_mean_NEG = np.zeros(shape=embedding_dim, dtype="float32")

    current_vector = np.zeros(shape=embedding_dim, dtype="float32")
    current_dist = 0

    print("label: " + label)
    print("images : POS move : NEG move :distance : ğ›¥ distance")
    while current_n_POS < 10000:
        batch = list(data.take(1).get_single_element())
        im = batch[0]
        attribute = batch[1]

        _, _, z = vae.encoder.predict(np.array(im), verbose=0)

        z_POS = z[attribute == 1]
        z_NEG = z[attribute == -1]

        if len(z_POS) > 0:
            current_sum_POS = current_sum_POS + np.sum(z_POS, axis=0)
            current_n_POS += len(z_POS)
            new_mean_POS = current_sum_POS / current_n_POS
            movement_POS = np.linalg.norm(new_mean_POS - current_mean_POS)

        if len(z_NEG) > 0:
            current_sum_NEG = current_sum_NEG + np.sum(z_NEG, axis=0)
            current_n_NEG += len(z_NEG)
            new_mean_NEG = current_sum_NEG / current_n_NEG
            movement_NEG = np.linalg.norm(new_mean_NEG - current_mean_NEG)

        current_vector = new_mean_POS - new_mean_NEG
        new_dist = np.linalg.norm(current_vector)
        dist_change = new_dist - current_dist

        print(
            str(current_n_POS)
            + "    : "
            + str(np.round(movement_POS, 3))
            + "    : "
            + str(np.round(movement_NEG, 3))
            + "    : "
            + str(np.round(new_dist, 3))
            + "    : "
            + str(np.round(dist_change, 3))
        )

        current_mean_POS = np.copy(new_mean_POS)
        current_mean_NEG = np.copy(new_mean_NEG)
        current_dist = np.copy(new_dist)

        if np.sum([movement_POS, movement_NEG]) < 0.08:
            current_vector = current_vector / current_dist
            print("Found the " + label + " vector")
            break

    return current_vector
  
# ì†ì„± ë²¡í„° ì°¾ê¸°
attribute_vec = get_vector_from_label(labelled, vae, Z_DIM, LABEL)
```

label: Blond_Hair
images : POS move : NEG move :distance : ğ›¥ distance
22    : 3.032    : 1.447    : 3.291    : 3.291
44    : 2.159    : 0.92    : 2.689    : -0.603
65    : 1.162    : 0.523    : 2.379    : -0.31
87    : 0.882    : 0.423    : 2.122    : -0.258
110    : 0.683    : 0.316    : 1.907    : -0.215
128    : 0.508    : 0.26    : 1.837    : -0.07
150    : 0.488    : 0.219    : 1.832    : -0.005
171    : 0.395    : 0.201    : 1.792    : -0.04
197    : 0.405    : 0.165    : 1.77    : -0.022
218    : 0.294    : 0.145    : 1.756    : -0.014
241    : 0.28    : 0.132    : 1.737    : -0.019
260    : 0.247    : 0.118    : 1.724    : -0.013
282    : 0.243    : 0.11    : 1.696    : -0.028
302    : 0.216    : 0.102    : 1.633    : -0.063
317    : 0.178    : 0.09    : 1.625    : -0.008
337    : 0.194    : 0.09    : 1.608    : -0.017
356    : 0.17    : 0.089    : 1.587    : -0.02
377    : 0.196    : 0.078    : 1.557    : -0.031
395    : 0.162    : 0.074    : 1.54    : -0.017
415    : 0.158    : 0.068    : 1.563    : 0.023
436    : 0.17    : 0.064    : 1.553    : -0.01
453    : 0.125    : 0.067    : 1.556    : 0.003
472    : 0.114    : 0.058    : 1.562    : 0.006
489    : 0.131    : 0.06    : 1.535    : -0.027
501    : 0.092    : 0.065    : 1.529    : -0.005
525    : 0.139    : 0.056    : 1.523    : -0.006
545    : 0.115    : 0.048    : 1.535    : 0.012
563    : 0.124    : 0.05    : 1.534    : -0.002
588    : 0.115    : 0.044    : 1.532    : -0.001
600    : 0.077    : 0.047    : 1.533    : 0.001
617    : 0.093    : 0.045    : 1.536    : 0.003
639    : 0.102    : 0.046    : 1.527    : -0.009
657    : 0.08    : 0.042    : 1.517    : -0.01
670    : 0.072    : 0.044    : 1.514    : -0.003
688    : 0.094    : 0.041    : 1.514    : 0.001
708    : 0.082    : 0.039    : 1.505    : -0.01
732    : 0.104    : 0.034    : 1.51    : 0.006
752    : 0.083    : 0.034    : 1.511    : 0.001
772    : 0.085    : 0.036    : 1.504    : -0.007
791    : 0.079    : 0.036    : 1.501    : -0.003
810    : 0.078    : 0.032    : 1.496    : -0.005
832    : 0.077    : 0.035    : 1.493    : -0.003
853    : 0.073    : 0.03    : 1.494    : 0.001
875    : 0.074    : 0.029    : 1.495    : 0.002
890    : 0.063    : 0.031    : 1.495    : -0.0
911    : 0.073    : 0.032    : 1.494    : -0.001
927    : 0.057    : 0.028    : 1.49    : -0.004
945    : 0.061    : 0.025    : 1.487    : -0.002
972    : 0.07    : 0.027    : 1.487    : 0.0
990    : 0.061    : 0.029    : 1.488    : 0.0
1009    : 0.062    : 0.026    : 1.491    : 0.003
1026    : 0.06    : 0.025    : 1.487    : -0.004
1044    : 0.059    : 0.025    : 1.484    : -0.004
1066    : 0.06    : 0.024    : 1.48    : -0.003
1089    : 0.062    : 0.023    : 1.481    : 0.001
1109    : 0.06    : 0.026    : 1.473    : -0.008
1131    : 0.059    : 0.022    : 1.474    : 0.001
1145    : 0.045    : 0.025    : 1.473    : -0.001
Found the Blond_Hair vector

ì•„ì›ƒí’‹ê°’ì„ ë³´ë©´ ì•Œë‹¤ì‹œí”¼ í•´ë‹¹ ì†ì„±(blond_hair, ê¸ˆë°œ)ì„ ê°€ì§„ ì´ë¯¸ì§€ë§Œ ê±¸ëŸ¬ì§„ê±¸ ì•Œ ìˆ˜ ìˆë‹¤.

ì´ì œ ì´ê±¸ pytorchë²„ì „ìœ¼ë¡œ ë§Œë“¤ì–´ë³´ìë©´, 

ImageFolderëŠ” í´ë” êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ìë™ í• ë‹¹í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ pytorchì—ì„œ imageFloderë¥¼ ì´ìš©í•´ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ë°©ì‹ì€ imageFloderí•¨ìˆ˜ëŠ” ì„œë¸Œí´ë”ë“¤ì˜ êµ¬ì¡°ì— ì˜í•´ì„œ labelì´ ìë™ì ìœ¼ë¡œ ë‚˜ëˆ ì§€ë¯€ë¡œ

```py
# ImageFolderë¥¼ ì´ìš©í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
# ì£¼ì˜: í´ë” êµ¬ì¡°ëŠ” ì•„ë˜ì™€ ê°™ì´ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# ./img_align_celeba/
#     â”œâ”€â”€ Blond_Hair/
#     â”‚       img1.jpg, img2.jpg, ... 
#     â””â”€â”€ Non_Blond_Hair/
#             img3.jpg, img4.jpg, ...
```

ì´ëŸ°ì‹ìœ¼ë¡œ í´ë”êµ¬ì¡°ê°€ ë˜ìˆì–´ì•¼ í•œë‹¤, ê·¸ëŸ°ë° celebaface ë°ì´í„°ì…‹ì€ ì´ë¯¸ì§€ë“¤ì´ ì €ëŸ°ì‹ìœ¼ë¡œ í´ë”ë¡œ ë‚˜ëˆ ì ¸ìˆì§€ ì•Šê³ , í•˜ë‚˜ì˜ í´ë”(img_align_celeba)ì— ë­‰ì³ì ¸ ìˆê³ , csvíŒŒì¼(list_attr_celeba.csv)ë¡œ labelì„ êµ¬ë³„í•˜ê³  ìˆìœ¼ë¯€ë¡œ í•´ê²°í•˜ëŠ” ë°©ë²•, custom datasetì„ ì‘ì„±í•œë‹¤.

```py
import os
import pandas as pd
from PIL import Image
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms

class CelebACustomDataset(Dataset):
    """
    ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    - root_dir: ëª¨ë“  ì´ë¯¸ì§€ê°€ ì €ì¥ëœ í´ë”ì˜ ê²½ë¡œ
    - csv_file: ì´ë¯¸ì§€ ì†ì„± ì •ë³´ê°€ ê¸°ë¡ëœ CSV íŒŒì¼ ê²½ë¡œ
    - label_column: CSV íŒŒì¼ì—ì„œ ì‚¬ìš©í•  ë ˆì´ë¸” (ì˜ˆ: "Blond_Hair") ì—´ ì´ë¦„
    - transform: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ torchvision.transforms.Compose ê°ì²´
    """
    def __init__(self, root_dir, csv_file, label_column, transform=None):
        self.root_dir = root_dir
        self.transform = transform
        
        # CSV íŒŒì¼ ì½ì–´ì˜¤ê¸° (CSVì˜ í–‰ ìˆœì„œê°€ í´ë” ë‚´ ì´ë¯¸ì§€ ìˆœì„œì™€ ë™ì¼í•˜ë‹¤ê³  ê°€ì •)
        self.labels_df = pd.read_csv(csv_file)
        
        # íŒŒì¼ í•„í„°ë§: í´ë” ë‚´ì˜ ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì„ íƒ (ì •ë ¬í•˜ì§€ ì•Šê³  ì›ë˜ ìˆœì„œ ìœ ì§€)
        self.image_files = [
            f for f in os.listdir(root_dir)
            if os.path.isfile(os.path.join(root_dir, f)) and f.lower().endswith(('.jpg', '.jpeg', '.png'))
        ]
        
        # CSVì—ì„œ ì§€ì •ëœ ë ˆì´ë¸” ì—´ì˜ ê°’ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
        self.labels = self.labels_df[label_column].tolist()

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìƒì„±
        img_path = os.path.join(self.root_dir, self.image_files[idx])
        # ì´ë¯¸ì§€ë¥¼ ì—´ê³  RGB ëª¨ë“œë¡œ ë³€í™˜
        image = Image.open(img_path).convert("RGB")
        if self.transform is not None:
            image = self.transform(image)
        label = self.labels[idx]
        return image, label

# ìƒìˆ˜ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ê°’ ìˆ˜ì •)
IMAGE_SIZE = 64      # ë¦¬ì‚¬ì´ì¦ˆí•  ì´ë¯¸ì§€ í¬ê¸°
BATCH_SIZE = 128      # ë°°ì¹˜ í¬ê¸°
VALIDATION_SPLIT = 0.2

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ì˜ˆ: ë¦¬ì‚¬ì´ì¦ˆ ë° í…ì„œ ë³€í™˜)
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),
    # ì¶”ê°€ ì „ì²˜ë¦¬(ì •ê·œí™” ë“±)ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
])

# ë°ì´í„°ì…‹ ìƒì„±: ì´ë¯¸ì§€ í´ë”ì™€ CSV íŒŒì¼ ê²½ë¡œ, ì‚¬ìš©í•  ë ˆì´ë¸” ì—´ ì´ë¦„ ì§€ì •
dataset = CelebACustomDataset(
    root_dir="./data/img_align_celeba/img_align_celeba",
    csv_file="./data/list_attr_celeba.csv",      # ì˜ˆë¥¼ ë“¤ì–´, CSV íŒŒì¼ì´ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •
    label_column="Blond_Hair",
    transform=transform
)

# ì „ì²´ ë°ì´í„°ì…‹ì„ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• 
dataset_size = len(dataset)
val_size = int(dataset_size * VALIDATION_SPLIT)
train_size = dataset_size - val_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# DataLoader ìƒì„±
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)

# ë°ì´í„° í™•ì¸ (ì˜ˆì œ)
if __name__ == "__main__":
    for images, labels in train_loader:
        print("ì´ë¯¸ì§€ ë°°ì¹˜ í¬ê¸°:", images.shape)
        print("ë ˆì´ë¸”:", labels)
        break
```

ì´ë¯¸ì§€ ë°°ì¹˜ í¬ê¸°: torch.Size([128, 3, 64,64]) ë ˆì´ë¸”: tensor([-1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1,        -1, -1,  1, -1, -1, -1, -1,  1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,        -1,  1, -1, -1,  1, -1, -1, -1, -1,  1, -1, -1,  1,  1, -1, -1, -1, -1,        -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1,  1, -1, -1, -1,  1, -1, -1,        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1, -1,  1, -1, -1,         1,  1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1, -1, -1, -1, -1, -1, -1,        -1, -1])

ê·¸ ë‹¤ìŒì— í•´ì•¼ í•  ì¼ì€ labelë¡œ ë¶€í„° vectorë¥¼ ì–»ì–´ë‚´ëŠ” ì¼, ì¦‰ íŠ¹ì • ì†ì„±(label)ì„ ìˆ«ìë¡œ ë‚˜íƒ€ë‚´ëŠ”, ì†ì„± ë²¡í„°ë¡œ êµ¬í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì§œëŠ” ì¼ì´ë‹¤.

```py
import torch
import numpy as np

def get_vector_from_label(data, vae, embedding_dim, label, device):
    """
    PyTorch í™˜ê²½ì—ì„œ VAE ëª¨ë¸ì˜ ì¸ì½”ë”ë¥¼ ì´ìš©í•´ íŠ¹ì • ì†ì„±(label)ì— ëŒ€í•œ ì†ì„± ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜
    """
    # ì²« ë°°ì¹˜ë¥¼ ê°€ì ¸ì™€ latent dimension í™•ì¸
    data_iter = iter(data)
    try:
        im, attribute = next(data_iter)
    except StopIteration:
        raise ValueError("Data loader is empty.")
    
    im = im.to(device)
    attribute = attribute.to(device)
    
    with torch.no_grad():
        # VAE ëª¨ë¸ì˜ encode ë©”ì„œë“œ ì‚¬ìš©
        mu, logvar = vae.encode(im)
        z = vae.reparameterize(mu, logvar)
    
    # ì‹¤ì œ latent ì°¨ì› í™•ì¸
    latent_dim = z.size(1)
    
    # ì œê³µëœ embedding_dimê³¼ ì‹¤ì œ latent_dimì´ ë‹¤ë¥´ë©´ ê²½ê³  ì¶œë ¥ ë° ì‹¤ì œ ê°’ ì‚¬ìš©
    if embedding_dim != latent_dim:
        print(f"Warning: Provided embedding_dim ({embedding_dim}) does not match actual latent_dim ({latent_dim}). Using {latent_dim}.")
        embedding_dim = latent_dim
    
    # ë³€ìˆ˜ë“¤ì„ ì˜¬ë°”ë¥¸ ì°¨ì›(latent_dim)ìœ¼ë¡œ ì´ˆê¸°í™”
    current_sum_POS = torch.zeros(latent_dim, dtype=torch.float32, device=device)
    current_n_POS = 0
    current_mean_POS = torch.zeros(latent_dim, dtype=torch.float32, device=device)

    current_sum_NEG = torch.zeros(latent_dim, dtype=torch.float32, device=device)
    current_n_NEG = 0
    current_mean_NEG = torch.zeros(latent_dim, dtype=torch.float32, device=device)

    current_vector = torch.zeros(latent_dim, dtype=torch.float32, device=device)
    current_dist = 0.0

    print("label: " + label)
    print("images : POS move : NEG move :distance : Î” distance")
    
    # --- ì²« ë°°ì¹˜ ì²˜ë¦¬ ---
    with torch.no_grad():
        print("Batch labels:", attribute)
        pos_mask = (attribute == 1)
        neg_mask = (attribute == -1)
        
        if pos_mask.sum() > 0:
            z_POS = z[pos_mask]
            current_sum_POS += z_POS.sum(dim=0)
            current_n_POS += int(pos_mask.sum().item())
            # ì²« ë°°ì¹˜ì¸ ê²½ìš°, í‰ê· ì€ ë°”ë¡œ ëˆ„ì í•©/ê°œìˆ˜
            current_mean_POS = current_sum_POS / current_n_POS
            movement_POS = torch.norm(current_mean_POS).item()
        else:
            movement_POS = 0.0
        
        if neg_mask.sum() > 0:
            z_NEG = z[neg_mask]
            current_sum_NEG += z_NEG.sum(dim=0)
            current_n_NEG += int(neg_mask.sum().item())
            current_mean_NEG = current_sum_NEG / current_n_NEG
            movement_NEG = torch.norm(current_mean_NEG).item()
        else:
            movement_NEG = 0.0
        
        current_vector = current_mean_POS - current_mean_NEG
        current_dist = torch.norm(current_vector).item()
        print(f"{current_n_POS:4d} : {movement_POS:6.3f} : {movement_NEG:6.3f} : {current_dist:6.3f} : {0.000:6.3f}")

    # --- ì´í›„ ë°°ì¹˜ ì²˜ë¦¬ ---
    while current_n_POS < 10000:
        try:
            im, attribute = next(data_iter)
        except StopIteration:
            data_iter = iter(data)
            im, attribute = next(data_iter)
        
        im = im.to(device)
        attribute = attribute.to(device)
        
        with torch.no_grad():
            # VAE ëª¨ë¸ì˜ encode ë©”ì„œë“œ ì‚¬ìš©
            mu, logvar = vae.encode(im)
            z = vae.reparameterize(mu, logvar)
        
        pos_mask = (attribute == 1)
        neg_mask = (attribute == -1)
        
        if pos_mask.sum() > 0:
            z_POS = z[pos_mask]
            current_sum_POS += z_POS.sum(dim=0)
            current_n_POS += int(pos_mask.sum().item())
            new_mean_POS = current_sum_POS / current_n_POS
            movement_POS = torch.norm(new_mean_POS - current_mean_POS).item()
        else:
            movement_POS = 0.0
            new_mean_POS = current_mean_POS

        if neg_mask.sum() > 0:
            z_NEG = z[neg_mask]
            current_sum_NEG += z_NEG.sum(dim=0)
            current_n_NEG += int(neg_mask.sum().item())
            new_mean_NEG = current_sum_NEG / current_n_NEG
            movement_NEG = torch.norm(new_mean_NEG - current_mean_NEG).item()
        else:
            movement_NEG = 0.0
            new_mean_NEG = current_mean_NEG

        current_vector = new_mean_POS - new_mean_NEG
        new_dist = torch.norm(current_vector).item()
        dist_change = new_dist - current_dist

        print(f"{current_n_POS:4d} : {movement_POS:6.3f} : {movement_NEG:6.3f} : {new_dist:6.3f} : {dist_change:6.3f}")

        current_mean_POS = new_mean_POS.clone()
        current_mean_NEG = new_mean_NEG.clone()
        current_dist = new_dist

        # ìˆ˜ë ´ì¡°ê±´: ë‘ ê·¸ë£¹ì˜ í‰ê·  ë³€í™”ëŸ‰(movement)ì˜ í•©ì´ ì‘ê³ , ì¶©ë¶„í•œ ìƒ˜í”Œ ìˆ˜ë¥¼ ë´¤ì„ ë•Œ
        if (movement_POS + movement_NEG) < 0.02 and current_n_POS > 500:
            if new_dist != 0:
                current_vector = current_vector / new_dist
            print("Found the " + label + " vector")
            break

    return current_vector.cpu().numpy()
  
attribute_vector = get_vector_from_label(train_loader, model, 200, "Blond_Hair", device)

print("Obtained attribute vector for Blond_Hair:")
print(attribute_vector)
```

```py
def interpolate_attribute(vae, image, attribute_vector, alpha_values, device):
    """
    ì´ë¯¸ì§€ì— ì†ì„± ë²¡í„°ë¥¼ ë‹¤ì–‘í•œ ê°•ë„ë¡œ ì ìš©í•˜ì—¬ ì¡°ì‘ëœ ì´ë¯¸ì§€ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        vae: í•™ìŠµëœ VAE ëª¨ë¸
        image: ì¡°ì‘í•  ì›ë³¸ ì´ë¯¸ì§€ í…ì„œ (1, 3, H, W)
        attribute_vector: ì ìš©í•  ì†ì„± ë²¡í„° (numpy ë°°ì—´ ë˜ëŠ” torch í…ì„œ)
        alpha_values: ì†ì„± ë²¡í„°ë¥¼ ì ìš©í•  ê°•ë„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: [-3, -2, -1, 0, 1, 2, 3])
        device: ì—°ì‚°ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (GPU/CPU)
        
    Returns:
        ì¡°ì‘ëœ ì´ë¯¸ì§€ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
    """
    # ì´ë¯¸ì§€ë¥¼ ì¥ì¹˜ë¡œ ì´ë™í•˜ê³  ë°°ì¹˜ ì°¨ì›ì´ ìˆëŠ”ì§€ í™•ì¸
    if len(image.shape) == 3:  # (C, H, W)
        image = image.unsqueeze(0)  # (1, C, H, W)
    image = image.to(device)
    
    # ì†ì„± ë²¡í„°ë¥¼ torch í…ì„œë¡œ ë³€í™˜
    if isinstance(attribute_vector, np.ndarray):
        attribute_vector = torch.from_numpy(attribute_vector).to(device)
    
    # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
    vae.eval()
    
    # ì´ë¯¸ì§€ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©
    with torch.no_grad():
        mu, logvar = vae.encode(image)
        z = vae.reparameterize(mu, logvar)
        
        # ë‹¤ì–‘í•œ ê°•ë„ë¡œ ì†ì„± ë²¡í„° ì ìš©
        interpolated_images = []
        for alpha in alpha_values:
            # ì ì¬ ë²¡í„°ì— ì†ì„± ë²¡í„° ì ìš©
            modified_z = z + alpha * attribute_vector
            
            # ìˆ˜ì •ëœ ì ì¬ ë²¡í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë””ì½”ë”©
            decoded_image = vae.decode(modified_z)
            interpolated_images.append(decoded_image.squeeze(0).cpu())
    
    return interpolated_images

def visualize_attribute_interpolation(original_image, interpolated_images, alpha_values, attribute_name, figsize=(15, 3)):
    """
    ì›ë³¸ ì´ë¯¸ì§€ì™€ ì†ì„±ì´ ì¡°ì‘ëœ ì´ë¯¸ì§€ë“¤ì„ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        original_image: ì›ë³¸ ì´ë¯¸ì§€ í…ì„œ (C, H, W)
        interpolated_images: ì¡°ì‘ëœ ì´ë¯¸ì§€ í…ì„œë“¤ì˜ ë¦¬ìŠ¤íŠ¸ [(C, H, W), ...]
        alpha_values: ê° ì´ë¯¸ì§€ì— ì ìš©ëœ ì†ì„± ê°•ë„ ê°’ ë¦¬ìŠ¤íŠ¸
        attribute_name: ì¡°ì‘ëœ ì†ì„±ì˜ ì´ë¦„ (ì˜ˆ: "Blond_Hair")
        figsize: ê·¸ë¦¼ í¬ê¸° (width, height)
    """
    import matplotlib.pyplot as plt
    
    # ì›ë³¸ ì´ë¯¸ì§€ì™€ ì¡°ì‘ëœ ì´ë¯¸ì§€ë“¤ì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ê²°í•©
    all_images = [original_image.cpu()] + interpolated_images
    all_alphas = [0] + alpha_values  # ì›ë³¸ ì´ë¯¸ì§€ëŠ” alpha=0
    
    # ì´ë¯¸ì§€ ê°œìˆ˜ì— ë§ê²Œ ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(1, len(all_images), figsize=figsize)
    
    for i, (img, alpha) in enumerate(zip(all_images, all_alphas)):
        # ì´ë¯¸ì§€ ë°ì´í„° ë²”ìœ„ ì •ê·œí™” (0~1)
        if img.max() > 1:
            img = img / 255.0
            
        # ì±„ë„ ìˆœì„œ ë³€ê²½ (C,H,W) -> (H,W,C) for matplotlib
        if img.shape[0] == 3:
            img = img.permute(1, 2, 0)
            
        # ì„œë¸Œí”Œë¡¯ì— ì´ë¯¸ì§€ ì¶œë ¥
        axes[i].imshow(img)
        axes[i].set_title(f"Î± = {alpha}")
        axes[i].axis('off')
    
    plt.suptitle(f"ì†ì„± ì¡°ì‘: {attribute_name}", fontsize=16)
    plt.tight_layout()
    plt.show()

def manipulate_image_with_attribute(vae, image_loader, attribute_vector, attribute_name, alpha_values=None, device=None):
    """
    ë°ì´í„° ë¡œë”ì—ì„œ ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ ì†ì„± ë²¡í„°ë¡œ ì¡°ì‘í•˜ê³  ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤.
    
    Args:
        vae: í•™ìŠµëœ VAE ëª¨ë¸
        image_loader: ì´ë¯¸ì§€ë¥¼ ì œê³µí•˜ëŠ” ë°ì´í„° ë¡œë”
        attribute_vector: ì ìš©í•  ì†ì„± ë²¡í„°
        attribute_name: ì†ì„± ì´ë¦„ (ì˜ˆ: "Blond_Hair")
        alpha_values: ì†ì„± ê°•ë„ ê°’ ë¦¬ìŠ¤íŠ¸ (ê¸°ë³¸ê°’: [-3, -2, -1, 0, 1, 2, 3])
        device: ì—°ì‚° ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: 'cuda' ë˜ëŠ” ëª¨ë¸ì˜ ë””ë°”ì´ìŠ¤)
    """
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì„¤ì •
    if alpha_values is None:
        alpha_values = [-3, -2, -1, 0, 1, 2, 3]
    
    if device is None:
        device = next(vae.parameters()).device
    
    # ë°ì´í„° ë¡œë”ì—ì„œ ì´ë¯¸ì§€ í•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°
    for images, _ in image_loader:
        image = images[0]  # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
        break
    
    # ì›ë³¸ ì´ë¯¸ì§€ì˜ ì¬êµ¬ì„± ì´ë¯¸ì§€ ìƒì„± (ì°¸ì¡°ìš©)
    with torch.no_grad():
        image_batch = image.unsqueeze(0).to(device)
        recon_image, _, _ = vae(image_batch)
        recon_image = recon_image.squeeze(0).cpu()
    
    # ì†ì„± ì¡°ì‘ ì´ë¯¸ì§€ ìƒì„±
    interpolated_images = interpolate_attribute(
        vae, image, attribute_vector, alpha_values, device
    )
    
    # ê²°ê³¼ ì‹œê°í™”
    print(f"ì›ë³¸ ì´ë¯¸ì§€ì™€ '{attribute_name}' ì†ì„± ì¡°ì‘ ê²°ê³¼:")
    visualize_attribute_interpolation(
        image, interpolated_images, alpha_values, attribute_name
    )
    
    # ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ì™€ ì›ë³¸ ë¹„êµ
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(image.permute(1, 2, 0))
    plt.title("ì›ë³¸ ì´ë¯¸ì§€")
    plt.axis('off')
    
    plt.subplot(1, 2, 2)
    plt.imshow(recon_image.permute(1, 2, 0))
    plt.title("ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€")
    plt.axis('off')
    
    plt.suptitle("ì›ë³¸ vs ì¬êµ¬ì„±")
    plt.tight_layout()
    plt.show()

# ì˜ˆì œ: ê¸ˆë°œ ì†ì„± ë²¡í„°ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì¡°ì‘
def run_image_manipulation_example():
    # ì†ì„± ë²¡í„° ë¶ˆëŸ¬ì˜¤ê¸° ë˜ëŠ” ê³„ì‚°í•˜ê¸°
    attribute_vector = get_vector_from_label(val_loader, model, Z_DIM, "Blond_Hair", device)
    
    # ê° ê°•ë„ë³„ë¡œ ì´ë¯¸ì§€ ì¡°ì‘ ë° ì‹œê°í™”
    alpha_values = [-3, -1.5, 0, 1.5, 3]
    manipulate_image_with_attribute(
        model, val_loader, attribute_vector, "Blond_Hair", 
        alpha_values=alpha_values, device=device
    )

# ì´ë¯¸ì§€ ì¡°ì‘ í•¨ìˆ˜ ì‹¤í–‰
run_image_manipulation_example()
```



