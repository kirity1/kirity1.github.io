---
key:
title: '오토인코더 공부'
excerpt: 'AE'
tags: [AE]
---

생성형 인공지능을 만들고자 할 때 쓰이는 기본적인 이론인 VAE에 대해 정리

## 인코딩

먼저 오토인코더란 이름 그대로 자동적으로 인코더 해준다는 말인데, 여기서 인코더한다는 말은 입력데이터 $x$가 있을 떄, 이 입력데이터를 표현하는거를 압축한다, 즉 정보가 최소가 되게, 가장 효율적으로 표현한다? 이런 느낌인듯 하다. 즉 (정보에서) 엔트로피 $E$ 가  줄어드는 방향을 의미하는데, 여기서 가장 효율적으로 정보를 압축하지 못했을 떄, 그 정도를 측정하는게 크로스엔트로피 $CE$ 이다, 아무튼 이게 생성형 인공지능과 무슨 관련이 있냐면

## 오토인코더

이러한 인코딩을 자동적으로 해준다는게 오토 인코더인데, 당연히 컴퓨터가 자동적으로 하겠지만 그러한 오토인코더를 만드는 과정을 신경망을 이용한다.

![AutoEncoder](https://velog.velcdn.com/images/hyeda/post/cef78295-1a18-43c9-9cb6-90a89a7673cf/image.png)

위 그림은 손글씨 이미지 데이터인 숫자 1을 Auto Encoder 모델에 넣은 후 결과값으로 입력 데이터와 비슷한 손글씨 숫자 이미지 1을 출력하는 그림이다. 

여기서 입력데이터는 $28 * 28$ 픽셀로, 784개의 차원을 가진 숫자이미지이다, 여기서 

어떤 숫자인지에 대해서 인식해야만 한다. 당연히 사용자는 생성형 ai에게 "숫자 1이라는 이미지를 출력해줘"라고 요구하기 떄문에 ai는 숫자 1이라는게 무엇인지를 알아야만 한다, 그렇다면 그 컴퓨터가 숫자를 인식하는 과정은?

저번에 배웠던 고유벡터의 주성분 분석같은 개념과 비슷하게 숫자1을 표현하는 입력이미지 $x$를 차원을 줄여나가면서 "샘플링 할 수 있는 단순한 잠재 공간"으로 줄여나간다.

## 샘플링 할 수 있는 잠재 공간?

수학적으로는 고차원 비선형 매니폴드를 샘플링 가능한 단순한 잠재공간으로 변환함을 인코더-디코더기법이라고 하는데 그냥 **고차원을 저차원으로** 낮춰가면서 **이미지의 특징을 저차원으로 표현하게** 하는 걸 의미한다(고차원의 데이터를 저차원의 데이터로 낮추면서 데이터의 특징을 캐치한다).앞에서 배웠던 주성분분석처럼 차원을 낮추면 필요없는 정보를 쳐내고 원하는 정보만 취하는게 가능하다는걸 배웠다, 즉 우리가 원하는건 1이라는 이미지가 이루는 픽셀 모든 것들이 아니라 **1이라는 추상적인 개념을 인지**하는걸 원하는거다.

 즉 저 위의 그림에서는 Latent(잠재공간)의 차원이 784차원에서 2차원으로 낮춤으로써, 숫자 1부터 9까지의 이미지들을 x와 y로 2차원으로 표현하는거다. 여기서 샘플링한다는 말은 그냥 뽑을수 있다, 즉 디코더 할 수 있다 --> 잠재공간에서 1이라는 포인트를 선택하고? 그걸 디코딩을 통해 1이라는 **이미지로 다시 생성할 수도 있다**는 말이다. 즉 **생성이 가능**하다.

그렇다면 이 잠재공간은 앞에 말했듯이 두개의 숫자, 즉 (2D 벡터)로 표현이 가능하다, 신경망을 이용해서 인코더는 디코더가 정확하게 다시 원래 이미지에 가깝게 재구성할 수 있도록 가능한 한 많은 정보를 내포시키려 하기 때문에, 이 벡터(여기서는 2차원 이므로 포인트)를 임베딩이라고 한다. 그리고 그 위치가 1이라는 숫자의 잠재공간에서의 위치가 되겠다.

![img](https://velog.velcdn.com/images/alwls5773/post/669a13df-7eb8-40b7-899a-f730d809897c/image.png)

다시 돌아와서, 결국 앞에 말했던 정보를 최소화한다라는 말은 **입력이미지에서 차원을 낮춰가면서 특징을 포착해낸다** 라는 특성과 관련이 있음을 말한다. 그리고 특징을 포착해낸 모든 벡터(임베딩,여기선 포인트)들을 모아놓은 잠재공간에서 역으로 그 공간에서 포인트를 선택해 **이미지를 생성함(샘플링)도 가능**하다는 것이다.

입력 데이터인 784개의 뉴런들을 500개, 300개, 2개로 압축시킴으로써 입력 데이터의 대표적인 특성을 추출한다. 그리고 이를 기반으로 다시 대칭적인 구조로 300개, 500개 뉴런으로 확장시킨 후 최종적으로 입력 데이터와 똑같은 사이즈인 784개의 뉴런 개수로 최종 값을 출력시킨다. 이러한 구조들로 보통 만들어 진다고 한다.

![img2](https://gdsc-university-of-seoul.github.io/assets/images/post-VAE/img2.png)

잠재공간과 임베딩(최적화된 벡터)을 시각화해본 모습인데, 여기서 빨간색은 0 이런식으로 9까지 분류를 해 놓은 모습인데, 오토 인코더는 레이블을 따로 구별을 안해줘도 어차피 인코딩 하는 과정에서 자기가 스스로 잠재공간에서의 위치를 학습데이터들을 인코더를 통해 포인트들을 일일히 찍으면서 정하기 떄문에, 자동적으로 저렇게 비슷한 데이터들끼리 위치가 모인다. 즉 답을 주지 않고 스스로 학습하게 하는 **비지도 학습**을 하게 된다.

## 요약

### 정보 보존:

중요한 특성은 유지

불필요한 정보는 제거

### 매니폴드 학습:

데이터의 본질적 구조 파악

효율적인 저차원 표현 학습

#### 고차원 데이터의 중요 특성을 포착하고

#### 효율적인 저차원 표현을 학습하는

#### 정보 압축 과정!
