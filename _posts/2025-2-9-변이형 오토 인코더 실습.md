---
key:
title: '변이형 오토인코더 코드 만들어보기'
excerpt: '변이형 오토인코더'
tags: [VAE]
---

```py
# 1. 라이브러리 및 Fashion MNIST 데이터 불러오기
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import numpy as np

# 하이퍼파라미터 설정 (Keras vae_fashion.ipynb와 동일)
IMAGE_SIZE = 32        # 이미지 크기
BATCH_SIZE = 100
EPOCHS = 5
EMBEDDING_DIM = 2      # 잠재 공간 차원
BETA = 50        # 재구성 손실에 대한 가중치

# 데이터 전처리: 이미지 크기 조정 및 Tensor 변환
transform = transforms.Compose([
    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),
    transforms.ToTensor(),  # [0,1] 범위의 텐서로 변환
])

# Fashion MNIST 데이터셋 다운로드 및 DataLoader 구성
train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)

train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# 디바이스 설정 (GPU 사용 가능 시 사용)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

dataiter = iter(train_dataloader)
image , labels = next(dataiter)

print(labels)

images, _ = next(iter(train_dataloader))
print("입력 이미지 shape:", images.shape)  # [batch_size, 1, 32, 32] 형태일 것입니다

plt.figure(figsize=(20,4))
for idx in range(10):
    plt.subplot(1,10, idx + 1)
    img = image[idx].numpy()
    img = np.transpose(img, (1,2,0))
    plt.axis('off')
    plt.imshow(img, cmap='gray', interpolation='none')
    
plt.show
```

```py
# 2. VAE 모델 정의 (인코더, 재파라미터라이제이션, 디코더 포함)
class VAE(nn.Module):
    def __init__(self, image_size=32, latent_dim=2):
        super(VAE, self).__init__()
        self.image_size = image_size
        self.latent_dim = latent_dim
        
        # 인코더: 간단한 합성곱 네트워크 (입력 채널 1 → 32, 32 → 64)
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # 결과: [32, image_size/2, image_size/2]
            nn.ReLU(),
            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # 결과: [64, image_size/4, image_size/4]
            nn.ReLU(),
        )
        # 인코더의 출력 차원 계산 (image_size가 32라면 32//4 = 8)
        reduced_size = image_size // 4
        self.flatten_dim = 64 * reduced_size * reduced_size
        
        # 은닉 표현으로부터 잠재 변수(mean와 log-분산) 계산
        self.fc_mu = nn.Linear(self.flatten_dim, latent_dim)
        self.fc_logvar = nn.Linear(self.flatten_dim, latent_dim)
        
        # 디코더: latent vector에서 시작하여 원본 이미지 크기로 복원
        self.decoder_input = nn.Linear(latent_dim, self.flatten_dim)
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.ReLU(),
            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.Sigmoid(),  # 픽셀 값이 [0,1] 범위가 되도록
        )
    

    def encode(self, x):
        h = self.encoder(x)
        h = h.view(-1, self.flatten_dim)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        h = self.decoder_input(z)
        # [batch, 64, image_size/4, image_size/4] 형태로 reshape
        h = h.view(-1, 64, self.image_size // 4, self.image_size // 4)
        x_recon = self.decoder(h)
        return x_recon
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_recon = self.decode(z)
        return x_recon, mu, logvar

# 모델 생성 후 디바이스로 이동
model = VAE(image_size=IMAGE_SIZE, latent_dim=EMBEDDING_DIM).to(device)
print(model)
```

```py
def loss_function(recon_x, x, mu, logvar, beta):
    # 'mean' 방식으로 계산: 각 배치에서 평균 손실 
    BCE = F.binary_cross_entropy(recon_x, x, reduction='mean')
    KLD = -0.5 * torch.mean(torch.sum(1 + logvar - mu.pow(2) - torch.exp(logvar), dim=1))
    loss = beta * BCE + KLD
    return loss, BCE, KLD
  
def train_model_anneal(model, dataloader, optimizer, epochs, beta, device):
    model.train()
    # 에포크 초반에 KL 가중치가 낮고, 특정 시점부터 1로 고정하도록 설정 (예: 에포크 절반까지 선형 증가)
    for epoch in range(epochs):
        # KL weight : 에포크 절반까지 0에서 1로 선형 증가, 그 후엔 1 유지
        kl_weight = min(1.0, (epoch + 1) / (epochs * 0.5))
        
        total_loss, total_bce, total_kld = 0, 0, 0
        for batch_idx, (data, _) in enumerate(dataloader):
            data = data.to(device)
            optimizer.zero_grad()
            recon, mu, logvar = model(data)
            # beta를 KL weight와 곱해서 사용합니다.
            loss, bce, kld = loss_function(recon, data, mu, logvar, beta * kl_weight)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            total_bce += bce.item()
            total_kld += kld.item()
        
        avg_loss = total_loss / len(dataloader)
        avg_bce = total_bce / len(dataloader)
        avg_kld = total_kld / len(dataloader)
        print(f"Epoch [{epoch+1}/{epochs}], KL Weight: {kl_weight:.4f}, Loss: {avg_loss:.4f}, BCE: {avg_bce:.4f}, KL: {avg_kld:.4f}")

# 옵티마이저 정의 (예: 학습률 1e-3)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

# 모델 학습 실행
train_model_anneal(model, train_dataloader, optimizer, EPOCHS, BETA, device)
```

```py
# 6. 평가: 잠재 공간 시각화
def visualize_latent_space(model, dataloader, device):
    model.eval()
    all_mu = []
    all_labels = []
    with torch.no_grad():
        for data, labels in dataloader:
            data = data.to(device)
            _, mu, _ = model(data)
            all_mu.append(mu.cpu())
            all_labels.append(labels.cpu())
    all_mu = torch.cat(all_mu, dim=0)
    all_labels = torch.cat(all_labels, dim=0)
    
    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(all_mu[:, 0], all_mu[:, 1], c=all_labels, cmap='viridis', s=2)
    plt.colorbar(scatter, label="레이블")
    plt.xlabel("잠재 변수 z[0]")
    plt.ylabel("잠재 변수 z[1]")
    plt.title("잠재 공간 시각화")
    plt.show()

visualize_latent_space(model, test_dataloader, device)
```

```py
# 7. 평가: 원본 이미지와 재구성 이미지 비교
def plot_reconstructions(model, dataloader, device, n=10):
    model.eval()
    data, _ = next(iter(dataloader))
    data = data.to(device)
    with torch.no_grad():
        recon, _, _ = model(data)
    plt.figure(figsize=(20, 4))
    for i in range(n):
        # 원본 이미지 출력
        ax = plt.subplot(2, n, i + 1)
        plt.imshow(data[i].cpu().squeeze(), cmap='gray')
        ax.axis('off')
        # 재구성 이미지 출력
        ax = plt.subplot(2, n, i + n + 1)
        plt.imshow(recon[i].cpu().squeeze(), cmap='gray')
        ax.axis('off')
    plt.suptitle("원본 이미지와 재구성 이미지")
    plt.show()

plot_reconstructions(model, test_dataloader, device, n=10)
```

```py
# 8. 평가: 잠재 공간 격자 샘플링으로 이미지 생성 (잠재 차원이 2인 경우에만 수행)
def plot_latent_grid(model, device, image_size, latent_dim, grid_size=15):
    if latent_dim != 2:
        print("잠재 차원이 2가 아니어서 격자 샘플링을 수행할 수 없습니다.")
        return
    
    model.eval()
    # 표준 정규분포의 분위수(quantile)를 이용한 grid 생성
    grid_x = torch.linspace(-3, 3, grid_size)
    grid_y = torch.linspace(-3, 3, grid_size)
    generated_images = []
    with torch.no_grad():
        for yi in grid_y:
            row_images = []
            for xi in grid_x:
                z = torch.tensor([[xi, yi]], dtype=torch.float32).to(device)
                x_decoded = model.decode(z)
                # 출력 텐서 shape: [1, 1, IMAGE_SIZE, IMAGE_SIZE]
                img = x_decoded.cpu().squeeze().numpy()
                row_images.append(img)
            generated_images.append(row_images)
    
    # 격자 형식의 하나의 큰 이미지로 조합
    figure = np.zeros((grid_size * image_size, grid_size * image_size))
    for i in range(grid_size):
        for j in range(grid_size):
            figure[i * image_size:(i + 1) * image_size, j * image_size:(j + 1) * image_size] = generated_images[i][j]
    
    plt.figure(figsize=(10, 10))
    plt.imshow(figure, cmap='gray')
    plt.title("잠재 공간에서 균등 샘플링하여 생성한 이미지 그리드")
    plt.axis('off')
    plt.show()

plot_latent_grid(model, device, IMAGE_SIZE, EMBEDDING_DIM, grid_size=15)
```

