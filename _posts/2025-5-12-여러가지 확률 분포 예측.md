---
key:
title: '여러가지 확률 분포 예측'
excerpt: 'deeplearning'
tags: [deeplearning]
---

# 원형 데이터 예측을 위한 손실 함수 유도: 폰 미제스 분포와 NLL

## 1. 원형 데이터와 폰 미제스 분포

바람의 방향, 시계 각도, 주기적 현상 등은 $0$ ~ $2\pi$ 또는 $-\pi$ ~ $\pi$ 범위의 **원형 데이터**로 표현된다. 이런 데이터는 일반적인 정규분포로는 적절히 모델링할 수 없으므로, **폰 미제스 분포(von Mises distribution)**가 자주 사용된다.

![image-20250512152157796](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250512152157796.png)

폰 미제스 분포의 확률밀도함수는 다음과 같다:

$$
Pr(y \| \mu, \kappa) = \frac{\exp\left[\kappa \cos(y - \mu)\right]}{2\pi \cdot I_0(\kappa)}
$$

- $y$: 예측하고자 하는 각도(라디안)
- $\mu$: 분포의 평균 방향(중심)
- $\kappa$: 집중도(분산의 역수, 클수록 분포가 좁아짐)
- $I_0(\kappa)$: 0차 변형 베셀 함수 ($I_0(\kappa)$로 표기)

## 2. 신경망과 분포 파라미터 예측

신경망 모델 $f[\mathbf{x}, \phi]$는 입력 $\mathbf{x}$에 대해 평균 방향 $\mu$를 예측한다. 집중도 $\kappa$는 상수로 고정한다고 가정한다.

## 3. 음의 로그 가능도(NLL) 손실 함수 유도

모델이 예측한 $\mu$와 고정된 $\kappa$ 하에서, 실제 관측값 $y_i$가 나올 확률의 로그를 손실 함수로 사용한다. 데이터셋 $\{(\mathbf{x}_i, y_i)\}_{i=1}^I$에 대해 손실 함수는 다음과 같이 유도된다.

$$
L(\phi) = -\sum_{i=1}^I \log Pr(y_i \| f[\mathbf{x}_i, \phi], \kappa)
$$

폰 미제스 분포의 로그를 취하면,

$$
\log Pr(y_i \| \mu, \kappa) = \kappa \cos(y_i - \mu) - \log(2\pi) - \log I_0(\kappa)
$$

따라서 손실 함수는

$$
L(\phi) = -\sum_{i=1}^I \left[ \kappa \cos(y_i - f[\mathbf{x}_i, \phi]) - \log(2\pi) - \log I_0(\kappa) \right]
$$

여기서 $\log(2\pi)$와 $\log I_0(\kappa)$는 상수이므로, 최적화에는 영향을 주지 않는다.  
핵심적으로 $-\kappa \cos(y_i - f[\mathbf{x}_i, \phi])$ 항만 남는다.

## 4. 요약

- 원형 데이터 예측에는 폰 미제스 분포가 적합하다.
- 신경망은 분포의 평균 방향 $\mu$를 예측하고, 집중도 $\kappa$는 상수로 둔다.
- 손실 함수는 음의 로그 가능도(NLL)로, $-\kappa \cos(y_i - f[\mathbf{x}_i, \phi])$의 합 형태가 된다.

# 멀티모달 데이터 예측: 혼합 가우시안 모델과 NLL 손실 함수

## 1. 단일 분포의 한계와 멀티모달 데이터

하나의 입력 $\mathbf{x}$에 대해 여러 개의 유효한 출력 $y$가 존재할 수 있다. 이러한 데이터를 **멀티모달(multimodal) 데이터**라고 한다 (예: 그림 5.14a). 단일 정규분포(가우시안)는 하나의 '봉우리'만을 표현할 수 있기 때문에 이러한 멀티모달 데이터를 적절히 모델링하기 어렵다.

이런 경우, 여러 개의 정규분포(가우시안)를 가중합하여 사용하는 **혼합 가우시안 모델(Mixture of Gaussians, MoG)**이 효과적인 대안이 될 수 있다.

![image-20250512152625763](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250512152625763.png)

## 2. 두 개의 가우시안 혼합 모델

예를 들어, 두 개의 가우시안을 혼합한 모델의 파라미터 집합 $\theta = \{\lambda, \mu_1, \sigma_1^2, \mu_2, \sigma_2^2\}$는 다음과 같이 정의된다.
이때, 출력 $y$에 대한 확률 분포는 다음과 같이 표현된다 (수식 5.35):

$$
Pr(y \| \lambda, \mu_1, \mu_2, \sigma_1^2, \sigma_2^2) = \frac{\lambda}{\sqrt{2\pi\sigma_1^2}} \exp\left[-\frac{(y - \mu_1)^2}{2\sigma_1^2}\right] + \frac{1-\lambda}{\sqrt{2\pi\sigma_2^2}} \exp\left[-\frac{(y - \mu_2)^2}{2\sigma_2^2}\right]
$$

- $\lambda \in [0, 1]$: 두 가우시안 성분의 상대적 가중치(혼합 계수)를 조절한다.
- $\mu_1, \mu_2$: 각 가우시안 성분의 평균이다.
- $\sigma_1^2, \sigma_2^2$: 각 가우시안 성분의 분산이다.

이 모델은 두 개의 봉우리를 가진 분포(그림 5.14b)나, 하나의 봉우리를 가지지만 더 복잡한 형태의 분포(그림 5.14c)를 표현할 수 있다.

## 3. 신경망을 이용한 파라미터 예측 및 손실 함수

신경망 모델 $f[\mathbf{x}, \phi]$는 입력 $\mathbf{x}$를 받아 두 개의 가우시안 혼합 모델의 파라미터들을 예측한다. 손실 함수는 $I$개의 훈련 데이터 쌍 $\{(\mathbf{x}_i, y_i)\}$에 대한 음의 로그 가능도(NLL)로 구성된다.

모델이 예측해야 하는 파라미터는 다음과 같다:
- 혼합 계수 $\lambda$: 신경망 출력 $f_1[\mathbf{x}, \phi]$를 시그모이드 함수 $\text{sig}(\cdot)$에 통과시켜 $\hat{\lambda} = \text{sig}(f_1[\mathbf{x}, \phi])$로 예측 (0과 1 사이 값 보장).
- 첫 번째 가우시안의 평균 $\mu_1$: $\hat{\mu}_1 = f_2[\mathbf{x}, \phi]$.
- 첫 번째 가우시안의 분산 $\sigma_1^2$: $\hat{\sigma}_1^2 = \exp(f_3[\mathbf{x}, \phi])$ (분산이 항상 양수가 되도록 exp 함수 등으로 변환, 또는 문제 조건에 따라 상수로 간주).
- 두 번째 가우시안의 평균 $\mu_2$: $\hat{\mu}_2 = f_4[\mathbf{x}, \phi]$.
- 두 번째 가우시안의 분산 $\sigma_2^2$: $\hat{\sigma}_2^2 = \exp(f_5[\mathbf{x}, \phi])$ (마찬가지로 양수 보장).

각 가우시안 성분 $k$의 확률밀도함수를 $\mathcal{N}(y \| \mu_k, \sigma_k^2)$라고 하면, 손실 함수는 다음과 같다:

$$
L(\phi) = -\sum_{i=1}^I \log \left[ \hat{\lambda}_i \mathcal{N}(y_i \| \hat{\mu}_{1,i}, \hat{\sigma}_{1,i}^2) + (1-\hat{\lambda}_i) \mathcal{N}(y_i \| \hat{\mu}_{2,i}, \hat{\sigma}_{2,i}^2) \right]
$$

여기서 $\hat{\lambda}_i, \hat{\mu}_{1,i}, \hat{\sigma}_{1,i}^2, \hat{\mu}_{2,i}, \hat{\sigma}_{2,i}^2$는 입력 $\mathbf{x}_i$에 대한 신경망의 예측값들이다.

![image-20250512152357779](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250512152357779.png)

여기서 $\lambda$는 가중치니까 0부터 1사이의 값으로 어차피 둘꺼니까 sig 함수를 이용해서 저런식으로 가중치를 모델이 구하는 아웃풋으로 표현해도 된다.

## 4. 추론(Inference) 시 예상되는 문제점

혼합 가우시안 모델을 사용하여 추론(새로운 $y$ 값을 생성하거나 예측)할 때 몇 가지 문제점이 발생할 수 있다:

1.  **모드 붕괴 (Mode Collapse)**: 모델이 데이터의 모든 모드(봉우리)를 학습하지 못하고, 일부 모드에만 집중하거나 하나의 평균적인 모드만 학습하는 현상.
2.  **성분 식별 불가 (Non-identifiability)**: 예를 들어, 첫 번째 가우시안의 파라미터와 두 번째 가우시안의 파라미터를 서로 바꾸고 $\lambda$를 $1-\lambda$로 바꾸어도 전체 혼합 분포는 동일하다. 이는 파라미터의 유일한 해를 찾기 어렵게 만든다.
3.  **수치적 불안정성**: 로그 안에 합계가 들어가는 $\log(\sum \exp(\cdot))$ 형태는 한쪽 항이 매우 작을 경우 수치적으로 불안정해질 수 있다 (Log-Sum-Exp 트릭 등으로 완화 가능).
4.  **샘플링 방식**: $y$를 샘플링하려면 먼저 $\hat{\lambda}$에 따라 두 가우시안 중 하나를 선택한 후, 선택된 가우시안 분포에서 샘플링해야 한다.
5.  **최빈값(Mode) 선택**: 분포에서 가장 가능성이 높은 값을 선택하는 것이 항상 명확하지 않을 수 있다. 각 성분의 평균이 반드시 전체 분포의 최빈값과 일치하지 않을 수 있다.

## 5. 요약

- 멀티모달 데이터는 단일 분포보다 혼합 분포 모델(예: 혼합 가우시안)로 더 잘 표현할 수 있다.
- 신경망은 혼합 분포의 모든 파라미터(가중치, 각 성분의 평균 및 분산)를 예측하도록 학습된다.
- 손실 함수는 음의 로그 가능도를 사용하며, 이는 각 데이터 포인트가 예측된 혼합 분포로부터 나올 로그 확률의 합이다.
- 추론 시에는 모드 붕괴, 식별 불가, 수치적 문제 등 다양한 어려움이 따를 수 있다.

- # 카운트 데이터 예측: 푸아송 분포와 NLL 손실 함수

  ## 1. 카운트 데이터 모델링의 필요성

  특정 시간 동안 발생하는 사건의 횟수, 특정 영역 내의 객체 수 등 **카운트 데이터(count data)**는 음이 아닌 정수 값($y \in \{0, 1, 2, ...\}$)을 갖는다. 예를 들어, "다음 1분 동안 특정 지점을 통과할 보행자 수"를 예측하는 모델을 구축한다고 가정해보자. 이러한 카운트 데이터를 모델링하는 데 적합한 확률 분포 중 하나가 **푸아송 분포(Poisson distribution)**이다.

  ## 2. 푸아송 분포의 이해

  푸아송 분포는 단위 시간 또는 단위 공간에서 특정 사건이 발생하는 평균 횟수를 알고 있을 때, 그 사건이 실제로 특정 횟수만큼 발생할 확률을 나타내는 이산 확률 분포이다.

  푸아송 분포는 단 하나의 파라미터 $\lambda > 0$를 가지며, 이를 **비율(rate)**이라고 부른다. 이 $\lambda$는 분포의 평균이자 분산을 나타낸다. 즉, $\lambda$는 "단위 시간/공간 당 평균 발생 횟수"를 의미한다. 그림 5.15는 $\lambda$ 값에 따른 푸아송 분포의 형태를 보여준다. $\lambda$가 커질수록 분포의 중심이 오른쪽으로 이동하고, 더 넓게 퍼지는 경향을 보인다.

  ![image-20250512152942378](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250512152942378.png)

  푸아송 분포의 확률 질량 함수(Probability Mass Function, PMF)는 다음과 같다:

  $$
  Pr(y = k \| \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}
  $$

  - $k$: 특정 사건이 발생한 횟수 (음이 아닌 정수, $k \in \{0, 1, 2, ...\}$)
  - $\lambda$: 단위 시간/공간 당 사건의 평균 발생 횟수 (비율 파라미터, $\lambda > 0$)
  - $e$: 자연로그의 밑 (약 2.718)
  - $k!$: $k$의 계승 (factorial)

  ## 3. 신경망을 이용한 $\lambda$ 예측 및 손실 함수

  신경망 모델 $f[\mathbf{x}, \phi]$는 입력 데이터 $\mathbf{x}$(예: 시간대, 위치, 동네 유형)를 받아, 해당 조건에서의 평균 발생 횟수인 $\lambda$를 예측한다. $\lambda$는 항상 양수여야 하므로, 신경망의 원시 출력 $f[\mathbf{x}, \phi]$에 지수 함수(exponential function)를 적용하여 $\hat{\lambda} = \exp(f[\mathbf{x}, \phi])$와 같이 양수 값을 보장할 수 있다.

  손실 함수는 $I$개의 훈련 데이터 쌍 $\{(\mathbf{x}_i, y_i)\}$에 대한 음의 로그 가능도(NLL)로 구성된다. $y_i$는 실제 관측된 카운트 값이다.

  $$
  L(\phi) = -\sum_{i=1}^I \log Pr(y_i \| \hat{\lambda}_i)
  $$

  푸아송 분포의 PMF를 대입하면,

  $$
  L(\phi) = -\sum_{i=1}^I \log \left( \frac{\hat{\lambda}_i^{y_i} e^{-\hat{\lambda}_i}}{y_i!} \right)
  $$

  로그의 성질($\log(a/b) = \log a - \log b$, $\log(ab) = \log a + \log b$, $\log(a^b) = b \log a$)을 이용하여 식을 전개하면,

  $$
  L(\phi) = -\sum_{i=1}^I \left( y_i \log \hat{\lambda}_i - \hat{\lambda}_i - \log(y_i!) \right)
  $$

  $$
  L(\phi) = \sum_{i=1}^I \left( \hat{\lambda}_i - y_i \log \hat{\lambda}_i + \log(y_i!) \right)
  $$

  여기서 $\log(y_i!)$ 항은 모델 파라미터 $\phi$와 무관한 상수이므로, 최적화 과정에서 생략할 수 있다 (모델 학습에 영향을 주지 않음). 따라서 최종적인 손실 함수는 다음과 같이 단순화될 수 있다:

  $$
  L(\phi) \propto \sum_{i=1}^I \left( \hat{\lambda}_i - y_i \log \hat{\lambda}_i \right)
  $$

  ## 4. 요약

  - 카운트 데이터는 푸아송 분포를 사용하여 모델링할 수 있다.
  - 푸아송 분포는 단일 파라미터 $\lambda$(비율, 평균 발생 횟수)에 의해 결정된다.
  - 신경망은 입력 $\mathbf{x}$에 대해 $\lambda$를 예측하며, $\lambda$는 항상 양수여야 한다 (예: exp 변환 사용).
  - 손실 함수는 음의 로그 가능도(NLL)를 사용하며, 최종적으로 $\sum (\hat{\lambda}_i - y_i \log \hat{\lambda}_i)$ 형태에 비례한다.
  - 이 손실 함수를 최소화하도록 모델 파라미터 $\phi$를 학습함으로써, 모델은 주어진 입력에 대해 가장 가능성 높은 카운트 값을 예측하는 $\lambda$를 추정하게 된다.

# 딥러닝 손실 함수의 통일된 원리: 확률 분포

딥러닝 모델을 학습시킨다는 것은 결국 모델의 예측과 실제 값 사이의 "오차"를 줄여나가는 과정이다. 이 "오차"를 어떻게 정의하느냐가 바로 손실 함수(Loss Function)의 역할이다. 다양한 문제 유형(회귀, 분류, 카운트 예측 등)에 따라 여러 가지 손실 함수가 사용되지만, 이들은 사실 하나의 깊은 원리를 공유하고 있다. 그 핵심은 바로 **확률 분포**와 **최대 가능도 원리(Maximum Likelihood Estimation, MLE)**에 기반한 **음의 로그 가능도(Negative Log-Likelihood, NLL)**이다.

## 1. 모든 것은 데이터에 적합한 "확률 분포" 가정에서 시작한다

손실 함수를 설계하는 첫 단계는 우리가 풀고자 하는 문제의 출력 데이터 $y$가 어떤 확률 분포를 따를 것이라고 가정하는 것이다.

-   **연속적인 값 예측 (회귀 문제)**: 출력 $y$가 실수 전체 범위의 값을 갖는다면, **정규 분포(Gaussian Distribution)**를 가정할 수 있다. 모델은 이 정규 분포의 평균 $\mu$ (때로는 분산 $\sigma^2$까지)를 예측한다.
-   **이진 분류 (0 또는 1 예측)**: 출력 $y$가 두 가지 범주 중 하나라면, **베르누이 분포(Bernoulli Distribution)**를 가정한다. 모델은 성공 확률 $\lambda$ (예: 클래스 1일 확률)를 예측한다.
-   **다중 클래스 분류 (K개 중 하나 예측)**: 출력 $y$가 $K$개의 범주 중 하나라면, **카테고리 분포(Categorical Distribution)**를 가정한다. 모델은 각 클래스에 속할 확률 $\lambda_k$들을 예측한다 (Softmax 함수를 통해).
-   **카운트 데이터 예측 (0, 1, 2, ... 개수 예측)**: 출력 $y$가 음이 아닌 정수 값을 갖는다면, **푸아송 분포(Poisson Distribution)**를 가정할 수 있다. 모델은 단위 시간/공간 당 평균 발생 횟수인 비율 파라미터 $\lambda$를 예측한다.
-   **원형 데이터 예측 (각도 예측)**: 출력 $y$가 $0 \sim 2\pi$ 범위의 각도라면, **폰 미제스 분포(von Mises Distribution)**를 가정할 수 있다. 모델은 평균 방향 $\mu$ (때로는 집중도 $\kappa$까지)를 예측한다.
-   **멀티모달 데이터 예측 (여러 봉우리를 갖는 분포)**: 출력이 여러 개의 유효한 값을 가질 수 있다면, **혼합 모델(Mixture Model)** (예: 혼합 가우시안, Mixture of Gaussians)을 사용한다. 모델은 각 성분 분포의 파라미터들과 혼합 계수 $\lambda$를 예측한다.

이처럼, **"문제에 맞는 확률 분포를 선택하는 것"이 손실 함수 설계의 첫걸음**이다. 이 선택 자체가 이미 손실 함수의 큰 골격을 결정짓는다.

## 2. 확률 분포의 "레시피"에 모델의 예측과 실제 값을 대입한다

일단 확률 분포를 선택했다면, 그 분포의 확률 질량 함수(PMF, 이산 변수) 또는 확률 밀도 함수(PDF, 연속 변수)가 손실 함수의 기본 재료가 된다. 이 함수는 보통 $Pr(y \| \theta)$ 형태로 표현되는데, 여기서 $\theta$는 분포의 파라미터들(예: $\mu, \sigma^2, \lambda$ 등)을 나타낸다.

손실 함수를 만드는 핵심적인 아이디어는 다음과 같다:

1.  **분포 파라미터 $\theta$ 자리에 신경망 모델의 출력 $f(\mathbf{x}_i, \phi)$를 대입한다.**  
    신경망은 입력 $\mathbf{x}_i$와 내부 가중치 $\phi$를 이용해 분포의 파라미터를 예측한다. 예를 들어 정규 분포에서는 $\mu = f(\mathbf{x}_i, \phi)$가 되고, 푸아송 분포에서는 $\lambda = f(\mathbf{x}_i, \phi)$가 된다. 이때, 신경망의 원시 출력이 분포 파라미터의 제약 조건(예: 분산이나 푸아송의 $\lambda$는 양수여야 함, 베르누이의 $\lambda$는 0과 1 사이여야 함)을 만족하도록 시그모이드, 소프트맥스, 지수 함수(exp) 등의 변환 함수를 거치는 경우가 많다.

2.  **분포의 변수 $y$ 자리에 실제 관측된 정답 레이블 $y_i$를 대입한다.**  
    우리는 모델이 예측한 분포 하에서 "실제로 관측된 값 $y_i$"가 나타날 가능성을 평가하고 싶기 때문이다.

이렇게 대입하고 나면, $Pr(y_i \| f(\mathbf{x}_i, \phi))$는 **"모델이 예측한 분포(파라미터 $f(\mathbf{x}_i, \phi)$에 의해 결정됨)에서 실제 정답 $y_i$가 관측될 확률 또는 확률 밀도"**를 의미하게 된다. 이것이 바로 **가능도(Likelihood)**이다.

## 3. 가능도를 최대화, 즉 음의 로그 가능도(NLL)를 최소화한다

모델 학습의 목표는 모든 훈련 데이터 $\{(\mathbf{x}_i, y_i)\}_{i=1}^I$에 대해 이 가능도를 최대화하는 파라미터 $\phi$를 찾는 것이다. 하지만 최적화 알고리즘은 보통 "최소화" 문제를 다루고, 곱셈보다는 덧셈이 수치적으로 안정적이며 계산이 용이하다.

그래서 다음 두 가지 변환을 거친다:

1.  **로그(Logarithm)를 취한다**: 가능도의 곱셈을 로그 가능도의 덧셈으로 바꾼다 ($\log(\prod_i P_i) = \sum_i \log P_i$).
2.  **음수(-) 부호를 붙인다**: 가능도 "최대화" 문제를 손실 "최소화" 문제로 바꾼다.

이렇게 해서 얻어지는 것이 바로 **음의 로그 가능도(Negative Log-Likelihood, NLL)** 손실 함수이다.

$$
L(\phi) = -\sum_{i=1}^I \log Pr(y_i \| f[\mathbf{x}_i, \phi])
$$

결국, **어떤 확률 분포를 가정하든, 그 분포의 PMF/PDF에 모델 예측값과 실제 정답값을 대입한 후, 음의 로그를 취하고 모든 데이터에 대해 합산하면 그것이 바로 해당 문제에 대한 손실 함수가 된다.**

## 4. 손실 함수 설계의 대원칙

결론적으로, 다양한 딥러닝 손실 함수들은 겉보기에는 달라 보여도 다음과 같은 통일된 원칙에 따라 설계된다고 이해할 수 있다:

1.  **출력 데이터 $y$의 특성에 맞는 확률 분포를 가정한다.** (예: 정규분포, 베르누이 분포, 푸아송 분포 등)
2.  선택한 분포의 **확률 (밀도/질량) 함수** $Pr(y \| \theta)$를 가져온다.
3.  분포의 **파라미터 $\theta$ 자리에 모델의 예측(출력) $f(\mathbf{x}_i, \phi)$** (필요시 변환 함수 적용)을 대입한다.
4.  분포의 **변수 $y$ 자리에 실제 정답 레이블 $y_i$**를 대입한다.
5.  이렇게 얻은 **가능도 $Pr(y_i \| f[\mathbf{x}_i, \phi])$에 음의 로그를 취하고, 모든 데이터 샘플에 대해 합산하여 최종 손실 함수 $L(\phi)$**를 만든다.

이러한 이해는 평균 제곱 오차(MSE)가 왜 정규분포 가정 하에서의 NLL과 연결되는지, 교차 엔트로피가 왜 베르누이/카테고리 분포 가정 하에서의 NLL과 연결되는지를 명확하게 설명해준다. 더 나아가, 새로운 유형의 문제나 데이터를 만났을 때 적절한 확률 분포를 가정하고 위 원칙에 따라 직접 손실 함수를 설계할 수 있는 강력한 기반이 된다. 딥러닝 모델 학습의 핵심은 결국 데이터가 가장 잘 설명될 수 있는 확률 분포의 파라미터를 찾아가는 여정인 것이다.