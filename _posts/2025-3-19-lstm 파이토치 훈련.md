---
key:
title: 'LSTM 생성 파이토치로 훈련과 실행'
excerpt: 'LSTM'
tags: [인공지능]
---

```py
# 장치 설정 (GPU 사용 가능하면 사용)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"사용 장치: {device}")

# LSTM 모델 정의
class LSTMLM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.1):
        super(LSTMLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, x, hidden=None):
        # 임베딩 레이어
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
        
        # LSTM 레이어
        if hidden is None:
            output, hidden = self.lstm(embedded)
        else:
            output, hidden = self.lstm(embedded, hidden)
        
        # 드롭아웃 적용
        output = self.dropout(output)
        
        # 선형 레이어 (각 시퀀스 위치에서 다음 토큰 예측)
        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]
        
        return logits, hidden
    
    def init_hidden(self, batch_size):
        # 히든 스테이트 초기화 (옵션)
        h0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        c0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        return (h0, c0)
```

nn.Module (부모 클래스) , LSTMLM (자식 클래스), 그러므로 super().__init__은 nn.Module에 대한 초기화코드라고 보면 되고, 여기서 각각 nn.LSTM과 nn.embedding 등등은 nn.Module을 상속받은 파이토치 라이브러리에 이미 내장되있는 클래스이다.

구조로 보면 

nn.Module
  ├── nn.Embedding
  ├── nn.LSTM
  ├── nn.Linear
  └── ...

 그러므로

```py 
def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.1):
        super(LSTMLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(hidden_dim, vocab_size)

```

는 이 클래스를 처음 불러올 떄 시작하는 초기화코드이면서 그떄 embedding, lstm, dropout, fc 각각의 인스턴스에 해당하는 클래스들을 nn.module에서 가져와서 정의하는 초기화코드이다, 저번 변이형 오토 인코더는 그냥 내장 클래스를 이용하지않고 직접 vae의 인코더, 디코더, linear 등등 구조를 짠거다. 

- **`nn.Embedding`**: **단어의 룩업 테이블(Lookup Table)**이다. 컴퓨터는 '사과'라는 글자를 모른다. 대신 5번이라는 인덱스를 주면, 이 레이어가 5번에 해당하는 고유한 벡터(숫자들의 배열)를 꺼내준다. 자연어 처리의 필수 요소다.
- **`nn.LSTM`**: 시계열 데이터의 핵심이다. `input_size`에는 임베딩된 벡터의 차원이 들어가고, `hidden_size`는 이 모델이 기억할 수 있는 메모리의 크기를 결정한다.
- **`batch_first=True`**: 매우 중요한 옵션이다. 보통 입력 데이터는 `(배치 크기, 시퀀스 길이, 특징)` 순서로 들어오는데, PyTorch LSTM의 기본값은 `(시퀀스 길이, 배치 크기, ...)`다. 이 헷갈리는 차원 순서를 우리가 익숙한 배치가 먼저 오는 순서로 맞춰준다.

lstm은 상당히 복잡하고 c++같은 로우레벨 언어로 최적화시켜야하는 편이 좋기 떄문에 파이토치 클래스에서 내장되있는 클래스를 활용하기로 했다.

## forward

```py
def forward(self, x, hidden=None):
        # 임베딩 레이어
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]

```

입력된 정수 ID들이 의미를 가진 밀집 벡터(Dense Vector)로 변환된다.

```py/
if hidden is None:
    output, hidden = self.lstm(embedded)
else:
    output, hidden = self.lstm(embedded, hidden)
```

여기서 `hidden` 변수의 처리가 중요하다.

- **`hidden`이 없을 때:** 문장의 시작이거나, 이전 문맥과 관계없는 새로운 데이터를 처리할 때다. LSTM은 내부 상태를 0으로 초기화하고 시작한다.
- **`hidden`이 있을 때:** 이전에 처리하던 문맥을 기억하고 있다가, 그 상태(State)를 이어서 학습한다. 긴 글을 생성할 때 필수적이다.

이 연산의 결과로 나오는 `output`은 각 시점(Time Step)에서의 은닉 상태들이고, `hidden`은 맨 마지막 시점의 상태(Short-term인 $h_n$과 Long-term인 $c_n$의 튜플)다.

```py
output = self.dropout(output)
```

과적합 방지를 위해 LSTM 출력의 일부 뉴런을 랜덤하게 0으로 설정하는데, 0.3 드롭아웃이면 30%의 값이 0으로 변경됨

학습 시에만 활성화되고 추론 시에는 비활성화됨

```py
logits = self.fc(output)  # [batch_size, seq_len, vocab_size]
```

LSTM이 내놓은 결과를 `vocab_size`만큼의 크기로 변환한다. 여기서 **로짓(Logits)**이라는 개념이 등장한다.

### 3. Logits(로짓)이란 무엇인가?

`logits`는 모델이 뱉어내는 **'원시 점수(Raw Score)'**다. 이 점수는 아직 확률이 아니다. 마이너스가 될 수도 있고, 100이 넘을 수도 있다. 전체 흐름은 다음과 같다.

1. **Logits:** 모델이 각 단어가 정답일 가능성을 매긴 날것의 점수.
2. **Softmax:** 로짓을 0~1 사이의 값으로 변환하고, 총합이 1이 되도록 만든다 (확률 변환).
3. **Sampling:** 확률 분포에 따라 다음 단어를 선택한다.

즉, `forward` 함수의 최종 리턴값인 `logits`는 **"내가 보기에 다음 단어로 단어장 1번은 5점, 2번은 -3점, 3번은 10점 정도 적절해 보여"**라고 말하는 모델의 판단 결과물이다. 우리는 나중에 이 점수를 확률로 바꿔서 글을 쓰게 된다.

### forward의 전체 흐름

토큰 ID를 받아 임베딩 벡터로 변환

LSTM으로 시퀀스 처리하고 각 시점의 출력 생성

드롭아웃으로 과적합 방지

선형 레이어로 각 위치에서 다음 단어 확률 계산

예측과 상태 반환

```py
def init_hidden(self, batch_size):
        # 히든 스테이트 초기화 (옵션)
        h0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        c0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        return (h0, c0)
```

여기서 1은 lstm의 층의 개수를 의미한다, 이번 lstm은 다층 구조가 아닌 lstm이기 떄문에 1로 한다.

```py
# 배치 크기가 32인 경우 초기 상태 생성
hidden = model.init_hidden(32)

# 첫 번째 배치 처리
output, hidden = model(input_batch, hidden)

# 다음 배치 처리(이전 상태 유지)
output, hidden = model(next_batch, hidden)
```

이런식으로 foward에다가 hidden이랑 x자리에 batch랑 hidden을 넣어주면 될 듯 하다.

## 텍스트 생성기

```py
# 텍스트 생성기 클래스
class PyTorchTextGenerator:
    def __init__(self, model, idx2token, token2idx, device):
        self.model = model
        self.idx2token = idx2token
        self.token2idx = token2idx
        self.device = device
        self.model.eval()  # 평가 모드로 설정
        
    def sample_from(self, probs, temperature=1.0):
        # 온도를 적용하여 확률 조정
        probs = probs ** (1 / temperature)
        probs = probs / probs.sum()
        
        # 확률에 따라 토큰 샘플링
        sample_idx = torch.multinomial(probs, 1).item()
        return sample_idx, probs.cpu().numpy()
    
    def generate(self, start_prompt, max_tokens=100, temperature=1.0):
        self.model.eval()
        tokens = []
        
        # 시작 프롬프트 토큰화
        for word in start_prompt.lower().split():
            tokens.append(self.token2idx.get(word, self.token2idx['<UNK>']))
        
        info = []  # 생성 과정 정보 저장
        
        # 토큰 생성 루프
        with torch.no_grad():
            for _ in range(max_tokens):
                # 현재까지의 토큰 시퀀스를 텐서로 변환
                x = torch.tensor([tokens], dtype=torch.long).to(self.device)
                
                # 예측
                logits, _ = self.model(x)
                next_token_logits = logits[0, -1, :]  # 마지막 위치의 로짓
                
                # softmax로 확률 변환
                probs = torch.softmax(next_token_logits, dim=0)
                
                # 토큰 샘플링
                next_token, word_probs = self.sample_from(probs, temperature)
                
                # 현재 프롬프트 저장
                current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
                info.append({"prompt": current_prompt, "word_probs": word_probs})
                
                # 생성된 토큰 추가
                tokens.append(next_token)
                
                # 종료 조건: EOS 토큰이나 <PAD> 토큰이 나오면 중단
                if next_token == self.token2idx['<EOS>'] or next_token == self.token2idx['<PAD>']:
                    break
        
        # 생성된 텍스트
        generated_text = " ".join([self.idx2token[idx] for idx in tokens])
        print(f"\n생성된 텍스트:\n{generated_text}\n")
        
        return info
```

클래스 초기화

```py
def __init__(self, model, idx2token, token2idx, device):
    self.model = model
    self.idx2token = idx2token
    self.token2idx = token2idx
    self.device = device
    self.model.eval()  # 평가 모드로 설정
```

- model: 학습된 LSTM 언어 모델

- idx2token: 인덱스→단어 변환 사전 (예: {0: '<PAD>', 1: '<UNK>', 2: '강아지', ...})

- token2idx: 단어→인덱스 변환 사전 (예: {'<PAD>': 0, '<UNK>': 1, '강아지': 2, ...})
- model.eval(): 모델을 평가 모드로 설정(드롭아웃 비활성화)

```py
def sample_from(self, probs, temperature=1.0):
    # 온도를 적용하여 확률 조정
    probs = probs ** (1 / temperature)
    probs = probs / probs.sum()
    
    # 확률에 따라 토큰 샘플링
    sample_idx = torch.multinomial(probs, 1).item()
    return sample_idx, probs.cpu().numpy()
```

### `sample_from`: 창의성을 조절하는 온도(Temperature)

이 함수가 생성기의 성격을 결정한다. `torch.multinomial`을 사용하여 확률 분포에 따라 주사위를 굴리는데, 이때 **Temperature($T$)**라는 개념을 도입하여 확률 분포의 모양을 바꾼다.

수식으로 표현하면 $P_i = \frac{\exp(z_i/T)}{\sum \exp(z_j/T)}$ 와 같다.

- **Low Temperature ($T < 1.0$):** 분포가 뾰족해진다(Sharpening). 확률이 높은 단어는 더 높아지고, 낮은 단어는 더 낮아진다. 결과적으로 모델은 **보수적이고 정확하지만, 다소 반복적인 텍스트**를 생성한다.
- **High Temperature ($T > 1.0$):** 분포가 평평해진다(Flattening). 확률이 낮았던 단어들도 뽑힐 기회가 생긴다. 결과적으로 **창의적이고 다양한 어휘를 쓰지만, 문법이 틀리거나 횡설수설할 위험**이 커진다.

우리는 보통 $T=1.0$을 기준으로 두고, 상황에 따라 조절하며 최적의 값을 찾는다.

- torch.multinomial(): 확률에 따라 토큰을 무작위로 샘플링

```py
def generate(self, start_prompt, max_tokens=100, temperature=1.0):
     tokens = []
   for word in start_prompt.lower().split():
       tokens.append(self.token2idx.get(word, self.token2idx['<UNK>']))
```

1. **초기화:** 사용자가 준 `start_prompt`("옛날 옛적에")를 숫자로 바꾼다.
2. **예측 (`model(x)`):** 현재까지의 문장을 모델에 넣는다. 모델은 문장의 모든 단어에 대해 예측값을 내놓지만, 우리는 **맨 마지막 단어의 다음 예측값(`logits[0, -1, :]`)**에만 관심이 있다.
3. **샘플링:** 위에서 만든 `sample_from` 함수로 다음 단어를 하나 뽑는다.
4. **업데이트:** 뽑힌 단어를 현재 문장 뒤에 붙인다. ("옛날 옛적에 호랑이가")
5. **반복:** 완성된 문장이 나오거나(`<EOS>`), 정해진 길이(`max_tokens`)가 될 때까지 2~4번을 반복한다.

그 후 시작프롬프트를 토큰 id로 변환

```py
with torch.no_grad():
            for _ in range(max_tokens):
                # 현재까지의 토큰 시퀀스를 텐서로 변환
                x = torch.tensor([tokens], dtype=torch.long).to(self.device)
                
                # 예측
                logits, _ = self.model(x)
                next_token_logits = logits[0, -1, :]  # 마지막 위치의 로짓
                
                # softmax로 확률 변환
                probs = torch.softmax(next_token_logits, dim=0)
                
                # 토큰 샘플링
                next_token, word_probs = self.sample_from(probs, temperature)
                
                # 현재 프롬프트 저장
                current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
                info.append({"prompt": current_prompt, "word_probs": word_probs})
                
                # 생성된 토큰 추가
                tokens.append(next_token)
                
                # 종료 조건: EOS 토큰이나 <PAD> 토큰이 나오면 중단
                if next_token == self.token2idx['<EOS>'] or next_token == self.token2idx['<PAD>']:
                    break
        
```

### lstm의 핵심

이 부분이 제일 중요한 핵심인데,

먼저 tokens는 지금까지의 단어들을 정수화시킨 정수리스트이고 [tokens] 이렇게 괄호를 넣은 이유는 배치차원을 넣기 위해서이다, 이것들을 model에 넣어 forward시켜 tokens에 있는 정수토큰들의 다음 토큰(-1은 맨 마지막을 뜻하므로)을 예측하는건데, 그에 대한 출력값 logits(토큰과는 다른, 원시 점수 개념)는 지금 [batch_size, seq_len, vocab_size]를 값으로 가진 리스트이다, 여기서 vocab_size는 앞에 나왔던 거처럼 다음 단어로 나올 확률을 담은 10000개의 값을 가진 거고 이걸 softmax로 들고간다. 그러면 여기서 우리가 원한건 특정 단어(토큰) 뒤에 올 것에 대한 걸 구하고싶은건데,

지금 바깥에 있는 for문은 max_tokens만큼 tokens를 반복하며 돌고 있다, 즉 먼저 처음에

[5, 10, 2] (입력 토큰)
   ↓ (LSTM 처리)
[
  [점수1_1, 점수1_2, ..., 점수1_10000],  # 5 다음에 올 단어 예측
  [점수2_1, 점수2_2, ..., 점수2_10000],  # 10 다음에 올 단어 예측
  [점수3_1, 점수3_2, ..., 점수3_10000]   # 2 다음에 올 단어 예측
]

가 일어나고

1번째 반복:
tokens = [5, 10, 2]
x = torch.tensor([[5, 10, 2]])
logits 형태 = [1, 3, vocab_size]
next_token_logits = logits[0, 2, :] (마지막 위치 = 2번 인덱스)
다음 토큰으로 15 선택
tokens = [5, 10, 2, 15] (여기서 vocab_size가 10000이니까, 40000개의 값이 있는거라 생각해야 한다.)

2번째 반복:
tokens = [5, 10, 2, 15]
x = torch.tensor([[5, 10, 2, 15]])
logits 형태 = [1, 4, vocab_size]
next_token_logits = logits[0, 3, :] (마지막 위치 = 3번 인덱스)
다음 토큰으로 7 선택
tokens = [5, 10, 2, 15, 7] 

...이런 식으로 계속된다.

그렇다는 건 처음 lstm을 설계했을 떄 지금까지의 token을 모은 tokens을 모델에 넣어 처리했었는데, 그 말은 이런 방식대로라면 **lstm이 지금까지의 모든 맥락을, lstm이 내뱉는 값마다 모든 token을 고려할 수 있다.** LSTM은 Long Short-Term Memory의 약자로, "장기-단기 메모리"라는 의미인데, 이름 그대로 긴 시퀀스에서 중요한 맥락을 오랫동안 기억할 수 있도록 특별히 설계된 것으로 지금까지의 맥락을 통해서 다음 단어를 유추한다는 점이 핵심이다. 그거를 모델에서 셀 상태라는 것과 게이트 매커니즘같은걸 통해서 여러 토큰들을 가지고도 **어떤 정보를 버릴지, 어떤 새 정보를 받아들일지, 어떤 셀 상태를 출력할 지**를 결정하기 떄문에 저렇게 토큰을 넣어 작동하는거다.

그 다음 소트프 맥스는 이 10000개의 모든 점수값을 맨마지막 로짓(토큰과 다른, 원시점수 개념) next_token_logits를 모두 다 더하면 1이 되는 확률 값으로 변하게 한다.

```py
next_token, word_probs = self.sample_from(probs, temperature)
```

이제 LSTM 모델이 계산한 확률 분포에서 실제로 다음 토큰을 선택하는 핵심 부분으로 앞에서 짠 sample_from은

온도(안전하게 생성할건지, 창의적으로 생성할건지)에 따라 확률 probs로 토큰을 샘플링 하는 코드였고,

그렇게 내뱉는 값에서 sample_idx가 바로 next_token, 즉 우리가 구하고 싶었던 다음 토큰이다, 즉 여기서 확률에서 토큰으로 바꾸는거다.

```py
current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
```

이건 토큰을 실제 이해할 수 있는 단어로 바꾸는 걸로, 이렇게 문장을 최종적으로 내뱉는 듯 하다.

여기서 idx, 인덱스는 어휘사전이 자주 쓰이는 단어에 따라 인덱스순으로 나열했기 때문에 그게 token을 idx로 바꿈으로써, 실제 단어로 바꿀수 있다, 인덱스는 이미 순서가 정해진 어휘사전이 있기 떄문이다.



```py
   current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
   info.append({"prompt": current_prompt, "word_probs": word_probs})
   tokens.append(next_token)
```

```py
# 모델 초기화 함수
def initialize_model(vocab_size, embedding_dim, hidden_dim, dropout_rate=0.1):
    model = LSTMLM(vocab_size, embedding_dim, hidden_dim, dropout_rate)
    model = model.to(device)
    
    # 손실 함수 및 옵티마이저 설정
    criterion = nn.CrossEntropyLoss(ignore_index=0)  # <PAD> 토큰 무시
    optimizer = optim.Adam(model.parameters())
    
    return model, criterion, optimizer

# 훈련 함수
def train(model, train_loader, criterion, optimizer, epoch, device):
    model.train()
    epoch_loss = 0
    
    # 진행 상황 표시
    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}')
    
    for x, y in progress_bar:
        # 데이터를 GPU로 이동
        x, y = x.to(device), y.to(device)
        
        # 경사 초기화
        optimizer.zero_grad()
        
        # 순방향 전파
        logits, _ = model(x)
        
        # 손실 계산 (reshape 필요)
        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))
        
        # 역전파 및 최적화
        loss.backward()
        optimizer.step()
        
        # 손실 누적
        epoch_loss += loss.item()
        
        # 진행 상황 업데이트
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
    
    return epoch_loss / len(train_loader)

# 검증 함수
def evaluate(model, val_loader, criterion, device):
    model.eval()
    val_loss = 0
    
    with torch.no_grad():
        for x, y in val_loader:
            x, y = x.to(device), y.to(device)
            logits, _ = model(x)
            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))
            val_loss += loss.item()
    
    return val_loss / len(val_loader)
```

```py
        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))
```

이 부분에서 보면

- logits: 모델 출력 [batch_size, seq_len, vocab_size]

- 예: [32, 50, 10000] - 32개 문장, 각 50개 단어, 10000개 어휘 점수

- y: 타겟(정답) [batch_size, seq_len]

- 예: [32, 50] - 32개 문장, 각 50개 단어의 정답 ID

형태인데, CrossEntropyLoss는 다음 형태의 입력을 해야한다

- 예측: [N, C] - N개 샘플, C개 클래스(어휘)

- 타겟: [N] - N개 샘플의 정답 클래스 ID

그러므로 결과값과 정답값을 오차함수에 맞게 변형하는 과정으로 

- logits.size(-1): 마지막 차원 크기(vocab_size) 가져오기

- view(-1, vocab_size): 3D 텐서를 2D로 변형

- 결과: [batch_size×seq_len, vocab_size]

- 예: [32×50, 10000] = [1600, 10000]

가 되고 y 또한 저 과정을 통해서 

- 2D 텐서를 1D로 평탄화

- 결과: [batch_size×seq_len]

- 예: [32×50] = [1600]

가 된다.

lstm모델 또한 loss계산을 할 때 굳이 시퀀스를 구별할 필요가 없다, 왜냐하면 lstm이 이미 문장의 순서와 맥락을 고려해서 처리를 하여 처리 logits를 내뱉었기에 문맥정보는 반영되있기 떄문에 상관 없다. 그러니까 y와 logits의 순서만 맞는 상태면 그냥 값만 오차를 구하면 된다.

```py
# 학습 및 텍스트 생성 코드 (구글 코랩용)
def run_training(train_loader, val_loader, vocab_size, embedding_dim, hidden_dim, 
                epochs, idx2token, token2idx, device):
    # 구글 드라이브 마운트 (코랩에서만 필요)
    import sys
    if 'google.colab' in sys.modules:
        from google.colab import drive
        drive.mount('/content/drive')
        model_save_path = '/content/drive/MyDrive/lstm_models'
        
        # 저장 디렉토리 생성
        import os
        os.makedirs(model_save_path, exist_ok=True)
        print(f"모델이 {model_save_path}에 저장됩니다.")
    else:
        model_save_path = './lstm_models'
        import os
        os.makedirs(model_save_path, exist_ok=True)
    
    # 모델, 손실 함수, 옵티마이저 초기화
    model, criterion, optimizer = initialize_model(vocab_size, embedding_dim, hidden_dim)
    
    # 텍스트 생성기 초기화
    text_generator = PyTorchTextGenerator(model, idx2token, token2idx, device)
    
    # 훈련 시작
    
    for epoch in range(epochs):
        start_time = time.time()
        
        # 훈련
        train_loss = train(model, train_loader, criterion, optimizer, epoch, device)
        
        # 검증
        val_loss = evaluate(model, val_loader, criterion, device)
        
        # 에폭 시간
        epoch_time = time.time() - start_time
        
        # 결과 출력
        print(f"Epoch {epoch+1}/{epochs} | 시간: {epoch_time:.2f}s")
        print(f"훈련 손실: {train_loss:.4f} | 검증 손실: {val_loss:.4f}")
        
        # 텍스트 생성 예시
        text_generator.generate("recipe for", max_tokens=100, temperature=1.0)
        
        
    # 모델 저장 경로 생성
    filename = f"{model_save_path}/best_lstm_model_epoch_{epoch+1}.pt"
    
    # 모델 저장
    torch.save({
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'train_loss': train_loss,
        'val_loss': val_loss,
        'epoch': epoch,
        'vocab': vocab,
        'token2idx': token2idx,
        'idx2token': idx2token
    }, filename)
    print(f"모델 저장됨: {filename}")
    
        
    return model, text_generator
```

```py
# Colab에서 사용할 경우 위젯으로 구현
def interactive_text_generation_colab(text_generator):
    import ipywidgets as widgets
    from IPython.display import display, HTML
    
    prompt_input = widgets.Text(
        value='recipe for',
        placeholder='시작 프롬프트를 입력하세요',
        description='프롬프트:',
        disabled=False
    )
    
    max_tokens_slider = widgets.IntSlider(
        value=100,
        min=10,
        max=500,
        step=10,
        description='최대 토큰:',
        disabled=False,
        continuous_update=False,
        orientation='horizontal',
        readout=True,
        readout_format='d'
    )
    
    temperature_slider = widgets.FloatSlider(
        value=1.0,
        min=0.1,
        max=2.0,
        step=0.1,
        description='온도:',
        disabled=False,
        continuous_update=False,
        orientation='horizontal',
        readout=True,
        readout_format='.1f'
    )
    
    output = widgets.Output()
    
    generate_button = widgets.Button(
        description='텍스트 생성',
        disabled=False,
        button_style='',
        tooltip='클릭하여 텍스트 생성',
        icon='play'
    )
    
    def on_button_clicked(b):
        with output:
            output.clear_output()
            print("생성 중...")
            info = text_generator.generate(
                prompt_input.value, 
                max_tokens=max_tokens_slider.value, 
                temperature=temperature_slider.value
            )
    
    generate_button.on_click(on_button_clicked)
    
    # 위젯 배치
    display(HTML("<h3>대화형 텍스트 생성기</h3>"))
    display(prompt_input, max_tokens_slider, temperature_slider, generate_button, output)


text_generator = model[1]

# 코드 실행 (환경에 따라 적절한 함수 선택)
import sys
if 'google.colab' in sys.modules:
    interactive_text_generation_colab(text_generator)
else:
    interactive_text_generation(text_generator)
```





























