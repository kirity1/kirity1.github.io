---
key:
title: 'LSTM 생성 파이토치로 훈련과 실행'
excerpt: 'LSTM'
tags: [LSTM]
---

```py
# 장치 설정 (GPU 사용 가능하면 사용)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"사용 장치: {device}")

# LSTM 모델 정의
class LSTMLM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.1):
        super(LSTMLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(hidden_dim, vocab_size)
        
    def forward(self, x, hidden=None):
        # 임베딩 레이어
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]
        
        # LSTM 레이어
        if hidden is None:
            output, hidden = self.lstm(embedded)
        else:
            output, hidden = self.lstm(embedded, hidden)
        
        # 드롭아웃 적용
        output = self.dropout(output)
        
        # 선형 레이어 (각 시퀀스 위치에서 다음 토큰 예측)
        logits = self.fc(output)  # [batch_size, seq_len, vocab_size]
        
        return logits, hidden
    
    def init_hidden(self, batch_size):
        # 히든 스테이트 초기화 (옵션)
        h0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        c0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        return (h0, c0)
```

nn.Module (부모 클래스) , LSTMLM (자식 클래스), 그러므로 super().__init__은 nn.Module에 대한 초기화코드라고 보면 되고, 여기서 각각 nn.LSTM과 nn.embedding 등등은 nn.Module을 상속받은 파이토치 라이브러리에 이미 내장되있는 클래스이다.

구조로 보면 

nn.Module
  ├── nn.Embedding
  ├── nn.LSTM
  ├── nn.Linear
  └── ...

 그러므로

```py 
def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout_rate=0.1):
        super(LSTMLM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(
            input_size=embedding_dim,
            hidden_size=hidden_dim,
            batch_first=True
        )
        self.dropout = nn.Dropout(dropout_rate)
        self.fc = nn.Linear(hidden_dim, vocab_size)

```

는 이 클래스를 처음 불러올 떄 시작하는 초기화코드이면서 그떄 embedding, lstm, dropout, fc 각각의 인스턴스에 해당하는 클래스들을 nn.module에서 가져와서 정의하는 초기화코드이다, 저번 변이형 오토 인코더는 그냥 내장 클래스를 이용하지않고 직접 vae의 인코더, 디코더, linear 등등 구조를 짠거다. 

## 1. nn.Embedding

### 기능

- 단어 인덱스를 고정된 크기의 밀집 벡터로 변환하는 룩업 테이블

- 자연어 처리에서 텍스트를 수치적으로 표현하는 데 필수적

### 주요 매개변수

- num_embeddings: 어휘 사전 크기 (단어의 개수)

- embedding_dim: 임베딩 벡터의 크기

## 2. nn.LSTM

### 기능

- Long Short-Term Memory 셀을 구현한 순환 신경망 층

- 시퀀스 데이터 처리 및 장기 의존성 학습

- 텍스트, 시계열 데이터 등의 순차 데이터 모델링에 사용

### 주요 매개변수

- input_size: 입력 특성의 수 (여기서는 embedding_dim)

- hidden_size: 은닉 상태 벡터의 크기

- batch_first: 입력 텐서의 첫 번째 차원이 배치 크기인지 여부

lstm은 상당히 복잡하고 c++같은 로우레벨 언어로 최적화시켜야하는 편이 좋기 떄문에 파이토치 클래스에서 내장되있는 클래스를 활용하기로 했다.

## forward

```py
def forward(self, x, hidden=None):
        # 임베딩 레이어
        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]

```

LSTM 모델의 핵심 부분인 forward 메서드입니다. 이 메서드는 모델에 입력이 들어왔을 때 어떻게 처리되는지를 정의합니다.

- x: 입력 텐서로, 토큰 ID들의 시퀀스 [batch_size, seq_len]

- hidden: 이전 타임스텝의 히든 스테이트(선택적 파라미터)

```py
if hidden is None:
    output, hidden = self.lstm(embedded)
else:
    output, hidden = self.lstm(embedded, hidden)
```

- hidden이 제공되지 않으면 LSTM이 자체적으로 0으로 초기화

- hidden이 제공되면 이전 상태에서 계속 처리 (상태 유지)

- output: 모든 시점의 LSTM 출력 [배치크기, 시퀀스길이, 은닉차원]

- hidden: 마지막 시점의 상태 튜플 (h_n, c_n)

- h_n: 은닉 상태 (hidden state)

- c_n: 셀 상태 (cell state)

```py
output = self.dropout(output)
```

- 과적합 방지를 위해 LSTM 출력의 일부 뉴런을 랜덤하게 0으로 설정

- 예: 0.3 드롭아웃이면 30%의 값이 0으로 변경됨

- 학습 시에만 활성화되고 추론 시에는 비활성화됨

```py
logits = self.fc(output)  # [batch_size, seq_len, vocab_size]
```

- 선형 레이어를 통해 각 위치에서 다음 단어의 확률 분포 생성
- 은닉 차원(hidden_dim)에서 어휘 크기(vocab_size) 차원으로 변환
- 출력 크기: [배치크기, 시퀀스길이, 어휘크기]
- 각 위치의 벡터는 다음 단어에 대한 점수(logits)

여기서 logits라는 개념이 나오는데 전체적인 흐름은

1. 토큰은 어휘 사전에서 각 단어에 할당된 고유 ID입니다

2. 로짓은 모델이 각 단어가 다음에 올 가능성에 대해 매긴 점수입니다

3. 로짓을 softmax로 변환하여 확률을 얻습니다

4. 이 확률에서 샘플링하여 새 토큰을 선택합니다

이렇게 흘러간다. 즉 logits는 모델이 매긴 원시점수 개념이다.

### forward의 전체 흐름

토큰 ID를 받아 임베딩 벡터로 변환

LSTM으로 시퀀스 처리하고 각 시점의 출력 생성

드롭아웃으로 과적합 방지

선형 레이어로 각 위치에서 다음 단어 확률 계산

예측과 상태 반환

```py
def init_hidden(self, batch_size):
        # 히든 스테이트 초기화 (옵션)
        h0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        c0 = torch.zeros(1, batch_size, self.lstm.hidden_size).to(device)
        return (h0, c0)
```

여기서 1은 lstm의 층의 개수를 의미한다, 이번 lstm은 다층 구조가 아닌 lstm이기 떄문에 1로 한다.

```py
# 배치 크기가 32인 경우 초기 상태 생성
hidden = model.init_hidden(32)

# 첫 번째 배치 처리
output, hidden = model(input_batch, hidden)

# 다음 배치 처리(이전 상태 유지)
output, hidden = model(next_batch, hidden)
```

이런식으로 foward에다가 hidden이랑 x자리에 batch랑 hidden을 넣어주면 될 듯 하다.

## 텍스트 생성기

```py
# 텍스트 생성기 클래스
class PyTorchTextGenerator:
    def __init__(self, model, idx2token, token2idx, device):
        self.model = model
        self.idx2token = idx2token
        self.token2idx = token2idx
        self.device = device
        self.model.eval()  # 평가 모드로 설정
        
    def sample_from(self, probs, temperature=1.0):
        # 온도를 적용하여 확률 조정
        probs = probs ** (1 / temperature)
        probs = probs / probs.sum()
        
        # 확률에 따라 토큰 샘플링
        sample_idx = torch.multinomial(probs, 1).item()
        return sample_idx, probs.cpu().numpy()
    
    def generate(self, start_prompt, max_tokens=100, temperature=1.0):
        self.model.eval()
        tokens = []
        
        # 시작 프롬프트 토큰화
        for word in start_prompt.lower().split():
            tokens.append(self.token2idx.get(word, self.token2idx['<UNK>']))
        
        info = []  # 생성 과정 정보 저장
        
        # 토큰 생성 루프
        with torch.no_grad():
            for _ in range(max_tokens):
                # 현재까지의 토큰 시퀀스를 텐서로 변환
                x = torch.tensor([tokens], dtype=torch.long).to(self.device)
                
                # 예측
                logits, _ = self.model(x)
                next_token_logits = logits[0, -1, :]  # 마지막 위치의 로짓
                
                # softmax로 확률 변환
                probs = torch.softmax(next_token_logits, dim=0)
                
                # 토큰 샘플링
                next_token, word_probs = self.sample_from(probs, temperature)
                
                # 현재 프롬프트 저장
                current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
                info.append({"prompt": current_prompt, "word_probs": word_probs})
                
                # 생성된 토큰 추가
                tokens.append(next_token)
                
                # 종료 조건: EOS 토큰이나 <PAD> 토큰이 나오면 중단
                if next_token == self.token2idx['<EOS>'] or next_token == self.token2idx['<PAD>']:
                    break
        
        # 생성된 텍스트
        generated_text = " ".join([self.idx2token[idx] for idx in tokens])
        print(f"\n생성된 텍스트:\n{generated_text}\n")
        
        return info
```

클래스 초기화

```py
def __init__(self, model, idx2token, token2idx, device):
    self.model = model
    self.idx2token = idx2token
    self.token2idx = token2idx
    self.device = device
    self.model.eval()  # 평가 모드로 설정
```

- model: 학습된 LSTM 언어 모델

- idx2token: 인덱스→단어 변환 사전 (예: {0: '<PAD>', 1: '<UNK>', 2: '강아지', ...})

- token2idx: 단어→인덱스 변환 사전 (예: {'<PAD>': 0, '<UNK>': 1, '강아지': 2, ...})
- model.eval(): 모델을 평가 모드로 설정(드롭아웃 비활성화)

```py
def sample_from(self, probs, temperature=1.0):
    # 온도를 적용하여 확률 조정
    probs = probs ** (1 / temperature)
    probs = probs / probs.sum()
    
    # 확률에 따라 토큰 샘플링
    sample_idx = torch.multinomial(probs, 1).item()
    return sample_idx, probs.cpu().numpy()
```

- 온도(temperature) 조절:

- 낮은 온도(예: 0.2): 확률이 높은 단어가 더 많이 선택됨 → 안전하지만 반복적인 텍스트

- 높은 온도(예: 1.5): 확률 분포가 더 균등해짐 → 다양하지만 덜 일관된 텍스트

- 온도 1.0: 원래 모델 확률 그대로 사용

- torch.multinomial(): 확률에 따라 토큰을 무작위로 샘플링

```py
def generate(self, start_prompt, max_tokens=100, temperature=1.0):
     tokens = []
   for word in start_prompt.lower().split():
       tokens.append(self.token2idx.get(word, self.token2idx['<UNK>']))
```

- start_prompt: 생성을 시작할 초기 텍스트

- max_tokens: 최대 생성할 토큰 수

- temperature: 샘플링 다양성 조절

그 후 시작프롬프트를 토큰 id로 변환

```py
with torch.no_grad():
            for _ in range(max_tokens):
                # 현재까지의 토큰 시퀀스를 텐서로 변환
                x = torch.tensor([tokens], dtype=torch.long).to(self.device)
                
                # 예측
                logits, _ = self.model(x)
                next_token_logits = logits[0, -1, :]  # 마지막 위치의 로짓
                
                # softmax로 확률 변환
                probs = torch.softmax(next_token_logits, dim=0)
                
                # 토큰 샘플링
                next_token, word_probs = self.sample_from(probs, temperature)
                
                # 현재 프롬프트 저장
                current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
                info.append({"prompt": current_prompt, "word_probs": word_probs})
                
                # 생성된 토큰 추가
                tokens.append(next_token)
                
                # 종료 조건: EOS 토큰이나 <PAD> 토큰이 나오면 중단
                if next_token == self.token2idx['<EOS>'] or next_token == self.token2idx['<PAD>']:
                    break
        
```

### lstm의 핵심

이 부분이 제일 중요한 핵심인데,

먼저 tokens는 지금까지의 단어들을 정수화시킨 정수리스트이고 [tokens] 이렇게 괄호를 넣은 이유는 배치차원을 넣기 위해서이다, 이것들을 model에 넣어 forward시켜 tokens에 있는 정수토큰들의 다음 토큰(-1은 맨 마지막을 뜻하므로)을 예측하는건데, 그에 대한 출력값 logits(토큰과는 다른, 원시 점수 개념)는 지금 [batch_size, seq_len, vocab_size]를 값으로 가진 리스트이다, 여기서 vocab_size는 앞에 나왔던 거처럼 다음 단어로 나올 확률을 담은 10000개의 값을 가진 거고 이걸 softmax로 들고간다. 그러면 여기서 우리가 원한건 특정 단어(토큰) 뒤에 올 것에 대한 걸 구하고싶은건데,

지금 바깥에 있는 for문은 max_tokens만큼 tokens를 반복하며 돌고 있다, 즉 먼저 처음에

[5, 10, 2] (입력 토큰)
   ↓ (LSTM 처리)
[
  [점수1_1, 점수1_2, ..., 점수1_10000],  # 5 다음에 올 단어 예측
  [점수2_1, 점수2_2, ..., 점수2_10000],  # 10 다음에 올 단어 예측
  [점수3_1, 점수3_2, ..., 점수3_10000]   # 2 다음에 올 단어 예측
]

가 일어나고

1번째 반복:
tokens = [5, 10, 2]
x = torch.tensor([[5, 10, 2]])
logits 형태 = [1, 3, vocab_size]
next_token_logits = logits[0, 2, :] (마지막 위치 = 2번 인덱스)
다음 토큰으로 15 선택
tokens = [5, 10, 2, 15] (여기서 vocab_size가 10000이니까, 40000개의 값이 있는거라 생각해야 한다.)

2번째 반복:
tokens = [5, 10, 2, 15]
x = torch.tensor([[5, 10, 2, 15]])
logits 형태 = [1, 4, vocab_size]
next_token_logits = logits[0, 3, :] (마지막 위치 = 3번 인덱스)
다음 토큰으로 7 선택
tokens = [5, 10, 2, 15, 7] 

...이런 식으로 계속된다.

그렇다는 건 처음 lstm을 설계했을 떄 지금까지의 token을 모은 tokens을 모델에 넣어 처리했었는데, 그 말은 이런 방식대로라면 **lstm이 지금까지의 모든 맥락을, lstm이 내뱉는 값마다 모든 token을 고려할 수 있다.** LSTM은 Long Short-Term Memory의 약자로, "장기-단기 메모리"라는 의미인데, 이름 그대로 긴 시퀀스에서 중요한 맥락을 오랫동안 기억할 수 있도록 특별히 설계된 것으로 지금까지의 맥락을 통해서 다음 단어를 유추한다는 점이 핵심이다. 그거를 모델에서 셀 상태라는 것과 게이트 매커니즘같은걸 통해서 여러 토큰들을 가지고도 **어떤 정보를 버릴지, 어떤 새 정보를 받아들일지, 어떤 셀 상태를 출력할 지**를 결정하기 떄문에 저렇게 토큰을 넣어 작동하는거다.

그 다음 소트프 맥스는 이 10000개의 모든 점수값을 맨마지막 로짓(토큰과 다른, 원시점수 개념) next_token_logits를 모두 다 더하면 1이 되는 확률 값으로 변하게 한다.

```py
next_token, word_probs = self.sample_from(probs, temperature)
```

이제 LSTM 모델이 계산한 확률 분포에서 실제로 다음 토큰을 선택하는 핵심 부분으로 앞에서 짠 sample_from은

온도(안전하게 생성할건지, 창의적으로 생성할건지)에 따라 확률 probs로 토큰을 샘플링 하는 코드였고,

그렇게 내뱉는 값에서 sample_idx가 바로 next_token, 즉 우리가 구하고 싶었던 다음 토큰이다, 즉 여기서 확률에서 토큰으로 바꾸는거다.

```py
current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
```

이건 토큰을 실제 이해할 수 있는 단어로 바꾸는 걸로, 이렇게 문장을 최종적으로 내뱉는 듯 하다.

여기서 idx, 인덱스는 어휘사전이 자주 쓰이는 단어에 따라 인덱스순으로 나열했기 때문에 그게 token을 idx로 바꿈으로써, 실제 단어로 바꿀수 있다, 인덱스는 이미 순서가 정해진 어휘사전이 있기 떄문이다.



```py
   current_prompt = " ".join([self.idx2token[idx] for idx in tokens])
   info.append({"prompt": current_prompt, "word_probs": word_probs})
   tokens.append(next_token)
```

- 현재 프롬프트와 단어 확률을 저장합니다.

- 생성된 토큰을 추가합니다.



























