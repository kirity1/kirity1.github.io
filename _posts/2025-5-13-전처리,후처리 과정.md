---
key:
title: '다변량 회귀에서 특정 항의 치우침을 해결할 방법들'
excerpt: 'deeplearning'
tags: [인공지능]
---

# 다변량 회귀에서 출력 스케일 불균형 문제와 해결책

## 1. 문제 상황: 서로 다른 단위와 범위를 가진 다중 출력 예측

다변량 회귀 문제에서는 하나의 입력 $\mathbf{x}$로부터 여러 개의 연속적인 출력 변수 $\mathbf{y} = (y_1, y_2, ..., y_{D_o})$를 동시에 예측한다. 예를 들어, 어떤 사람에 대한 데이터 $\mathbf{x}$ (나이, 성별, 식습관 등)를 기반으로 그 사람의 키($y_1$, 예: 미터 단위)와 몸무게($y_2$, 예: 킬로그램 단위)를 예측하는 모델을 구축한다고 가정해보자.

여기서 키와 몸무게는 각각 미터(m)와 킬로그램(kg)이라는 서로 다른 단위를 가지며, 값의 범위 또한 현저히 다르다 (예: 키는 $1.5 \sim 2.0$ 사이, 몸무게는 $40 \sim 100$ 사이).

## 2. 출력 스케일 불균형으로 인해 발생하는 문제점

모델 학습 시, 각 출력 변수에 대한 손실(예: 평균 제곱 오차, MSE)을 계산하고 이를 합산하여 전체 손실 함수를 구성하는 것이 일반적이다. 만약 각 출력이 독립적인 정규 분포를 따른다고 가정하고 음의 로그 가능도(NLL)를 사용하면, 이는 각 차원별 제곱 오차의 (가중된) 합과 유사한 형태로 귀결될 수 있다.

이때, 출력 변수들의 단위와 값의 범위가 크게 다르면 다음과 같은 문제점들이 발생할 수 있다:

1.  **손실 함수의 지배 현상 (Loss Domination)**:
    값의 범위가 더 큰 변수(예: 몸무게)에서 발생하는 오차의 절대적인 크기가 다른 변수(예: 키)의 오차보다 훨씬 클 수 있다. 예를 들어, 몸무게 오차가 $2kg$이면 제곱 오차는 $4$이지만, 키 오차가 $0.05m$이면 제곱 오차는 $0.0025$이다. 이 경우, 전체 손실 값과 그에 따른 그래디언트가 주로 몸무게 예측 오차에 의해 결정되어, 모델이 몸무게 예측 성능을 향상시키는 데에만 집중하고 키 예측 성능은 등한시할 수 있다.

2.  **학습 불안정 및 비효율**:
    각 변수에 대한 손실 값의 스케일 차이가 크면, 손실 함수의 표면이 한쪽으로 길게 늘어진 형태가 될 수 있다. 이는 경사 하강법(Gradient Descent)과 같은 최적화 알고리즘이 안정적으로 최적점을 찾아가는 것을 방해하고, 학습 속도를 저해하거나 학습이 특정 방향으로 편향되게 만들 수 있다.

3.  **하이퍼파라미터 튜닝의 어려움**:
    학습률(learning rate)과 같은 하이퍼파라미터를 설정할 때, 모든 출력 변수에 대해 적절한 값을 찾기 어려워진다. 어떤 변수에는 적합한 학습률이 다른 변수에는 너무 크거나 작을 수 있다.

## 3. 문제 해결을 위한 두 가지 접근법

이러한 출력 스케일 불균형 문제를 해결하기 위한 두 가지 주요 접근법은 다음과 같다.

### 해결책 1: 데이터 전처리 (Data Preprocessing) - 정규화 또는 표준화

가장 직접적이고 일반적인 방법은 모델 학습 전에 출력 변수(그리고 종종 입력 변수도)를 **전처리하여 스케일을 유사하게 맞추는 것**이다.

-   **최소-최대 정규화 (Min-Max Normalization)**:
    각 변수의 값을 특정 범위(예: $0$과 $1$ 사이, 또는 $-1$과 $1$ 사이)로 조정한다.
    $$
    y'_d = \frac{y_d - \min(y_d)}{\max(y_d) - \min(y_d)}
    $$
    여기서 $\min(y_d)$와 $\max(y_d)$는 훈련 데이터에서 해당 변수 $d$의 최소값과 최대값이다.

-   **표준화 (Standardization / Z-score Normalization)**:
    각 변수의 값을 평균이 $0$이고 표준편차가 $1$인 분포로 변환한다.
    $$
    y'_d = \frac{y_d - \text{mean}(y_d)}{\text{std\_dev}(y_d)}
    $$
    여기서 $\text{mean}(y_d)$와 $\text{std\_dev}(y_d)$는 훈련 데이터에서 해당 변수 $d$의 평균과 표준편차이다.

**작동 원리**:
데이터 전처리를 통해 모든 출력 변수가 비슷한 값의 범위와 스케일을 갖게 되므로, 각 변수에서 발생하는 오차의 크기가 비슷해진다. 이는 손실 함수에서 각 변수의 기여도를 보다 균형 있게 만들고, 모델 학습을 안정화하며 최적화 효율을 높인다.
모델 예측 후에는 원래 스케일로 되돌리기 위해 역변환(inverse transform)을 수행해야 한다.

### 해결책 2: 모델 기반 분산 예측 (Heteroscedastic Model)

두 번째 접근법은 문제 5.8에서 다룬 **이분산성(heteroscedastic) 모델**을 활용하는 것이다. 이 방식은 신경망이 각 출력 차원 $d$에 대해 평균 $\mu_d$뿐만 아니라 **분산 $\sigma_d^2$도 데이터의 함수로 직접 예측**하도록 하는 것이다.

각 차원이 독립적인 정규 분포를 따른다고 가정할 때, 음의 로그 가능도(NLL) 손실 함수는 다음과 같은 형태를 띤다 (상수항 제외):

$$
L(\phi) \propto \sum_{i=1}^I \sum_{d=1}^{D_o} \left( \log(\hat{\sigma}_{id}^2) + \frac{(y_{id} - \hat{\mu}_{id})^2}{\hat{\sigma}_{id}^2} \right)
$$

여기서 $\hat{\mu}_{id} = \mu_d(\mathbf{x}_i, \phi)$ 와 $\hat{\sigma}_{id}^2 = \sigma_d^2(\mathbf{x}_i, \phi)$는 신경망이 예측한 $i$번째 샘플, $d$번째 차원의 평균과 분산이다. (분산 $\hat{\sigma}_{id}^2$는 양수여야 하므로, 신경망 출력에 $\exp(\cdot)$ 등의 함수를 적용한다).

**작동 원리**:
-   **자동 스케일 조정**: 손실 함수의 두 번째 항 $\frac{(y_{id} - \hat{\mu}_{id})^2}{\hat{\sigma}_{id}^2}$를 보면, 제곱 오차가 모델이 예측한 분산 $\hat{\sigma}_{id}^2$으로 나누어진다. 이는 각 출력 차원의 고유한 변동성(스케일)에 따라 오차의 크기를 적절히 조절하는 효과를 낸다.
    -   모델이 특정 출력 $y_d$의 예측에 대해 불확실하다고 판단하여 큰 분산 $\hat{\sigma}_{id}^2$를 예측하면, 해당 제곱 오차항의 기여도는 작아진다 (즉, "불확실하니 좀 틀려도 괜찮다").
    -   모델이 예측에 대해 확신하여 작은 분산 $\hat{\sigma}_{id}^2$를 예측하면, 해당 제곱 오차항의 기여도는 커진다 (즉, "확신했으니 틀리면 큰 페널티").
-   **데이터 기반 가중치**: 이 방식은 각 출력 차원의 손실 기여도를 수동으로 설정하는 대신, 모델이 데이터로부터 각 출력의 예측 불확실성을 학습하고 이를 손실 함수에 반영하여 자동으로 균형을 맞춘다.
-   첫 번째 항 $\log(\hat{\sigma}_{id}^2)$은 모델이 단순히 분산을 무한히 크게 예측하여 손실을 줄이려는 꼼수를 방지하는 정규화(regularization) 역할을 한다.

## 4. 요약

다변량 회귀 문제에서 출력 변수들의 단위와 스케일이 크게 다를 경우, 학습 불균형 등의 문제가 발생할 수 있다. 이를 해결하기 위한 주요 방법은 다음과 같다:

1.  **데이터 전처리**: 정규화(Normalization) 또는 표준화(Standardization)를 통해 모든 출력 변수의 스케일을 학습 전에 유사하게 맞춘다.
2.  **모델 기반 분산 예측**: 이분산성 모델을 사용하여 신경망이 각 출력의 평균과 함께 분산(불확실성)을 예측하고, 이를 손실 함수에 반영하여 각 출력 오차의 기여도를 데이터 기반으로 자동 조절한다.

이러한 방법들은 모델이 모든 출력 변수를 균형 있게 학습하고, 보다 안정적이고 효율적으로 최적화될 수 있도록 돕는다.