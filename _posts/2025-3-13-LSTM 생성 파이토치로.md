---
key:
title: 'LSTM 생성 파이토치로 데이터 전처리'
excerpt: 'LSTM'
tags: [인공지능]
---

```py
import numpy as np
import json
import re
import string
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
from collections import Counter
import torch.optim as optim

VOCAB_SIZE = 10000
MAX_LEN = 200
EMBEDDING_DIM = 100
N_UNITS = 128
VALIDATION_SPLIT = 0.2
SEED = 42
LOAD_MODEL = False
BATCH_SIZE = 32
EPOCHS = 25
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

import sys

# 코랩일 경우 데이터셋 다운로드
if 'google.colab' in sys.modules:
    # 캐글-->Setttings-->API-->Create New Token에서
    # kaggle.json 파일을 만들어 코랩에 업로드하세요.
    from google.colab import files
    files.upload()
    !mkdir ~/.kaggle
    !cp kaggle.json ~/.kaggle/
    !chmod 600 ~/.kaggle/kaggle.json
    # epirecipes 데이터셋을 다운로드하고 압축을 해제합니다.
    !kaggle datasets download -d hugodarwood/epirecipes
    !unzip -q epirecipes.zip
```

```py
# 전체 데이터셋 로드
with open("./full_format_recipes.json") as json_data:
    recipe_data = json.load(json_data)

# 데이터셋 필터링
filtered_data = [
    "Recipe for " + x["title"] + " | " + " ".join(x["directions"])
    for x in recipe_data
    if "title" in x
    and x["title"] is not None
    and "directions" in x
    and x["directions"] is not None
]

# 구두점을 분리하여 별도의 '단어'로 취급합니다.
def pad_punctuation(s):
    s = re.sub(f"([{string.punctuation}])", r" \1 ", s)
    s = re.sub(" +", " ", s)
    return s

text_data = [pad_punctuation(x) for x in filtered_data]
```

json.load() 함수는 Python의 내장 json 모듈에 포함된 함수로, JSON 형식으로 작성된 파일을 Python 객체로 변환하는 역할.

- JSON 파일을 Python 데이터 구조(딕셔너리, 리스트 등)로 변환합니다

- 파일 객체(file object)에서 직접 읽어 들입니다

- JSON 객체 {} → Python 딕셔너리

- JSON 배열 [] → Python 리스트

- JSON 문자열 → Python 문자열

- JSON 숫자(정수, 실수) → Python int/float

- JSON true/false → Python True/False

- JSON null → Python None

```py
filtered_data = [
    "Recipe for " + x["title"] + " | " + " ".join(x["directions"])  # 형식 지정
    for x in recipe_data                                           # 반복 대상
    if "title" in x                                               # 필터링 조건 1
    and x["title"] is not None                                    # 필터링 조건 2
    and "directions" in x                                         # 필터링 조건 3
    and x["directions"] is not None                               # 필터링 조건 4
]
```

레시피 데이터를 필터링하고 텍스트 형식을 지정하는 리스트 컴프리헨션(list comprehension)

```py
print(recipe_data[9])
```

{'directions': ['Chop enough parsley leaves to measure 1 tablespoon; reserve. Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan, covered, 5 minutes.', 'Meanwhile, sprinkle gelatin over water in a medium bowl and let soften 1 minute. Strain broth through a fine-mesh sieve into bowl with gelatin and stir to dissolve. Season with salt and pepper. Set bowl in an ice bath and cool to room temperature, stirring.', 'Toss ham with reserved parsley and divide among jars. Pour gelatin on top and chill until set, at least 1 hour.', 'Whisk together mayonnaise, mustard, vinegar, 1/4 teaspoon salt, and 1/4 teaspoon pepper in a large bowl. Stir in celery, cornichons, and potatoes.', 'Pulse peas with marjoram, oil, 1/2 teaspoon pepper, and 1/4 teaspoon salt in a food processor to a coarse mash.', 'Layer peas, then potato salad, over ham.'], 'fat': 41.0, 'date': '2008-10-23T22:24:26.000Z', 'categories': ['Salad', 'Mustard', 'Potato', 'Picnic', 'Lunch', 'Mayonnaise', 'Ham', 'Pea', 'Summer', 'Gourmet', 'Sugar Conscious', 'Dairy Free', 'Wheat/Gluten-Free', 'Peanut Free', 'Tree Nut Free', 'Soy Free'], 'calories': 602.0, 'desc': 'Transform your picnic into un pique-nique to remember with this elegant arrangement of tangy potato salad, peas gently mashed with marjoram, and salty ham softly set in parsleyed gelatin.', 'protein': 23.0, 'rating': 3.75, 'title': 'Ham Persillade with Mustard Potato Salad and Mashed Peas ', 'ingredients': ['6 long parsley sprigs, divided', '1 3/4 cups reduced-sodium chicken broth', '1 large garlic clove, minced', '2 teaspoon unflavored gelatin (from 1 envelope)', '3 tablespoons water', '1 (3/4-pound) piece baked ham, cut into 1/2-inch cubes (2 cups)', '1/2 cup mayonnaise', '2 tablespoons Dijon mustard', '2 tablespoons white-wine vinegar', '2 celery ribs, finely chopped (1 cup)', '1/4 cup chopped cornichons or sour gherkins', '1 pound boiled potatoes, peeled and cut into 1/2-inch cubes (2 1/2 cups)', '1 (10-ounce) box frozen baby peas, thawed', '2 teaspoons finely chopped marjoram', '3 tablespoons extra-virgin olive oil', 'Equipment: 4 (16-ounce) wide jars or containers with lids', 'Garnish: celery leaves'], 'sodium': 1696.0}

```json
{
    "title": "초콜릿 케이크",
    "directions": ["오븐 예열", "반죽 만들기", "굽기"]
}
```

이런식으로 json파일로 예시로 10번쨰꺼를 보면 저런 구조로 레시피가 쓰여져있는데, 여기서 ai에 학습할 필요한 어휘만 꺼내쓰는게 목적이다.

### 반복 대상

- for x in recipe_data: recipe_data 리스트의 각 레시피(딕셔너리)를 순회

### 필터링 조건

- "title" in x: 레시피에 "title" 키가 존재하는지 확인

- x["title"] is not None: 제목이 None이 아닌지 확인

- "directions" in x: 레시피에 "directions" 키가 존재하는지 확인

- x["directions"] is not None: 조리 방법이 None이 아닌지 확인

### 형식 지정

- "Recipe for " + x["title"]: "Recipe for "라는 접두사 뒤에 레시피 제목 추가

- " | ": 제목과 조리 방법 사이에 구분자 추가

- " ".join(x["directions"]): 조리 단계들(리스트)을 공백으로 연결하여 하나의 문자열로 변환, 즉 "" 라는 빈 공백의 문자열에 directions라는 키를 가진 값들을 x를 변수로 모두 돌면서 저 문자열에 집어넣겠다는 뜻

## 4. 결과

- 각 레시피가 "Recipe for [제목] | [조리 방법]" 형식의 문자열로 변환됨

- 유효하지 않은 레시피(제목이나 조리 방법이 없는)는 제외됨

Recipe for Ham Persillade with Mustard Potato Salad and Mashed Peas | Chop enough parsley leaves to measure 1 tablespoon ; reserve . Chop remaining leaves and stems and simmer with broth and garlic in a small saucepan , covered , 5 minutes . Meanwhile , sprinkle gelatin over water in a medium bowl and let soften 1 minute . Strain broth through a fine - mesh sieve into bowl with gelatin and stir to dissolve . Season with salt and pepper . Set bowl in an ice bath and cool to room temperature , stirring . Toss ham with reserved parsley and divide among jars . Pour gelatin on top and chill until set , at least 1 hour . Whisk together mayonnaise , mustard , vinegar , 1 / 4 teaspoon salt , and 1 / 4 teaspoon pepper in a large bowl . Stir in celery , cornichons , and potatoes . Pulse peas with marjoram , oil , 1 / 2 teaspoon pepper , and 1 / 4 teaspoon salt in a food processor to a coarse mash . Layer peas , then potato salad , over ham . 

이런식으로 필터링됌.

여기서 구두점(.이나 , 같은거)도 별개의 단어로 취급하기 위한 처리를 함, 보면 ,랑 . 사이에 양 옆으로 띄어쓰기가 되어있는것을 볼 수 있음

```py
from collections import Counter

# 토큰화 및 어휘 사전 구축
def build_vocab(texts, max_vocab_size):
    counter = Counter()
    for text in texts:
        counter.update(text.lower().split())

    # 특수 토큰 추가
    special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']

    # 가장 빈도가 높은 단어들 선택 (특수 토큰 제외한 개수)
    most_common = counter.most_common(max_vocab_size - len(special_tokens))
    vocab = special_tokens + [word for word, _ in most_common]

    # 토큰:인덱스 매핑
    token2idx = {token: idx for idx, token in enumerate(vocab)}
    idx2token = {idx: token for idx, token in enumerate(vocab)}

    return vocab, token2idx, idx2token

# 텍스트를 인덱스로 변환
def tokenize_text(text, token2idx, max_len=None):
    tokens = text.lower().split()
    if max_len is not None:
        tokens = tokens[:max_len-2]  # <BOS>, <EOS> 토큰을 위한 공간 확보

    # <UNK> 토큰으로 OOV(Out of Vocabulary) 처리
    ids = [token2idx.get(token, token2idx['<UNK>']) for token in tokens]

    # <BOS>, <EOS> 추가
    ids = [token2idx['<BOS>']] + ids + [token2idx['<EOS>']]

    return ids

# PyTorch Dataset 클래스
class RecipeDataset(torch.utils.data.Dataset):
    def __init__(self, texts, token2idx, max_len):
        self.texts = texts
        self.token2idx = token2idx
        self.max_len = max_len

        # 모든 텍스트를 토큰화
        self.tokenized_texts = [tokenize_text(text, token2idx, max_len) for text in texts]

    def __len__(self):
        return len(self.tokenized_texts)

    def __getitem__(self, idx):
        ids = self.tokenized_texts[idx]

        # 패딩 및 마스크 생성
        padding_len = self.max_len - len(ids)

        if padding_len > 0:
            ids = ids + [self.token2idx['<PAD>']] * padding_len
        else:
            ids = ids[:self.max_len]

        # 텐서로 변환
        ids_tensor = torch.tensor(ids, dtype=torch.long)

        # 입력과 타겟 분리 (자기회귀 모델링을 위해)
        x = ids_tensor[:-1]  # 마지막 토큰 제외
        y = ids_tensor[1:]   # 첫 번째 토큰 제외

        return x, y

# 데이터로더 생성 함수
def create_dataloaders(text_data, max_vocab_size, max_len, batch_size, val_split=0.2, seed=42):
    # 어휘 사전 구축
    vocab, token2idx, idx2token = build_vocab(text_data, max_vocab_size)

    # 훈련/검증 데이터 분할
    np.random.seed(seed)
    indices = np.random.permutation(len(text_data))
    val_size = int(len(text_data) * val_split)
    train_indices = indices[val_size:]
    val_indices = indices[:val_size]

    train_texts = [text_data[i] for i in train_indices]
    val_texts = [text_data[i] for i in val_indices]

    # 데이터셋 생성
    train_dataset = RecipeDataset(train_texts, token2idx, max_len)
    val_dataset = RecipeDataset(val_texts, token2idx, max_len)

    # 데이터로더 생성
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True
    )
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False
    )

    return train_loader, val_loader, vocab, token2idx, idx2token
```

이 함수는 텍스트 데이터에서 어휘사전을 만드는 함수,

- 모든 텍스트에서 사용된 단어들을 수집하고 빈도수를 계산

- 가장 많이 사용된 단어들로 어휘 사전을 구축

- 텍스트를 숫자로 변환하기 위한 매핑 생성(토큰화)

그러므로 여기서 어휘사전의 의미는 텍스트에서 자주 쓰이는 글자들의 빈도수에 따라 to, and, that 같은 자주 나오는 단어들에 1,2,3 이렇게 숫자를 매겨(토큰화) 저 함수의 입력 매개변수 max_vocab_size만큼의 갯수를 단어로 가진 어휘사전을 만든다.

- 신경망은 텍스트를 직접 처리할 수 없고 숫자만 처리 가능

- 이 매핑을 통해 텍스트↔숫자 간 변환이 가능해짐

- LSTM이나 VAE 모델이 텍스트를 학습하고 생성하기 위한 기본 전처리 과정

#### 단어 빈도 계산, 특수 토큰 정의

```py
counter = Counter()
for text in texts:
    counter.update(text.lower().split())
    
special_tokens = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']
```

- Counter(): 항목의 개수를 세는 파이썬 컨테이너

- 모든 텍스트를 소문자로 변환하고 공백으로 분리하여 단어 단위로 분리

- counter.update(): 각 단어의 등장 횟수를 누적하여 계산

- <PAD>: 패딩 토큰 (시퀀스 길이를 맞추기 위해 사용)

- <UNK>: 알 수 없는 단어 토큰 (어휘 사전에 없는 단어)

- <BOS>: 시퀀스 시작 토큰 (Beginning of Sequence)

- <EOS>: 시퀀스 종료 토큰 (End of Sequence) 

### 가장 많이 사용된 단어 선택

```py
most_common = counter.most_common(max_vocab_size - len(special_tokens))
vocab = special_tokens + [word for word, _ in most_common]
```

- counter.most_common(): 가장 빈도가 높은 단어들을 선택, 여기서 special tokens는 단어로 볼 수 없기 떄문에 max vocab size에서 제외하고 그 갯수만큼을 선택

- 특수 토큰을 어휘 사전의 앞부분에 배치, 여기서 special_tokens와 [~~~] 두개의 리스트를 +하는 리스트 결합 연산이 쓰임, Python에서 + 연산자는 두 리스트를 결합할 때 왼쪽 리스트의 모든 요소가 먼저 오고, 오른쪽 리스트의 요소가 그 뒤에 배치, 
- special_tokens: 특수 토큰 리스트 ['<PAD>', '<UNK>', '<BOS>', '<EOS>']

- [word for word, _ in most_common]: 빈도수 기준으로 정렬된 일반 단어들의 리스트

- _는 빈도수 값을 무시하는 관례적 표기법 , <pad>, <unk> 이런 스페셜 토큰에 0,1,2,3 이렇게 붙여 나가고 그 후에 일반 단어들에 숫자를 뒤 이어 붙임
- most_common은 Counter.most_common() 메서드의 반환값, 이는 (단어, 빈도수) 형태의 튜플 리스트

```py
most_common = [('recipe', 100), ('for', 90), ('the', 80), ('and', 70), ...]

[word for word, _ in most_common]
#저 문법은 구조적으로는 아래의 문장과 동일하다
result = []
for word, _ in most_common:
    result.append(word)
```

most_common 리스트의 각 튜플을 순회

각 튜플을 두 변수 word와 _에 언패킹(unpacking)

word에는 튜플의 첫 번째 요소(단어)가 저장

_에는 튜플의 두 번째 요소(빈도수)가 저장

_ 기호는 Python에서 "이 값은 사용하지 않을 것"이라는 관례적 표기

리스트 컴프리헨션은 word 값만 포함하는 새 리스트를 생성, 즉 단어-빈도수의 쌍의 리스트를 가진 most_common에서(counter.most_common이 단어와 빈도수 두 개의 값을 가진 튜플 리스트를 내뱉기떄문에) 단어만 쏙 뺴기 위한 구문이다.

### 매핑 생성

```py
token2idx = {token: idx for idx, token in enumerate(vocab)}
idx2token = {idx: token for idx, token in enumerate(vocab)}
```

- token2idx: 단어→인덱스 매핑 (모델 입력용)

- idx2token: 인덱스→단어 매핑 (모델 출력 해석용)

여기서 enumerate라는 함수가 시퀀스(리스트, 튜플 등)를 순회하면서 인덱스와 값의 쌍을 자동으로 딕셔너리를 생성하기 떄문에 idx가 자연스럽게 생긴거다.

### 반환값

- vocab: 전체 어휘 사전 (리스트)

- token2idx: 단어→인덱스 매핑 (딕셔너리)

- idx2token: 인덱스→단어 매핑 (딕셔너리)

```py
def tokenize_text(text, token2idx, max_len=None):
    # 텍스트를 소문자로 변환하고 공백 기준으로 분리
    tokens = text.lower().split()
    
    # 최대 길이 제한 (필요시)
    if max_len is not None:
        tokens = tokens[:max_len-2]  # <BOS>, <EOS> 토큰을 위한 공간 확보
    
    # 단어를 인덱스로 변환 (어휘 사전에 없는 단어는 <UNK> 토큰으로 대체)
    ids = [token2idx.get(token, token2idx['<UNK>']) for token in tokens]
    
    # 시작과 끝 토큰 추가
    ids = [token2idx['<BOS>']] + ids + [token2idx['<EOS>']]
    
    return ids
```

이제 텍스트를 인덱스로(어휘사전에서 자주 나오는 단어순서대로 나열한거) 변환하는 함수, 여기서 일단 명확하게 tokenize_text는 하나의 텍스트를 다루는거고 앞에 본 build_vocab은 텍스트모음집을 가져다가 어휘사전을 만드는거다, 그래서 이거는 매개변수가 text고 앞에는 texts로 표시해놨다, 그러므로 vocab을 이용하는건 token2idx 즉  token2idx = {'<PAD>': 0, '<UNK>': 1, '<BOS>': 2, '<EOS>': 3, 'recipe': 4, 'for': 5, ...} 이런식으로 이 매개변수가 사전을 의미하는거고, 이거에 따라 텍스트를 인덱스로 변환시키는거다.

- 시퀀스가 너무 길면 max_len 매개변수로 제한

- max_len-2로 자르는 이유는 시작(<BOS>)과 끝(<EOS>) 토큰을 위한 공간을 확보하기 위함
- token2idx.get(token, token2idx['<UNK>']): 어휘 사전에 없는 단어(Out of Vocabulary)는 <UNK> 토큰으로 대체, 그러면서 동시에 text를 idx로 바꿈

#### 딕셔너리의 get() 메서드

dictionary.get(key, default_value)

- key: 찾고자 하는 키

- default_value: 키가 딕셔너리에 없을 경우 반환할 기본값

즉 저 리스트컴프리헨션에서 일어난 일은

```py
ids = []
for token in tokens:
    # 단어가 사전에 있는지 확인
    if token in token2idx:
        # 있으면 해당 인덱스 사용
        index = token2idx[token]
    else:
        # 없으면 <UNK> 토큰의 인덱스 사용
        index = token2idx['<UNK>']
    ids.append(index)
```

라고 생각하면 된다.

예시를 들어 tokens 리스트가 ["recipe", "for", "unknown_word"]이고, token2idx 딕셔너리가 다음과 같다고 가정. token2idx = {
    '<PAD>': 0,
    '<UNK>': 1,
    '<BOS>': 2,
    '<EOS>': 3,
    'recipe': 4,
    'for': 5
}

라고 했을 떄, 

token = "recipe" → token2idx.get("recipe", token2idx['<UNK>']) → 4 (사전에 있음)

token = "for" → token2idx.get("for", token2idx['<UNK>']) → 5 (사전에 있음)

token = "unknown_word" → token2idx.get("unknown_word", token2idx['<UNK>']) → 1 (사전에 없어서 <UNK>의 인덱스인 1 반환)

결과: ids = [4, 5, 1]

```py
class RecipeDataset(torch.utils.data.Dataset):
    def __init__(self, texts, token2idx, max_len):
        self.texts = texts
        self.token2idx = token2idx
        self.max_len = max_len

        # 모든 텍스트를 토큰화
        self.tokenized_texts = [tokenize_text(text, token2idx, max_len) for text in texts]

    def __len__(self):
        return len(self.tokenized_texts)

    def __getitem__(self, idx):
        ids = self.tokenized_texts[idx]

        # 패딩 및 마스크 생성
        padding_len = self.max_len - len(ids)

        if padding_len > 0:
            ids = ids + [self.token2idx['<PAD>']] * padding_len
        else:
            ids = ids[:self.max_len]

        # 텐서로 변환
        ids_tensor = torch.tensor(ids, dtype=torch.long)

        # 입력과 타겟 분리 (자기회귀 모델링을 위해)
        x = ids_tensor[:-1]  # 마지막 토큰 제외
        y = ids_tensor[1:]   # 첫 번째 토큰 제외

        return x, y

```

데이터 파이프라인 설계부분

torch.utils.data.Dataset는 파이토치의 데이터 로딩 파이프라인과 호환되도록 상속시키는 부분, 이 데이터로딩 파이프라인은 반드시 __init__, __len__, __getitem__ 메서드를 구현해야 함.

```py
def __init__(self, texts, token2idx, max_len):
    self.texts = texts
    self.token2idx = token2idx
    self.max_len = max_len
    
    # 모든 텍스트를 토큰화
    self.tokenized_texts = [tokenize_text(text, token2idx, max_len) for text in texts]
```

- texts: 원본 텍스트 리스트

- token2idx: 단어→인덱스 매핑 딕셔너리

- max_len: 최대 시퀀스 길이
- 이렇게 하면 매번 데이터를 가져올 떄마다 토큰화를 할 필요가 없음

```py
def __getitem__(self, idx):
    ids = self.tokenized_texts[idx]
    
    # 패딩 및 마스크 생성
    padding_len = self.max_len - len(ids)
    
    if padding_len > 0:
        ids = ids + [self.token2idx['<PAD>']] * padding_len
    else:
        ids = ids[:self.max_len]
        
    # 텐서로 변환
    ids_tensor = torch.tensor(ids, dtype=torch.long)
    
    # 입력과 타겟 분리 (자기회귀 모델링을 위해)
    x = ids_tensor[:-1]  # 마지막 토큰 제외
    y = ids_tensor[1:]   # 첫 번째 토큰 제외
    
    return x, y
```

지정된 인덱스(idx)의 토큰화된 텍스트를 가져옴, 그리고 그 텍스트가 토큰화된 것들을 ids(어휘사전에 따른 숫자)에 대응하여 바꿈

- 시퀀스가 최대 길이보다 짧으면 <PAD> 토큰으로 채웁니다.

- 시퀀스가 최대 길이보다 길면 잘라냅니다.

이렇게 시퀀스 길이들을 다 맞춰놔야 gpu연산할 떄 문제가 없다.

- 리스트를 PyTorch 텐서로 변환합니다.

- dtype=torch.long: 인덱스는 정수형이므로 long 데이터 타입 사용
- x = ids_tensor[:-1]: 마지막 토큰을 제외한 모든 토큰 (입력)

- y = ids_tensor[1:]: 첫 번째 토큰을 제외한 모든 토큰 (타겟)

- 이렇게 하면 각 위치에서 다음 토큰을 예측하는 자기회귀 모델 학습이 가능

즉 x에 따라 y를 예측하기 위해서(해당 단어 다음에 나올 단어를 학습하기 위해서)

```py
def create_dataloaders(text_data, max_vocab_size, max_len, batch_size, val_split=0.2, seed=42):
  vocab, token2idx, idx2token = build_vocab(text_data, max_vocab_size)

```

텍스트 데이터를 받아 PyTorch 모델 훈련에 필요한 모든 데이터 관련 객체를 준비하는 종합 함수

- 원시 텍스트 데이터에서 훈련/검증용 DataLoader 생성

- 어휘 사전과 관련 매핑 구축

- 데이터 분할 및 전처리를 한 번에 처리

```py
np.random.seed(seed)
indices = np.random.permutation(len(text_data))
val_size = int(len(text_data) * val_split)
train_indices = indices[val_size:]
val_indices = indices[:val_size]

train_texts = [text_data[i] for i in train_indices]
val_texts = [text_data[i] for i in val_indices]
```

- np.random.permutation: 인덱스를 무작위로 섞음

- val_size: 전체 데이터의 20%(기본값)를 검증 데이터로 사용

- 섞인 인덱스를 사용해 텍스트를 훈련/검증 세트로 분할

즉 text_data라는 텍스트묶음들을 무작위 순서로 섞고(텍스트 안의 단어들을 섞는다는게 아니라 하나의 문단 글 그런 시퀀스) 

- val_indices는 처음 200개의 인덱스 (0~199번째)

- train_indices는 나머지 800개의 인덱스 (200~999번째)

이런식으로 나눠서 훈련용과 평가용을 나눠둠

원본 데이터가 ["텍스트1", "텍스트2", "텍스트3", "텍스트4", "텍스트5"]이고 val_split=0.4라면:

indices = [3, 0, 4, 1, 2] (무작위 순서)

val_size = 5 * 0.4 = 2

val_indices = [3, 0] (처음 2개)

train_indices = [4, 1, 2] (나머지)

val_texts = ["텍스트4", "텍스트1"]

train_texts = ["텍스트5", "텍스트2", "텍스트3"]

```py
# 7. 전체 과정 실행
train_loader, val_loader, vocab, token2idx, idx2token = create_dataloaders(
    text_data,
    VOCAB_SIZE,
    MAX_LEN,
    BATCH_SIZE,
    VALIDATION_SPLIT,
    SEED
)

# 8. 결과 확인
print(f"어휘 크기: {len(vocab)}")
print(f"특수 토큰: {vocab[:4]}")
print(f"자주 사용되는 토큰: {vocab[4:14]}")

# 데이터로더에서 샘플 하나 확인
inputs, targets = next(iter(train_loader))
print(f"입력 배치 크기: {inputs.shape}")  # [배치 크기, 시퀀스 길이-1]
print(f"타겟 배치 크기: {targets.shape}")  # [배치 크기, 시퀀스 길이-1]

# 샘플 문장 디코딩 (첫 번째 배치의 첫 번째 문장)
sample_ids = inputs[0].tolist()
sample_text = ' '.join([idx2token[idx] for idx in sample_ids if idx != token2idx['<PAD>']])
print(f"샘플 입력 문장: {sample_text}")
```

어휘 크기: 10000 특수 토큰: ['<PAD>', '<UNK>', '<BOS>', '<EOS>'] 자주 사용되는 토큰: ['.', ',', 'and', 'to', 'in', 'the', 'with', 'a', 'until', '1'] 입력 배치 크기: torch.Size([32, 199]) 타겟 배치 크기: torch.Size([32, 199]) 샘플 입력 문장: <BOS> recipe for pappardelle with pancetta , broccoli rabe , and pine nuts | heat 2 tablespoons oil in heavy large skillet over medium - high heat . add garlic and cook until golden brown , stirring frequently , about 3 minutes . discard garlic . add onion , pancetta , and fennel seeds to skillet ; sauté until onion is tender and pancetta begins to brown , about 8 minutes . add dried crushed red pepper , then broccoli rabe stems and cook 4 minutes to soften slightly , stirring occasionally . stir in broccoli rabe tops , sprinkle with salt , and add 1 cup water . cover and cook until stems and tops are tender , about 5 minutes . season to taste with salt and pepper . meanwhile , cook pasta in large pot of boiling salted water until just tender but still firm to bite . drain pasta , reserving 1 cup cooking liquid . add pasta to skillet with broccoli rabe and stir over low heat to combine , adding reserved cooking liquid by tablespoonfuls to moisten if necessary . stir in remaining 2 tablespoons oil and 1 cup cheese . season to









