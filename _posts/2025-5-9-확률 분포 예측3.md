---
key:
title: '바이너리, 카테고리 손실함수, 음의 가능도 손실함수'
excerpt: 'deeplearning'
tags: [인공지능]
---

# 딥러닝에서의 확률적 예측: 분포, 파라미터, 그리고 가능도

딥러닝 모델은 단순한 값 예측을 넘어, 예측의 불확실성까지 포착하는 확률적 예측을 수행할 수 있다. 이는 모델이 특정 확률 분포를 가정하고, 그 분포를 특징짓는 파라미터를 예측함으로써 달성된다. 이 과정에서 음의 로그 가능도(NLL)는 모델 학습을 위한 핵심적인 손실 함수 역할을 한다.

## 1. 이진 분류와 베르누이 분포

출력 변수 $y$가 0 또는 1의 두 가지 값 중 하나를 갖는 이진 분류 문제에서, $y$는 **베르누이 분포(Bernoulli distribution)**를 따른다고 가정한다. 베르누이 분포는 단일 파라미터 $\lambda$에 의해 완전히 결정되며, $\lambda$는 $y=1$일 확률을 나타낸다 ($P(y=1) = \lambda$). 따라서 $y=0$일 확률은 $P(y=0) = 1-\lambda$가 된다. 이 분포의 확률 질량 함수는 다음과 같이 간결하게 표현될 수 있다:

$P(y\|\lambda) = \lambda^y (1-\lambda)^{1-y}$

신경망 모델 $f(x, \phi)$ (여기서 $x$는 입력, $\phi$는 모델 가중치)는 직접적으로 $0$과 $1$ 사이의 확률 값 $\lambda$를 출력하도록 보장하기 어렵다. 따라서 모델의 원시 출력 $z = f(x, \phi)$를 **로지스틱 시그모이드 함수(logistic sigmoid function)**에 통과시킨다:

$\hat{\lambda} = \text{sig}(z) = \frac{1}{1 + e^{-z}}$

이 변환을 통해 얻은 $\hat{\lambda}$는 항상 $0$과 $1$ 사이의 값을 가지므로, 베르누이 분포의 파라미터(즉, $P(y=1\|x)$의 예측값)로 사용될 수 있다.

주어진 입력 $x_i$와 실제 레이블 $y_i$에 대해, 모델이 예측한 파라미터 $\hat{\lambda}_i$ 하에서의 가능도(likelihood)는 $P(y_i\|\hat{\lambda}_i)$이다. 모델 학습은 전체 훈련 데이터에 대한 음의 로그 가능도(NLL)를 최소화하는 방향으로 진행된다. 이진 분류의 경우, 이는 **이진 교차 엔트로피(binary cross-entropy)** 손실과 동일하다:

$L(\phi) = -\sum_i [y_i \log(\hat{\lambda}_i) + (1-y_i) \log(1-\hat{\lambda}_i)]$

여기서 $\hat{\lambda}_i = \text{sig}(f(x_i, \phi))$이다.

## 2. 다중 클래스 분류와 카테고리 분포

출력 변수 $y$가 $K$개의 가능한 범주 중 하나에 속하는 다중 클래스 분류 문제에서는 **카테고리 분포(Categorical distribution)**를 가정한다. 이 분포는 각 범주 $k$에 속할 확률을 나타내는 $K$개의 파라미터 $\vec{\lambda} = (\lambda_1, \lambda_2, ..., \lambda_K)$에 의해 결정되며, 다음 두 가지 제약 조건을 만족해야 한다:
1.  모든 $k$에 대해 $\lambda_k \ge 0$
2.  $\sum_{k=1}^K \lambda_k = 1$

### 신경망의 소프트맥스와 교차 엔트로피 손실

신경망은 $K$개의 원시 출력(로짓) $\vec{z} = (z_1, ..., z_K)$을 생성하고, 이들을 소프트맥스 함수(softmax function)에 통과시켜 위의 제약 조건을 만족하는 확률 벡터 $\hat{\vec{\lambda}}$를 얻는다.

$$
\hat{\lambda}_k = \text{softmax}(\vec{z})_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}
$$

각 $\hat{\lambda}_k$는 모델이 입력 $x$에 대해 범주 $k$를 예측하는 확률 $P(y=k\|x)$가 된다.

만약 실제 레이블 $y_i$가 원-핫 인코딩된 벡터(실제 클래스에 해당하는 원소만 1이고 나머지는 0) $y_{ik}$로 표현된다면, NLL은 카테고리 교차 엔트로피(categorical cross-entropy) 손실이 된다.

$$
L(\phi) = -\sum_i \sum_k y_{ik} \log(\hat{\lambda}_{ik})
$$

## 3. 핵심 원리

다양한 유형의 문제(회귀, 이진 분류, 다중 클래스 분류 등)에서 NLL이 일관된 프레임워크로 사용될 수 있는 근본적인 이유는 모델이 실제 관측값 $y_i$ 자체를 직접 예측하는 것이 아니라, $y_i$가 특정 확률 분포를 따른다고 가정하고 그 **분포를 특정짓는 파라미터(들)을 예측**하기 때문이다.

*   **연속형 출력 (회귀)**: 모델은 정규분포의 평균 $\mu$ (때로는 분산 $\sigma^2$까지)를 예측한다.
*   **이진 출력**: 모델은 베르누이 분포의 성공 확률 $\lambda$를 예측한다.
*   **다중 클래스 출력**: 모델은 카테고리 분포의 각 클래스별 확률 $\lambda_k$들을 예측한다.

모델의 직접적인 출력(신경망의 마지막 계층 값)은 이러한 분포 파라미터를 얻기 위한 중간 단계이며, 필요에 따라 시그모이드, 소프트맥스, 또는 분산 예측을 위한 지수 함수나 제곱 함수 등의 변환 함수를 거쳐 최종 파라미터 값으로 가공된다. 이 예측 능력은 바로 음의 로그 가능도(NLL)를 최소화하는 학습 과정을 통해 길러진다.

이러한 접근 방식은 오차제곱합(Sum of Squared Errors, SSE)과 대비된다. SSE는 $L = \sum_i (y_i - \hat{y}_i)^2$로 정의되며, 모델의 직접적인 예측치 $\hat{y}_i$와 실제값 $y_i$의 차이를 사용한다. 흥미롭게도, SSE는 NLL의 특별한 경우로 볼 수 있다. 만약 모델 오차 $y_i - \hat{y}_i$가 평균이 0이고 분산이 일정한 정규분포를 따른다고 가정하면 (즉, $y_i \sim \mathcal{N}(\hat{y}_i, \sigma^2)$이고 $\sigma^2$이 상수), NLL을 최소화하는 것은 SSE를 최소화하는 것과 동등해진다. NLL은 이처럼 특정 가정을 넘어 다양한 확률 분포와 문제 유형에 적용될 수 있는 더 일반적이고 유연한 원칙을 제공한다.

## 4. 학습 과정

딥러닝 모델이 어떻게 최적의 분포 파라미터를 예측하도록 학습하는지 그 과정을 살펴보자.

1.  **초기 모델 상태**: 학습 시작 시, 모델의 가중치 $\phi$는 보통 무작위로 초기화된다. 이로 인해 각 입력 $x_i$에 대해 모델 $f(x_i, \phi_{\text{initial}})$이 예측하는 분포 파라미터(예: $\hat{\lambda}_i$) 역시 임의적이며, 이 파라미터로 정의된 초기 확률 분포 $P(y\|x_i, \phi_{\text{initial}})$는 실제 데이터 $y_i$를 잘 설명하지 못할 가능성이 크다. 이는 낮은 가능도, 즉 높은 NLL 값으로 나타난다.

2.  **전체 데이터셋에 대한 손실 계산**: 모델의 성능은 개별 데이터 포인트가 아닌 전체 훈련 데이터셋에 대해 평가된다. 음의 로그 가능도(NLL)는 각 데이터 샘플 $(x_i, y_i)$에 대한 $\log P(y_i\|x_i, \phi)$ 값을 모두 합한 후 음수를 취하여 계산된다:
    $L(\phi) = -\sum_i \log P(y_i\|x_i, \phi)$
    (각 샘플이 독립적이라고 가정할 때, 전체 가능도는 각 샘플 가능도의 곱이고, 로그를 취하면 합이 된다.)

3.  **반복적 최적화**:
    *   계산된 총 NLL (손실)을 줄이기 위해, 모델의 가중치 $\phi$가 경사 하강법과 같은 최적화 알고리즘을 통해 조금씩 업데이트된다.
    *   업데이트된 가중치 $\phi_{\text{new}}$는 동일한 입력 $x_i$에 대해서도 다른 분포 파라미터 $\hat{\lambda}_i(\phi_{\text{new}})$를 예측하게 만든다.
    *   결과적으로, 모델이 각 입력 $x_i$에 대해 제안하는 조건부 확률 분포 $P(y\|x_i, \phi)$ 자체가 점진적으로 변하게 된다.
    *   학습의 목표는 이처럼 변화하는 분포들이 실제 관측된 $y_i$ 값들이 나타날 가능성을 이전보다 더 높게 부여하도록, 즉 전체 NLL을 최소화하도록 하는 것이다.

이 과정을 훈련 데이터셋 전체에 대해 반복함으로써, 모델은 점차 각 입력에 대해 실제 데이터의 패턴을 가장 잘 반영하는 확률 분포 파라미터를 예측하는 능력을 학습하게 된다.

## 5. 베르누이 분포의 $\lambda$

정규분포에서 평균 $\mu$와 분산 $\sigma^2$이 분포의 모양(위치, 퍼짐 정도)을 결정하는 파라미터라는 점은 직관적이다. 그러나 베르누이 분포의 파라미터 $\lambda$는 그 자체가 "$y=1$일 확률"이라는 직접적인 의미를 갖기 때문에, 이것이 어떻게 분포 전체를 대표하는 "파라미터" 역할을 하는지 의문이 들 수 있다.

확률 분포의 파라미터란, 그 값이 정해졌을 때 해당 확률 분포의 모든 수학적/통계적 속성을 **유일하게 결정짓는 상수**를 의미한다. 베르누이 분포의 경우, 파라미터 $\lambda$ 값이 하나로 정해지면 다음과 같은 모든 특성이 결정된다:
*   $P(y=1) = \lambda$
*   $P(y=0) = 1-\lambda$
*   분포의 평균(기댓값) $E[y] = \lambda$
*   분포의 분산 $\text{Var}(y) = \lambda(1-\lambda)$

즉, $\lambda$ 값 하나가 베르누이 시행의 결과가 어떻게 분포되어 있는지(0과 1이 각각 어느 정도의 확률로 나타나는지)를 완벽하게 기술한다. $\lambda$가 0.1일 때의 베르누이 분포와 0.9일 때의 베르누이 분포는 그 성격이 완전히 다르다. 이처럼 $\lambda$는 베르누이 분포의 정체성을 규정하는 핵심 지표이므로, 비록 직접적인 확률값을 의미하더라도 분포를 특정짓는 파라미터로서의 자격을 충분히 갖는다.

## 6. 모델 예측의 "최적"에 대한 다각적 이해

모델이 학습을 통해 "최적"의 예측을 한다는 것은 어떤 의미일까? 예를 들어, 특정 입력 $x_0 = 0.5$에 대해 모델이 베르누이 파라미터 $\hat{\lambda} = 0.8$을 예측했다고 가정하자. 만약 이 $x_0 = 0.5$에 해당하는 관측 데이터가 10개 있었고, 그중 $y=1$이 8개, $y=0$이 2개였다면, 이 10개의 데이터만 놓고 봤을 때 $\hat{\lambda} = 0.8$은 이 특정 데이터를 가장 잘 설명하는 파라미터 값, 즉 최대 가능도 추정치(MLE)이다.

그러나 딥러닝 모델은 이처럼 국소적인 데이터 부분만을 고려하여 파라미터를 결정하지 않는다. 모델은 전체 훈련 데이터셋에 대한 총 NLL을 최소화하도록 학습된다. 따라서 모델이 최종적으로 학습된 후 $x_0 = 0.5$에 대해 $\hat{\lambda} = 0.8$을 예측했다면, 이는 $x_0$ 주변의 데이터뿐만 아니라 다른 모든 입력값에서의 데이터 패턴까지 종합적으로 고려하여 얻어진 "전체 모델 관점에서의 최적" 예측이다. 이 예측된 파라미터 $\hat{\lambda}$는 주어진 전체 훈련 데이터를 바탕으로, 해당 입력 조건 하에서 $y=1$일 확률에 대한 모델의 최선의 추정치를 나타낸다. 물론, 이 추정치는 우리가 알지 못하는 "진짜" 근원적인 확률과 정확히 일치하지 않을 수 있으며, 관측된 데이터의 양과 질에 따라 그 정확도가 달라질 수 있다.

