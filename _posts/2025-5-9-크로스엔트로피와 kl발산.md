---
key:
title: 'kl발산과 크로스 엔트로피'
excerpt: 'deeplearning'
tags: [deeplearning]
---

# 교차 엔트로피, KL 발산, 그리고 음의 로그 가능도의 관계

이전 논의에서 우리는 음의 로그 가능도(Negative Log-Likelihood, NLL)를 최소화하는 방식으로 모델을 학습시켜 왔다. 이 접근법은 모델이 예측하는 확률 분포 하에서 실제 관측된 데이터가 나타날 가능성을 최대화하는 직관적인 목표를 가진다. 이번에는 이 NLL 기반 학습이 정보 이론의 중요한 개념인 교차 엔트로피(Cross-Entropy) 및 쿨백-라이블러 발산(Kullback-Leibler divergence, KL 발산)과 어떻게 연결되는지 살펴본다. 이를 통해 NLL 손실 함수의 이론적 토대를 더 깊이 이해할 수 있다.

## 1. 손실 함수의 다른 관점: 분포 간의 거리

모델 학습의 목표는 모델이 생성하는 확률 분포 $P(y|\theta)$ (여기서 $\theta$는 모델 파라미터)가 실제 데이터가 따르는 참 분포 $q(y)$에 최대한 가까워지도록 하는 것이다. 두 확률 분포 사이의 "거리" 또는 "차이"를 측정하는 대표적인 방법 중 하나가 **KL 발산**이다.

![image-20250509141046463](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250509141046463.png)

그림 5.12는 이 개념을 시각적으로 보여준다:
*   **a) 경험적 데이터 분포 (Empirical data distribution) $q(y)$**: 우리가 가진 훈련 샘플들의 실제 분포를 나타낸다. 각 데이터 포인트는 해당 위치에서만 확률 질량을 갖는 디랙 델타 함수로 표현될 수 있다. 이것이 모델이 근사하고자 하는 "목표" 분포이다. 여기서 원래는 그냥 점으로 표현된 데이터 점 집합에서 이걸 확률 질량 분포로 나타내겠다는 말이다.
*   **b) 모델 분포 (Model distribution) $P(y|\theta)$**: 모델이 예측하는 확률 분포이다. 그림에서는 파라미터 $\theta = \{\mu, \sigma^2\}$를 갖는 정규분포로 예시되어 있다. 모델은 이 파라미터 $\theta$를 학습을 통해 조정한다.

교차 엔트로피 접근 방식의 핵심 아이디어는 모델 파라미터 $\theta$를 조정하여 이 두 분포, 즉 경험적 분포 $q(y)$와 모델 분포 $P(y|\theta)$ 사이의 **KL 발산을 최소화**하는 것, 즉 두 분포 사이의 거리를 줄이겠다는, 다시 말하면 유사도를 근접하게 비슷하게 만들겠다는 소리다.

## 2. KL 발산과 교차 엔트로피

두 확률 분포 $q(z)$와 $p(z)$ 사이의 KL 발산은 다음과 같이 정의된다 (연속 변수의 경우):

$D_{KL}[q||p] = \int_{-\infty}^{\infty} q(z) \log[q(z)] dz - \int_{-\infty}^{\infty} q(z) \log[p(z)] dz \quad (5.27)$

이 식에서:
*   첫 번째 항 $\int q(z) \log[q(z)] dz$는 실제 데이터 분포 $q(z)$의 **음의 엔트로피**이다. 이 값은 $q(z)$ 자체에만 의존하며, 모델 파라미터 $\theta$ (즉, $p(z) = P(z|\theta)$를 결정하는)와는 무관하다.
*   두 번째 항 \(-\int q(z) \log[p(z)] dz\)는 분포 $q(z)$와 $p(z)$ 사이의 **교차 엔트로피(Cross-Entropy)**라고 불린다.

우리가 KL 발산 $D_{KL}[q||p]$를 모델 파라미터 $\theta$에 대해 최소화하고자 할 때, $\theta$와 무관한 첫 번째 항(음의 엔트로피)은 최적화 과정에 영향을 주지 않는다. 따라서, **KL 발산을 최소화하는 것은 교차 엔트로피를 최소화하는 것과 동등하다.**

## 3. 경험적 분포와 교차 엔트로피의 최소화

이제 실제 훈련 데이터셋 $\{y_i\}_{i=1}^{I}$이 주어졌을 때, 경험적 데이터 분포 $q(y)$를 수학적으로 표현해보자. 각 데이터 포인트 $y_i$가 동일한 확률 $1/I$로 발생한다고 가정하면, $q(y)$는 각 데이터 포인트 위치에 디랙 델타 함수 $\delta[y-y_i]$를 놓고 가중치를 부여한 합으로 나타낼 수 있다:

$q(y) = \frac{1}{I} \sum_{i=1}^{I} \delta[y - y_i] \quad (5.28)$

우리의 목표는 모델 분포 $P(y|\theta)$와 이 경험적 분포 $q(y)$ 사이의 KL 발산을 최소화하는 최적의 파라미터 $\hat{\theta}$를 찾는 것이다. 이는 앞서 설명했듯이 교차 엔트로피를 최소화하는 것과 같다:

$\hat{\theta} = \underset{\theta}{\text{argmin}} \left[ -\int_{-\infty}^{\infty} q(y) \log[P(y|\theta)] dy \right] \quad (\text{수식 5.29 일부})$

이제 위 식의 $q(y)$ 자리에 수식 (5.28)의 경험적 분포 정의를 대입한다:

$\hat{\theta} = \underset{\theta}{\text{argmin}} \left[ -\int_{-\infty}^{\infty} \left(\frac{1}{I} \sum_{i=1}^{I} \delta[y - y_i]\right) \log[P(y|\theta)] dy \right]$

디랙 델타 함수의 중요한 성질 중 하나인 "sifting property" ($ \int g(y)\delta[y-y_i]dy = g(y_i) $)를 이용하면, 위 적분은 다음과 같이 단순화된다. (적분과 합의 순서를 바꾸고 각 항에 sifting property 적용)

$ \hat{\theta} = \underset{\theta}{\text{argmin}} \left[ -\frac{1}{I} \sum_{i=1}^{I} \log[P(y_i|\theta)] \right] $

여기서 상수 배율 인자 $1/I$는 최소값의 위치 $\theta$에 영향을 주지 않으므로, 최적화 목표에서 제외할 수 있다. 그러면 최종적으로 다음을 얻는다 (수식 5.30 일부):

$ \hat{\theta} = \underset{\theta}{\text{argmin}} \left[ -\sum_{i=1}^{I} \log[P(y_i|\theta)] \right] $

## 4. 결론: NLL 최소화는 교차 엔트로피 최소화와 동등

위의 마지막 식은 정확히 음의 로그 가능도(Negative Log-Likelihood, NLL)의 총합을 최소화하는 것과 동일하다. 기계 학습의 맥락에서 모델 파라미터 $\theta$는 입력 $x_i$와 모델 내부 가중치 $\phi$의 함수로 결정되므로 ($P(y_i|\theta)$는 $P(y_i|f[x_i, \phi])$로 표현됨), 최종적인 최적화 목표는 다음과 같이 표현된다 (수식 5.31):

$ \hat{\phi} = \underset{\phi}{\text{argmin}} \left[ -\sum_{i=1}^{I} \log[P(y_i|f[x_i, \phi])] \right] $

이는 이 장에서 다양한 모델(회귀, 이진 분류, 다중 클래스 분류)에 걸쳐 일관되게 사용해 온 손실 함수와 정확히 일치한다.

결론적으로, 음의 로그 가능도를 최소화하는 방식으로 모델을 학습시키는 것은, 정보 이론적 관점에서 볼 때 실제 데이터의 경험적 분포와 모델이 예측하는 분포 사이의 교차 엔트로피(또는 KL 발산)를 최소화하려는 시도와 수학적으로 동등하다. 이는 NLL을 손실 함수로 사용하는 것에 대한 강력한 이론적 정당성을 제공한다.