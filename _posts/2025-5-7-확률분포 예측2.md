---
key:
title: '정규분포 손실함수'
excerpt: 'deeplearning'
tags: [deeplearning]
---

# 딥러닝 모델의 확률적 예측과 손실 함수: 정규 분포에서 최소 제곱 오차로의 유도

딥러닝 모델이 단일 값 예측을 넘어 예측의 불확실성을 분포로 표현하는 방법론을 탐구한다. 본 문서는 모델이 예측값의 분포를 학습하는 과정, 이로부터 최소 제곱 오차(Least Squares Error) 손실 함수가 유도되는 원리, 그리고 학습된 모델을 통해 최종 예측값을 도출하는 추론 과정을 기술한다.

## 1. 모델의 평균 예측: $y$에 대한 정규 분포 가정

모델은 주어진 입력 $\mathbf{x}$에 대해 출력 $y$가 특정 확률 분포를 따른다고 가정한다. 일반적인 가정은 $y$가 정규 분포(Normal Distribution)를 따른다는 것이다.

정규 분포는 평균 $\mu$와 분산 $\sigma^2$로 기술된다.

$$
Pr(y|\mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{(y - \mu)^2}{2\sigma^2} \right] \quad (5.7)
$$

핵심은 모델 $f[\mathbf{x}, \phi]$ (여기서 $\phi$는 모델의 학습 가능한 파라미터)가 이 정규 분포의 평균 $\mu$를 예측한다는 점이다.

$$
\mu = f[\mathbf{x}, \phi]
$$

따라서, 모델의 예측을 반영한 $y$의 조건부 확률 분포는 다음과 같다.

$$
Pr(y|f[\mathbf{x}, \phi], \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{(y - f[\mathbf{x}, \phi])^2}{2\sigma^2} \right] \quad (5.8)
$$

### 분산 $\sigma^2$의 처리

위 식에서 분산 $\sigma^2$는 모델 파라미터 $\phi$에 포함되지 않는다. 즉, 현재 설정은 모델이 분산을 학습하지 않고, 모든 데이터 포인트에 대해 분산이 일정한 상수 값이라는 가정에 기반한다. 이는 회귀 문제에서 단순화를 위해 사용되는 동분산성(homoscedasticity) 가정과 관련된다. 모델은 데이터의 평균적 경향 포착에 집중하며, 예측 주변의 불확실성 정도는 고정된 것으로 간주한다. 분산 자체를 모델링하는 더 복잡한 접근도 가능하다.

## 2. 손실 함수 유도: 최대 가능도에서 최소 제곱 오차로

모델 파라미터 $\phi$의 학습은 최대 가능도 추정(Maximum Likelihood Estimation, MLE) 원리를 따른다. MLE는 주어진 훈련 데이터셋 $\{\mathbf{x}_i, y_i\}$이 현재 모델 하에서 관찰될 가능성(Likelihood)을 최대로 만드는 파라미터 $\phi$를 찾는 방법론이다.

$I$개의 샘플로 구성된 전체 데이터셋에 대한 가능도는 각 샘플 확률의 곱으로 표현된다.

$$
\text{Likelihood}(\phi) = \prod_{i=1}^{I} Pr(y_i|f[\mathbf{x}_i, \phi], \sigma^2)
$$

곱셈 연산의 복잡성으로 인해, 일반적으로 로그(log)를 적용한 로그 가능도(Log-Likelihood)를 사용한다. 로그 변환은 최댓값의 위치를 변경하지 않는다.

$$
\text{LogLikelihood}(\phi) = \sum_{i=1}^{I} \log \left[ Pr(y_i|f[\mathbf{x}_i, \phi], \sigma^2) \right]
$$

최적화 문제는 통상적으로 최소화 문제로 정의되므로, 로그 가능도에 음수를 취한 음의 로그 가능도(Negative Log-Likelihood, NLL)를 손실 함수 $L[\phi]$로 사용한다.

$$
L[\phi] = - \sum_{i=1}^{I} \log \left[ Pr(y_i|f[\mathbf{x}_i, \phi], \sigma^2) \right] \quad (5.9 \text{의 일부})
$$

수식 (5.8)을 (5.9)에 대입하여 정리하는 과정은 수식 (5.10)과 같다.

$$
\begin{align*}
\hat{\phi} &= \underset{\phi}{\text{argmin}} \left[ - \sum_{i=1}^{I} \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left[ -\frac{(y_i - f[\mathbf{x}_i, \phi])^2}{2\sigma^2} \right] \right] \right] \\
&= \underset{\phi}{\text{argmin}} \left[ - \sum_{i=1}^{I} \left( \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \right] - \frac{(y_i - f[\mathbf{x}_i, \phi])^2}{2\sigma^2} \right) \right] \quad (\text{로그의 성질}) \\
&= \underset{\phi}{\text{argmin}} \left[ - \sum_{i=1}^{I} - \frac{(y_i - f[\mathbf{x}_i, \phi])^2}{2\sigma^2} \right] \quad (\phi\text{와 무관한 첫 번째 항 제거}) \\
&= \underset{\phi}{\text{argmin}} \left[ \sum_{i=1}^{I} \frac{(y_i - f[\mathbf{x}_i, \phi])^2}{2\sigma^2} \right] \quad (\text{부호 정리}) \\
&= \underset{\phi}{\text{argmin}} \left[ \sum_{i=1}^{I} (y_i - f[\mathbf{x}_i, \phi])^2 \right] \quad (\phi\text{와 무관한 양의 상수 분모 제거}) \quad (5.10)
\end{align*}
$$

세부 유도 과정:

*   두 번째 줄에서 세 번째 줄로의 변환: $ \log \left[ \frac{1}{\sqrt{2\pi\sigma^2}} \right] $ 항은 모델 파라미터 $\phi$를 포함하지 않는다. $\phi$에 대한 손실 함수 최소화 과정에서 이 항은 상수로 취급되어 최적 $\phi$ 값에 영향을 미치지 않으므로 제거된다.
*   네 번째 줄에서 다섯 번째 줄로의 변환: $ \frac{1}{2\sigma^2} $ 항은 $\phi$에 대해 상수이며 $\sigma^2 > 0$이므로 양수이다. 함수에 양의 상수를 곱해도 최소값의 위치는 변하지 않는다. 따라서 이 스케일링 인자는 최적 $\phi$ 결정에 영향을 주지 않아 제거된다.

결론적으로, $y$가 정규 분포를 따른다는 가정 하에 음의 로그 가능도를 최소화하는 문제는 최소 제곱 오차, 즉 $\sum (y_i - f[\mathbf{x}_i, \phi])^2$를 최소화하는 문제와 동일하게 귀결된다. 이는 선형 회귀에서 표준적으로 사용되는 손실 함수이다.

## 3. 추론: 최적 단일 예측값 $\hat{y}$의 결정

모델 학습 완료 후 최적 파라미터 $\hat{\phi}$가 도출되었다고 가정한다. 새로운 입력 $\mathbf{x}$에 대한 최종 예측값 $\hat{y}$는 어떻게 결정되는가?

모델은 $y$에 대한 전체 확률 분포 $Pr(y|f[\mathbf{x}, \hat{\phi}], \sigma^2)$를 제공하나, 실제로는 가장 가능성 높은 단일 예측값이 요구된다. 이를 위해 예측된 분포에서 확률(밀도)이 가장 높은 지점을 선택한다.

$$
\hat{y} = \underset{y}{\text{argmax}} \left[ Pr(y|f[\mathbf{x}, \hat{\phi}], \sigma^2) \right] \quad (5.12)
$$

$\underset{y}{\text{argmax}}$는 대괄호 안의 값을 최대로 만드는 $y$를 찾는 연산자이다.

가정된 분포는 정규 분포이며, 정규 분포의 확률 밀도는 평균($\mu$) 지점에서 최대가 된다. 모델 $f[\mathbf{x}, \hat{\phi}]$은 정규 분포의 평균 $\mu$를 예측하도록 설계되었다.

따라서 최종 예측값 $\hat{y}$는 다음과 같다.

$$
\hat{y} = f[\mathbf{x}, \hat{\phi}]
$$

즉, 모델이 예측한 평균값이 최종 예측값으로 사용된다.

## 4. 결론

본 문서는 딥러닝 모델이 출력값의 평균을 예측하고 오차가 정규 분포를 따른다고 가정할 때, 최대 가능도 추정 원리가 최소 제곱 오차 손실 함수로 유도됨을 보였다. 또한, 학습된 모델로부터 최빈값을 취하는 추론 과정이 모델의 평균 예측값을 사용하는 것과 동일함을 확인하였다. 이러한 확률적 접근은 모델 예측에 대한 깊이 있는 이해를 제공하며 다양한 고급 모델링 기법의 이론적 토대를 형성한다.

![image-20250507181030534](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250507181030534.png)

이걸 보면 위는 MSE, 아래는 NLL 인데 여기서 생각해봐야 할 것이 위의 식 과정은 어디까지나 분산을 상수로 고정 할 떄 손실함수로 파라미터를 찾는 과정을 나타낸거고 이 그래프는 실제 손실함수를 이용해서 값을 구하는 과정으로 이 떈 분산이 쓰인다.

# 최소 제곱 오차(MSE)와 음의 로그 가능도(NLL): 같은 듯 다른 두 손실 함수

선형 회귀 모델을 학습할 때 흔히 손실 함수로 최소 제곱 오차(MSE)를 사용한다. 그런데 확률적 관점에서 모델을 바라보면 음의 로그 가능도(NLL)라는 개념이 등장하고, 특정 조건 하에서는 이 NLL을 최소화하는 것이 MSE를 최소화하는 것과 동일한 모델 파라미터를 찾는 것으로 귀결된다. 그렇다면 이 둘은 같은 것일까, 아니면 다른 의미를 지니는 것일까?

## 1. 목표는 동일: 최적의 모델 파라미터 $ \phi $

앞선 논의에서 확인했듯이, 데이터의 오차(실제값과 예측값의 차이)가 평균이 0이고 분산이 $ \sigma^2 $인 정규 분포를 따른다고 가정하면, 음의 로그 가능도(NLL)를 최소화하는 문제는 결국 최소 제곱 오차(MSE)를 최소화하는 문제와 동일한 모델 파라미터 $ \phi $ (예: 회귀선의 기울기와 절편)를 찾게 된다.

수식으로 표현된 NLL은 다음과 같다.

$$
L[\phi, \sigma^2] = \sum_{i=1}^{I} \left[ \frac{1}{2}\log(2\pi\sigma^2) + \frac{(y_i - f[\mathbf{x}_i, \phi])^2}{2\sigma^2} \right]
$$

여기서 $ f[\mathbf{x}_i, \phi] $는 모델의 예측값이다. 만약 분산 $ \sigma^2 $을 특정 상수 값으로 간주하고 오직 $ \phi $에 대해서만 이 손실 함수를 최소화한다고 가정하면, $ \phi $와 무관한 항들($ \frac{1}{2}\log(2\pi\sigma^2) $과 분모의 $ 2\sigma^2 $)은 최적의 $ \phi $를 찾는 데 영향을 주지 않는다. 결과적으로 최적화 목표는 다음과 같이 단순화된다.

$$
\hat{\phi} = \underset{\phi}{\text{argmin}} \left[ \sum_{i=1}^{I} (y_i - f[\mathbf{x}_i, \phi])^2 \right]
$$

이것이 바로 MSE를 최소화하는 것과 동일하다. 즉, "최적의 선을 찾는" 관점에서는 두 방법이 같은 결과를 도출한다.

## 2. 무엇이 다른가? $ \sigma^2 $의 역할과 확률적 해석

그렇다면 왜 NLL이라는, 상대적으로 복잡해 보이는 경로를 고려하는 것일까?

### 2.1. 손실 함수의 '값'과 그 의미

최적의 $ \phi $는 같을지라도, 계산되는 손실 함수의 **값 자체**는 다르다. MSE는 예측값과 실제값 사이의 평균적인 제곱 거리라는 직관적인 값을 제공한다. 반면 NLL 값은 모델이 데이터 포인트들을 **확률적으로 얼마나 잘 설명하는지**를 나타낸다.

NLL 값에는 $ \sigma^2 $이 포함되어 계산된다. $ \sigma^2 $이 작을수록 모델은 데이터가 예측 평균 근처에 매우 밀집되어 있다고 가정하며, 실제 데이터가 이 평균에서 조금만 벗어나도 NLL 값은 크게 증가(나빠짐)한다. 반대로 $ \sigma^2 $이 크면 모델은 데이터가 넓게 퍼져 있다고 가정하므로, 평균에서 다소 벗어나도 NLL 값이 상대적으로 덜 민감하게 반응한다. 그림에서 각 데이터 포인트에 그려진 정규분포(종 모양) 곡선의 폭이 $ \sigma^2 $을 반영하며, NLL은 이 분포 하에서 실제 데이터 포인트가 나타날 확률(밀도)에 기반한다.

### 2.2. 확률적 가정의 명시와 이론적 근거

MSE를 사용하는 것은 그 자체로 강력한 방법이지만, "왜 제곱 오차인가?"에 대한 근본적인 통계적 이유는 제시하지 않는다. NLL 접근법은 "데이터 생성 과정에 확률적 노이즈가 존재하며, 이 노이즈는 특정 분포(여기서는 정규 분포)를 따른다"는 **명시적인 확률적 가정**에서 출발한다.

이 가정을 통해, MSE가 최적의 선택이 되는 이유가 "오차가 정규 분포를 따를 때, 가능도를 최대화(NLL을 최소화)하는 것과 같기 때문"이라는 이론적 토대를 마련한다. 만약 오차 분포가 다르다면 (예: 라플라스 분포), NLL은 다른 손실 함수(예: 평균 절대 오차, MAE)를 유도할 것이다. 이는 NLL이 문제의 특성에 맞는 손실 함수를 선택하는 데 있어 유연성과 정당성을 제공함을 의미한다.

### 2.3. 모델의 일반화 및 확장성

NLL 프레임워크는 매우 일반적이어서 다양한 유형의 문제에 적용될 수 있다.

*   분류 문제에서 출력값이 이진(0 또는 1)일 경우, 베르누이 분포를 가정하고 NLL을 적용하면 크로스 엔트로피 손실 함수가 유도된다.
*   출력이 카운트 데이터일 경우, 포아송 분포를 가정하면 포아송 손실 함수가 유도된다.

이처럼 NLL은 다양한 데이터와 모델에 대해 적절한 손실 함수를 체계적으로 유도하는 일반적인 원리이다.

### 2.4. 분산 $ \sigma^2 $ 추정의 가능성

지금까지 $ \sigma^2 $을 $ \phi $를 찾는 과정에서는 상수로 취급했다. 하지만 NLL 프레임워크에서는 $ \sigma^2 $ 자체도 데이터로부터 추정해야 할 미지의 파라미터로 간주할 수 있다. 이 경우, $ L[\phi, \sigma^2] $을 $ \phi $와 $ \sigma^2 $ 모두에 대해 동시에 최소화함으로써 데이터의 불확실성 정도까지 모델링할 수 있다. MSE만으로는 이러한 분산 추정을 직접적으로 수행하기 어렵다.

## 3. NLL에서 MSE 유도 과정의 의의

그렇다면 NLL에서 $ \sigma^2 $ 관련 항을 제거하여 MSE 형태를 유도한 과정은 무엇을 위함이었을까?

이는 주로 **MSE 사용의 정당성을 확률론적 관점에서 부여**하기 위함이다. 즉, "만약 당신의 데이터에서 오차가 정규 분포를 따르고 그 분산이 일정하다면, 당신이 사용하고 있는 MSE는 사실 확률적으로 가장 그럴듯한 모델을 찾는 NLL 원리에 부합하는 매우 합리적인 선택이다"라는 메시지를 전달한다. 이는 MSE가 단순한 경험적 선택이 아니라 특정 통계적 가정 하에서 이론적 지지를 받는다는 것을 보여준다.

## 4. 결론

MSE와 NLL(정규 분포 가정 하)은 최적의 모델 파라미터 $ \phi $를 찾는다는 점에서는 같은 목적지에 도달할 수 있다. 그러나 NLL은 그 과정에 확률적 해석을 부여하고, 모델의 가정과 데이터의 특성에 대한 더 깊은 이해를 가능하게 한다. 또한, 손실 함수의 값 자체에 $ \sigma^2 $을 반영하여 예측의 불확실성을 고려하며, 다양한 문제로 확장할 수 있는 일반성과 분산과 같은 추가 파라미터를 추정할 수 있는 유연성을 제공한다. 따라서 NLL은 단순한 손실 함수 계산을 넘어, 데이터 생성 과정에 대한 통계적 모델링 접근법으로 이해하는 것이 중요하다. 그러나 파라미터를 찾는 과정에서의 등가성은 NLL과 MSE가 측정하는 근본적인 값의 차이를 간과해서는 안 된다. NLL은 정규화 상수 등의 요인을 포함하여 확률적 적합도를 측정하는 반면, MSE는 오차 제곱의 합을 직접적으로 측정한다.

그러면 지금까진 분산이 일정하다는 **동분산성**가정으로 구했는데 만약 분산이 입력데이터에 따라 달라진다고 하면?

# 모델 불확실성의 이해: 동분산성에서 이분산성으로

통계 및 기계 학습 모델을 구축할 때, 모델의 예측이 얼마나 확실한지, 또는 불확실한지를 이해하는 것은 매우 중요하다. 이러한 불확실성은 종종 모델 오차의 분산으로 표현된다. 모델의 불확실성을 다루는 두 가지 주요 가정이 있는데, 바로 동분산성(homoscedasticity)과 이분산성(heteroscedasticity)이다.

## 동분산성 가정 (Homoscedasticity)

많은 기본적인 모델은 **동분산성**을 가정한다. 이는 모델 오차의 분산이 모든 입력 데이터 지점에서 일정하다고 보는 것이다. 즉, 모델 예측의 불확실성 정도가 입력 값에 관계없이 동일하다고 가정한다.

예를 들어, 입력 $x$에 대한 출력 $y$를 예측하는 확률 모델을 생각해보자. 모델은 입력 $x_i$에 대해 평균 예측 $f(x_i, \phi)$를 제공하며 (여기서 $\phi$는 모델 파라미터), 실제 관측값 $y_i$는 이 평균 예측 주변에 특정 확률 분포를 따른다고 가정한다. 이 분포의 분산을 $\sigma^2$라고 할 때, 동분산성 가정은 이 $\sigma^2$ 값이 모든 $i$에 대해 동일한 상수임을 의미한다.

이러한 모델의 적합도는 주로 음의 로그 가능도 (Negative Log Likelihood, NLL)를 통해 평가된다. 동분산성 가정 하에서 NLL은 다음과 같이 표현될 수 있다:

$NLL = -\sum_i \log(P(y_i \| f(x_i, \phi), \sigma^2))$

여기서 $P(y_i \| f(x_i, \phi), \sigma^2)$는 주어진 모델 예측 $f(x_i, \phi)$와 고정된 분산 $\sigma^2$ 하에서 실제 값 $y_i$가 관측될 확률이다. 예를 들어, 이 확률 분포가 정규분포(가우시안 분포)라고 가정하면, 각 $y_i$는 평균 $f(x_i, \phi)$와 분산 $\sigma^2$을 가지는 정규분포 $\mathcal{N}(f(x_i, \phi), \sigma^2)$에서 샘플링된다고 볼 수 있다. 시각적으로 표현하면, 각 입력 $x_i$에서의 예측값 $f(x_i, \phi)$를 중심으로 하는 확률 분포들의 "폭" (분산을 나타냄)이 모두 동일하게 그려진다.

## 이분산성으로의 확장 (Heteroscedasticity)

그러나 현실의 많은 데이터에서 동분산성 가정은 너무 단순하거나 비현실적일 수 있다. 어떤 경우에는 모델의 불확실성이 입력 데이터의 특성에 따라 달라질 수 있다. 예를 들어, 어떤 입력 구간에서는 모델이 매우 정확한 예측을 할 수 있지만 (작은 불확실성), 다른 입력 구간에서는 예측의 변동성이 클 수 있다 (큰 불확실성). 이러한 상황을 **이분산성**이라고 한다.

이분산성을 모델에 도입한다는 것은 분산 $\sigma^2$를 더 이상 고정된 상수로 취급하지 않고, 각 입력 $x_i$에 따라 변하는 값, 즉 $\sigma_i^2$ 또는 $\sigma^2(x_i)$로 다루는 것을 의미한다. 이 경우 모델은 입력 $x_i$에 따라 예측의 평균뿐만 아니라 예측의 분산(불확실성)도 함께 학습하거나 추론해야 한다.

시각적으로 이분산성은 각 입력 $x_i$에서의 예측값 $f(x_i, \phi)$를 중심으로 하는 확률 분포들의 "폭"이 입력 $x_i$에 따라 다르게 나타나는 것으로 표현된다. 어떤 $x_i$에서는 분포가 좁고 뾰족하며 (작은 $\sigma_i^2$), 다른 $x_i$에서는 넓고 완만하게 (큰 $\sigma_i^2$) 그려질 수 있다.

이분산성을 고려할 때 NLL 계산식은 다음과 같이 수정된다:

$NLL = -\sum_i \log(P(y_i \| f(x_i, \phi), \sigma_i^2))$

여기서 핵심적인 변화는 각 데이터 포인트 $y_i$의 가능도를 계산할 때 해당 입력 $x_i$에 대응하는 개별적인 분산 값 $\sigma_i^2$ (또는 $\sigma^2(x_i)$)를 사용한다는 점이다. 모델이 이 $\sigma_i^2$를 어떻게 결정하는지는 다양한 방식이 있을 수 있다. 예를 들어, 분산 자체를 입력 $x_i$의 또 다른 함수로 모델링할 수도 있고, 신경망의 경우 별도의 출력을 통해 분산 값을 예측하도록 할 수도 있다.

![image-20250509093712774](https://raw.githubusercontent.com/kirity1/blogimg/master/uPic/image-20250509093712774.png)