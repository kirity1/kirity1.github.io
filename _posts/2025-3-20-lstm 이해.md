---
key:
title: 'LSTM 구조 이해'
excerpt: 'LSTM'
tags: [인공지능]
---

lstm은 시퀀스 -> 임베딩층 -> lstm으로 이어가면서 학습을 한다

### 임베딩층(embedding)

시퀀스의 사이즈가 만약 200이라고 가정하고(글 길이가 200단어) 각각의 단어를 토큰화시켜서 정수로 넣었다고 가정하면 임베딩 층은 각 정수 토큰을 embedding_size 길이의 벡터로 변환하는 룩업 테이블이다, 먼저 임베딩 층의 목적은? "강아지" → [0.2, -0.4, 0.7, 0.1, ...] 이런식으로 단어를(여기선 정수화된 토큰) 컴퓨터가 이해할 수 있는 숫자 벡터로 변환한다. 이떄 이 숫자 벡터의 길이(차원)를 embedding_size까지 늘리는데, 이런 식으로 만약 어휘사전(vocab)이 10000개라고 사이즈를 정했다고 하면 10000가지의 각각의 토큰을 숫자 벡터로 변환하는 걸 모아놓은 표를 임베당 층이라고 하는거다.

   토큰 ID 0 → [0.1, 0.2, 0.3, ...]
   토큰 ID 1 → [0.4, 0.5, 0.6, ...]
   토큰 ID 2 → [0.7, 0.8, 0.9, ...]
   ...

그러므로 만약 5, 10, 2 이렇게 어떤 문장이 들어왔다고 가정하면    [5, 10, 2] → [[0.5, ...], [0.8, ...], [0.7, ...]] 이런식으로 변환되는데, 이제 이 룩업 테이블을 **학습**을 통해 알맞는 가중치(각 단어의 벡터값)로  역전파를 통해 변한다. 이를 통해 단어들의 의미적 관계가 점점 더 잘 반영 된 벡터공간을 형성된다,

```py
   self.embedding = nn.Embedding(vocab_size, embedding_dim)
```

여기서 vocab size는 어휘사전의 크기, dim은 임베딩층의 크기(차원)을 의미한다.

### 이런 임베딩 층을 하는 이유는

#### 1.차원 축소

##### 원-핫 인코딩

- 크기: 어휘 크기와 동일한 차원을 가짐

- 예: 어휘 크기가 10,000개라면, 단어 하나를 표현하기 위해 10,000차원 벡터 필요

- 구조: 단 하나의 요소만 1이고 나머지는 모두 0

"강아지" → [0,0,1,0,0,0,0,...,0] (10,000개 요소)

##### 임베딩

- 크기: 사용자가 지정한 차원 (일반적으로 100~300 정도)

- 예: 어휘 크기가 10,000개여도 단어 하나를 300차원 벡터로 표현

- 구조: 모든 요소가 실수값인 밀집 벡터

"강아지" → [0.2, -0.4, 0.7, ...] (300개 요소)

이런식으로 예를 들어 10,000개 단어가 있을 때:

- 원-핫 방식: 각 단어는 10,000차원 벡터 (99.99% 공간 낭비)

- 임베딩 방식: 각 단어는 300차원 벡터 (97% 차원 축소)

즉 단어의 **의미**를 저차원 공간으로 압축시키는걸로 이해할 수 있다. 물론 앞에 lstm을 짤 떄는 단어를 원 핫 인코딩이 아닌 정수토큰으로 바꿨기 떄문에 축소라는 개념으로 볼 수 없지만 보통은 원 핫인코딩을 저런식으로 압축시킨다는거같다. 

#### 2.의미 포착

정수 토큰의 한계:

- 정수 5와 10 사이에는 수학적 관계가 있지만(5+5=10)

- 실제 단어 '고양이'(5)와 '강아지'(10) 사이에는 그런 관계가 없음

- 신경망은 이런 숫자를 그대로 사용하면 잘못된 패턴을 학습함

- 신경망은 실수 벡터를 입력으로 필요로 함

- LSTM 셀은 숫자 하나가 아닌 벡터를 입력으로 받음

   '왕' - '남자' + '여자' = '여왕' 이런식으로 텍스트 혹은 정수토큰은 연산이 불가능하지만 이걸 벡터 연산이 가능하게 만들기 위해서이다. 즉 

- 비슷한 맥락에 등장하는 단어들은 비슷한 벡터를 가지게 됨

- "커피"와 "차"는 비슷한 문맥에서 사용되어 벡터가 유사해짐

- 단어를 그냥 "5"라는 숫자가 아닌 [0.2, -0.4, 0.7, ...]처럼 다차원 특성으로 표현

- 각 차원은 단어의 특성을 나타냄 (예: 남성/여성, 크기, 감정 등)

     "강아지"와 "개"는 벡터 공간에서 가까이 위치
   "고양이"도 상대적으로 가까이 위치
     "자동차"는 멀리 위치

  이런 원리로 연산이 가능해지게 해서 인공지능이 학습할수 있게 해준다.

### LSTM

주가, 날씨와 같은 순서가 있는 시계열 데이터들에 대해, 순차데이터에 대해 생성하거나 하기 위해서, 즉 과거 정보를 기억하기 위해서 이전에 사용한 데이터를 재사용한다, 데이터를 재사용하기 위해서 신경망층에 순환 될 필요가 있는데 이런 알고리즘을 **순환 신경망**(RNN)이라고 한다.

![An unrolled recurrent neural network](https://t1.daumcdn.net/cfile/tistory/9901A1415ACB86A021)

이런식으로 이전 것에 대한 가중치가 다음 데이터에 영향을 끼치는 모습인데, 이 순환 신경망의 다양한 방법중에 LSTM이라는 것이 있다.

lstm은 https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr를 참고합니다.

